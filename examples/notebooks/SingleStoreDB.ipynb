{"cells":[{"attachments":{},"cell_type":"markdown","id":"76ae2947","metadata":{"execution":{}},"source":"<div id=\"singlestore-header\" style=\"display: flex; background-color: rgba(235, 249, 245, 0.25); padding: 5px;\">\n    <div id=\"icon-image\" style=\"width: 90px; height: 90px;\">\n        <img width=\"100%\" height=\"100%\" src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/header-icons/database.png\" />\n    </div>\n    <div id=\"text\" style=\"padding: 5px; margin-left: 10px;\">\n        <div id=\"badge\" style=\"display: inline-block; background-color: rgba(0, 0, 0, 0.15); border-radius: 4px; padding: 4px 8px; align-items: center; margin-top: 6px; margin-bottom: -2px; font-size: 80%\">SingleStore Notebooks</div>\n        <h1 style=\"font-weight: 500; margin: 8px 0 0 4px;\">A Deep Dive Into Vector Databases</h1>\n    </div>\n</div>"},{"attachments":{},"cell_type":"markdown","id":"3990f5b0","metadata":{"execution":{}},"source":"**Required Installations**"},{"cell_type":"code","execution_count":9,"id":"dd59a575-368a-4d99-887d-ab6c5f1ce623","metadata":{"execution":{"iopub.execute_input":"2025-02-13T22:00:31.459381Z","iopub.status.busy":"2025-02-13T22:00:31.457812Z","iopub.status.idle":"2025-02-13T22:00:37.273939Z","shell.execute_reply":"2025-02-13T22:00:37.271277Z","shell.execute_reply.started":"2025-02-13T22:00:31.459339Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Collecting pypdf\n  Downloading pypdf-5.3.0-py3-none-any.whl.metadata (7.2 kB)\nDownloading pypdf-5.3.0-py3-none-any.whl (300 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.7/300.7 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pypdf\nSuccessfully installed pypdf-5.3.0\n"}],"source":"!pip install pypdf"},{"cell_type":"code","execution_count":10,"id":"f35b269b","metadata":{"execution":{"iopub.execute_input":"2025-02-13T22:00:37.280050Z","iopub.status.busy":"2025-02-13T22:00:37.278415Z","iopub.status.idle":"2025-02-13T22:00:54.600275Z","shell.execute_reply":"2025-02-13T22:00:54.595079Z","shell.execute_reply.started":"2025-02-13T22:00:37.279992Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Collecting openai\n  Downloading openai-1.63.0-py3-none-any.whl.metadata (27 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (1.26.4)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.0.3)\nRequirement already satisfied: singlestoredb in /opt/conda/lib/python3.11/site-packages (1.10.0)\nCollecting langchain==0.1.8\n  Downloading langchain-0.1.8-py3-none-any.whl.metadata (13 kB)\nCollecting langchain-community==0.0.21\n  Downloading langchain_community-0.0.21-py3-none-any.whl.metadata (8.1 kB)\nCollecting langchain-core==0.1.25\n  Downloading langchain_core-0.1.25-py3-none-any.whl.metadata (6.0 kB)\nCollecting langchain-openai==0.0.6\n  Downloading langchain_openai-0.0.6-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain==0.1.8) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.11/site-packages (from langchain==0.1.8) (2.0.21)\nCollecting aiohttp<4.0.0,>=3.8.3 (from langchain==0.1.8)\n  Downloading aiohttp-3.11.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\nCollecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.1.8)\n  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain==0.1.8) (1.33)\nCollecting langsmith<0.2.0,>=0.1.0 (from langchain==0.1.8)\n  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain==0.1.8) (2.10.6)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langchain==0.1.8) (2.31.0)\nCollecting tenacity<9.0.0,>=8.1.0 (from langchain==0.1.8)\n  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\nRequirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.11/site-packages (from langchain-core==0.1.25) (4.3.0)\nCollecting packaging<24.0,>=23.2 (from langchain-core==0.1.25)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting tiktoken<1,>=0.5.2 (from langchain-openai==0.0.6)\n  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from openai) (0.27.0)\nCollecting jiter<1,>=0.4.0 (from openai)\n  Downloading jiter-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /opt/conda/lib/python3.11/site-packages (from openai) (4.66.4)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.11/site-packages (from openai) (4.12.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2025.1)\nRequirement already satisfied: PyJWT in /opt/conda/lib/python3.11/site-packages (from singlestoredb) (2.8.0)\nRequirement already satisfied: build in /opt/conda/lib/python3.11/site-packages (from singlestoredb) (1.2.2.post1)\nRequirement already satisfied: parsimonious in /opt/conda/lib/python3.11/site-packages (from singlestoredb) (0.10.0)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from singlestoredb) (69.5.1)\nRequirement already satisfied: sqlparams in /opt/conda/lib/python3.11/site-packages (from singlestoredb) (6.2.0)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.11/site-packages (from singlestoredb) (0.43.0)\nCollecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8)\n  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\nCollecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8)\n  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8) (23.2.0)\nCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8)\n  Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\nCollecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8)\n  Downloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\nCollecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8)\n  Downloading propcache-0.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\nCollecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.8)\n  Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core==0.1.25) (3.3)\nCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.8)\n  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\nCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.8)\n  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2021.10.8)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.8) (2.4)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain==0.1.8)\n  Downloading orjson-3.10.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.0->langchain==0.1.8)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain==0.1.8) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain==0.1.8) (2.27.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.1.8) (2.0.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.1.8) (1.26.16)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.8) (3.0.0rc3)\nRequirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.11/site-packages (from tiktoken<1,>=0.5.2->langchain-openai==0.0.6) (2024.11.6)\nRequirement already satisfied: pyproject_hooks in /opt/conda/lib/python3.11/site-packages (from build->singlestoredb) (1.2.0)\nCollecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.8)\n  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\nDownloading langchain-0.1.8-py3-none-any.whl (816 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m816.1/816.1 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_community-0.0.21-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.1.25-py3-none-any.whl (242 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.1/242.1 kB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_openai-0.0.6-py3-none-any.whl (29 kB)\nDownloading openai-1.63.0-py3-none-any.whl (472 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.3/472.3 kB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading aiohttp-3.11.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\nDownloading jiter-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.6/345.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\nDownloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\nDownloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\nDownloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multidict-6.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading orjson-3.10.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.3/130.3 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading propcache-0.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.1/231.1 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\nDownloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.1/344.1 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\nInstalling collected packages: tenacity, propcache, packaging, orjson, mypy-extensions, multidict, jiter, frozenlist, aiohappyeyeballs, yarl, typing-inspect, tiktoken, requests-toolbelt, marshmallow, aiosignal, openai, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-openai, langchain-community, langchain\n  Attempting uninstall: tenacity\n    Found existing installation: tenacity 9.0.0\n    Uninstalling tenacity-9.0.0:\n      Successfully uninstalled tenacity-9.0.0\n  Attempting uninstall: packaging\n    Found existing installation: packaging 24.0\n    Uninstalling packaging-24.0:\n      Successfully uninstalled packaging-24.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njupyter-server 2.14.0 requires jinja2>=3.0.3, but you have jinja2 3.0.2 which is incompatible.\njupyterlab 4.1.8 requires jinja2>=3.0.3, but you have jinja2 3.0.2 which is incompatible.\njupyterlab-server 2.27.1 requires jinja2>=3.0.3, but you have jinja2 3.0.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed aiohappyeyeballs-2.4.6 aiohttp-3.11.12 aiosignal-1.3.2 dataclasses-json-0.6.7 frozenlist-1.5.0 jiter-0.8.2 langchain-0.1.8 langchain-community-0.0.21 langchain-core-0.1.25 langchain-openai-0.0.6 langsmith-0.1.147 marshmallow-3.26.1 multidict-6.1.0 mypy-extensions-1.0.0 openai-1.63.0 orjson-3.10.15 packaging-23.2 propcache-0.2.1 requests-toolbelt-1.0.0 tenacity-8.5.0 tiktoken-0.8.0 typing-inspect-0.9.0 yarl-1.18.3\n"}],"source":"!pip install openai numpy pandas singlestoredb langchain==0.1.8 langchain-community==0.0.21 langchain-core==0.1.25 langchain-openai==0.0.6"},{"attachments":{},"cell_type":"markdown","id":"62525eb4","metadata":{"execution":{}},"source":"## Vector Embedding Example\n\nIn this example, we demonstrate a rule based system that generates vector embeddings based on a word. The embedding that we generate contains 5 main features:\n- Length of word\n- Number of vowels in the word (normalized to the length of the word)\n- Whether the word starts with a vowel (1) or not (0)\n- Whether the word ends with a vowel (1) or not (0)\n- Percentage of consonants in the word\n\nThis is a simple implementation of a **rule** based system to demonstrate the essence of what vector embedding models do. However, they utlize neural networks that are trained on vast datasets to learn key features and self-corrects using gradient descent."},{"attachments":{},"cell_type":"markdown","id":"930b3443","metadata":{"execution":{}},"source":"## Vector Similarity Example\n\nIn this example, we demonstrate a way to determine the similarity between two vectors. There are many techniques to find the similiarity between two vectors but one of the most popular ways is using **cosine similarity**. Consine similarity is the the dot product between the two vectors divided by the product of the vector's normals (magnitudes).\n\nThis is just an example to show how vector databases search for similar vectors. The fundamental problem with a system like this is our rule-based embedding because it does not give us a semantic understanding of the word/sentences/paragraphs. Instead, it gives us a classification of a single word's structure."},{"attachments":{},"cell_type":"markdown","id":"959b415b","metadata":{"execution":{}},"source":"## Embedding Models\n\nIn order to generate semantic understanding of language within vectors, embedding models are required. Embedding models are trained on vast corpus of language data. Training embedding models starts by initializing word embeddings with random vectors. Each word in the vocabulary is assigned a vector of real numbers. They use neural networks trained on large datasets to predict a word from its context (Continuous Bag of Words model) or to predict the context given a word (Skip-Gram model). During training, the model adjusts the word vectors to minimize some loss function, often related to the likelihood of observing a word given its context (or vice versa) through gradient descent.\n\nExamples of embedding models include Word2Vec, GloVe, BERT, OpenAI text-embedding."},{"attachments":{},"cell_type":"markdown","id":"c9caf1e8","metadata":{"execution":{}},"source":"As you can see, this is a huge vector! Over 1000 dimensions just in this one vector. This is why it is important for us to have good dimensionality reduction techniques during the similarity searches."},{"attachments":{},"cell_type":"markdown","id":"e6fc6afd","metadata":{"execution":{}},"source":"## Creating a vector database with SingleStoreDB\n\nIn the following code we create a vector datbase with SingleStoreDB. We utilize Langchain to chunk and split the raw text into documents and use the OpenAI embeddings model to generate the vector embeddings. We then take the raw documents and embeddings and create a table with the columns \"docs\" and \"embeddings\".\n\nTo test this out, we perform a similarity search based on a query and it returns the most similar document in the vector database."},{"cell_type":"code","execution_count":11,"id":"f655d0bd-2b5d-472d-b21c-271c006a5ae7","metadata":{"execution":{"iopub.execute_input":"2025-02-13T22:00:54.605823Z","iopub.status.busy":"2025-02-13T22:00:54.605358Z","iopub.status.idle":"2025-02-13T22:01:12.563767Z","shell.execute_reply":"2025-02-13T22:01:12.562660Z","shell.execute_reply.started":"2025-02-13T22:00:54.605785Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Collecting embed-anything==0.4.15\n  Downloading embed_anything-0.4.15-cp311-cp311-manylinux_2_34_x86_64.whl.metadata (13 kB)\nCollecting onnxruntime==1.19.2 (from embed-anything==0.4.15)\n  Downloading onnxruntime-1.19.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\nCollecting coloredlogs (from onnxruntime==1.19.2->embed-anything==0.4.15)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nCollecting flatbuffers (from onnxruntime==1.19.2->embed-anything==0.4.15)\n  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\nRequirement already satisfied: numpy>=1.21.6 in /opt/conda/lib/python3.11/site-packages (from onnxruntime==1.19.2->embed-anything==0.4.15) (1.26.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from onnxruntime==1.19.2->embed-anything==0.4.15) (23.2)\nCollecting protobuf (from onnxruntime==1.19.2->embed-anything==0.4.15)\n  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\nCollecting sympy (from onnxruntime==1.19.2->embed-anything==0.4.15)\n  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime==1.19.2->embed-anything==0.4.15)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nCollecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime==1.19.2->embed-anything==0.4.15)\n  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\nDownloading embed_anything-0.4.15-cp311-cp311-manylinux_2_34_x86_64.whl (18.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.19.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\nDownloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: mpmath, flatbuffers, sympy, protobuf, humanfriendly, coloredlogs, onnxruntime, embed-anything\nSuccessfully installed coloredlogs-15.0.1 embed-anything-0.4.15 flatbuffers-25.2.10 humanfriendly-10.0 mpmath-1.3.0 onnxruntime-1.19.2 protobuf-5.29.3 sympy-1.13.3\n"}],"source":"!pip install embed-anything==0.4.15"},{"cell_type":"code","execution_count":12,"id":"2b3792d1-9a1c-4e25-b6c0-c2156f90bda7","metadata":{"execution":{"iopub.execute_input":"2025-02-13T22:01:12.580490Z","iopub.status.busy":"2025-02-13T22:01:12.579687Z","iopub.status.idle":"2025-02-13T22:01:17.215693Z","shell.execute_reply":"2025-02-13T22:01:17.212589Z","shell.execute_reply.started":"2025-02-13T22:01:12.580443Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Cloning into 'EmbedAnything'...\nremote: Enumerating objects: 4985, done.\u001b[K\nremote: Counting objects: 100% (410/410), done.\u001b[K\nremote: Compressing objects: 100% (76/76), done.\u001b[K\nremote: Total 4985 (delta 355), reused 334 (delta 334), pack-reused 4575 (from 2)\u001b[K\nReceiving objects: 100% (4985/4985), 30.85 MiB | 21.43 MiB/s, done.\nResolving deltas: 100% (3128/3128), done.\n"}],"source":"import os\nif not os.path.exists(\"EmbedAnything\"):\n  !git clone https://github.com/StarlightSearch/EmbedAnything.git"},{"cell_type":"code","execution_count":13,"id":"2b59039f-7be3-4bac-a42c-08d3457264d6","metadata":{"execution":{"iopub.execute_input":"2025-02-13T22:01:17.228001Z","iopub.status.busy":"2025-02-13T22:01:17.225355Z","iopub.status.idle":"2025-02-13T22:01:17.408105Z","shell.execute_reply":"2025-02-13T22:01:17.406064Z","shell.execute_reply.started":"2025-02-13T22:01:17.227959Z"},"language":"python","trusted":true},"outputs":[],"source":"import embed_anything\nimport numpy as np\nimport time\nfrom embed_anything import EmbedData, EmbeddingModel, TextEmbedConfig, WhichModel\nimport os"},{"cell_type":"code","execution_count":14,"id":"4392875f-5a74-4835-b448-9b2983cc0c97","metadata":{"execution":{"iopub.execute_input":"2025-02-13T22:01:19.616773Z","iopub.status.busy":"2025-02-13T22:01:19.616141Z","iopub.status.idle":"2025-02-13T22:01:19.889538Z","shell.execute_reply":"2025-02-13T22:01:19.888395Z","shell.execute_reply.started":"2025-02-13T22:01:19.616735Z"},"language":"python","trusted":true},"outputs":[],"source":"from langchain_core.documents.base import Document\n"},{"cell_type":"code","execution_count":15,"id":"28e00437-0e10-43fd-af6a-09f688bdbeed","metadata":{"execution":{"iopub.execute_input":"2025-02-13T22:01:20.849383Z","iopub.status.busy":"2025-02-13T22:01:20.848690Z","iopub.status.idle":"2025-02-13T22:01:21.902658Z","shell.execute_reply":"2025-02-13T22:01:21.902140Z","shell.execute_reply.started":"2025-02-13T22:01:20.849100Z"},"language":"python","trusted":true},"outputs":[],"source":"from langchain_community.embeddings import OpenAIEmbeddings\nfrom langchain_core.embeddings import Embeddings\nfrom langchain_community.vectorstores.singlestoredb import SingleStoreDB\n\n"},{"cell_type":"code","execution_count":16,"id":"00ad10f6-6948-4f45-9f84-0159ff33f728","metadata":{"execution":{"iopub.execute_input":"2025-02-13T22:01:23.858575Z","iopub.status.busy":"2025-02-13T22:01:23.857416Z","iopub.status.idle":"2025-02-13T22:01:23.888776Z","shell.execute_reply":"2025-02-13T22:01:23.875703Z","shell.execute_reply.started":"2025-02-13T22:01:23.858466Z"},"language":"python","trusted":true},"outputs":[],"source":"class EmbedAnythingEmbeddings(Embeddings):\n    import embed_anything\n    def __init__(self, model_type: WhichModel, model_id, config):\n        self.model = EmbeddingModel.from_pretrained_hf(WhichModel.Bert, model_id=model_id)\n        self.config =config\n\n    def embed_documents(self, texts:list[str]):\n        embed_data = embed_anything.embed_query(texts, self.model, config = self.config)\n        return [e.embedding for e in embed_data]\n\n    def embed_query(self, text:str):\n        embed_data = embed_anything.embed_query([text], self.model, config = self.config)[0]\n        return embed_data.embedding\n        "},{"cell_type":"code","execution_count":17,"id":"1e6a168c-cb6d-49e4-90af-41d1c0af13a0","metadata":{"execution":{"iopub.execute_input":"2025-02-13T22:01:28.204933Z","iopub.status.busy":"2025-02-13T22:01:28.204481Z","iopub.status.idle":"2025-02-13T22:01:30.211554Z","shell.execute_reply":"2025-02-13T22:01:30.210337Z","shell.execute_reply.started":"2025-02-13T22:01:28.204897Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"sudo: The \"no new privileges\" flag is set, which prevents sudo from running as root.\nsudo: If sudo is running in a container, you may need to adjust the container configuration to disable the flag.\n"}],"source":"! sudo apt-get install wget"},{"cell_type":"code","execution_count":18,"id":"b32413f8-6f74-4c8d-9aef-c4a0d927241b","metadata":{"execution":{"iopub.execute_input":"2025-02-13T22:01:34.184733Z","iopub.status.busy":"2025-02-13T22:01:34.183445Z","iopub.status.idle":"2025-02-13T22:01:39.535201Z","shell.execute_reply":"2025-02-13T22:01:39.533508Z","shell.execute_reply.started":"2025-02-13T22:01:34.184467Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"--2025-02-13 22:01:36--  https://www.biorxiv.org/content/10.1101/2025.01.23.634433v1.full.pdf\nResolving www.biorxiv.org (www.biorxiv.org)... 104.18.34.83, 172.64.153.173, 2606:4700:4400::ac40:99ad, ...\nConnecting to www.biorxiv.org (www.biorxiv.org)|104.18.34.83|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [application/pdf]\nSaving to: ‘EmbedAnything/bench/medicine.pdf’\n\nEmbedAnything/bench     [  <=>               ]   1.55M  7.31MB/s    in 0.2s    \n\n2025-02-13 22:01:39 (7.31 MB/s) - ‘EmbedAnything/bench/medicine.pdf’ saved [1625826]\n\n"}],"source":"!wget https://www.biorxiv.org/content/10.1101/2025.01.23.634433v1.full.pdf -O EmbedAnything/bench/medicine.pdf\n"},{"cell_type":"code","execution_count":37,"id":"8b8f7a31-9ccb-4538-9b0a-13fd5d31d9be","metadata":{"execution":{"iopub.execute_input":"2025-02-13T22:20:32.399273Z","iopub.status.busy":"2025-02-13T22:20:32.398184Z","iopub.status.idle":"2025-02-13T22:20:33.198375Z","shell.execute_reply":"2025-02-13T22:20:33.195002Z","shell.execute_reply.started":"2025-02-13T22:20:32.399205Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Loading weights from \"/home/jovyan/.cache/huggingface/hub/models--NeuML--pubmedbert-base-embeddings/snapshots/ba210f40b1b6d555d675c2d1ed6372e44570fc3c/pytorch_model.bin\"\nCan't find model.safetensors, loading from pytorch_model.bin\n"}],"source":"config = TextEmbedConfig(chunk_size=256, batch_size=32)\nembedding_fn = EmbedAnythingEmbeddings(WhichModel.Bert, \"NeuML/pubmedbert-base-embeddings\", config)\n"},{"cell_type":"code","execution_count":21,"id":"a0eabe68-404b-44ce-b54e-5ef63c77f9f9","metadata":{"execution":{"iopub.execute_input":"2025-02-13T22:02:40.381900Z","iopub.status.busy":"2025-02-13T22:02:40.381001Z","iopub.status.idle":"2025-02-13T22:02:42.915965Z","shell.execute_reply":"2025-02-13T22:02:42.913789Z","shell.execute_reply.started":"2025-02-13T22:02:40.381844Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"attention.pdf  colpali.pdf  medicine.pdf  mistral.pdf\n"}],"source":"!cd EmbedAnything/bench && ls "},{"cell_type":"code","execution_count":22,"id":"4ddac3c3-4fbd-4f5f-9485-df161f0aa389","metadata":{"execution":{"iopub.execute_input":"2025-02-13T22:02:42.925309Z","iopub.status.busy":"2025-02-13T22:02:42.921940Z","iopub.status.idle":"2025-02-13T22:02:43.951171Z","shell.execute_reply":"2025-02-13T22:02:43.950557Z","shell.execute_reply.started":"2025-02-13T22:02:42.925243Z"},"language":"python","trusted":true},"outputs":[],"source":"import os\nimport openai\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_community.embeddings import OpenAIEmbeddings\nfrom langchain_community.vectorstores.singlestoredb import SingleStoreDB\nfrom openai import OpenAI"},{"cell_type":"code","execution_count":24,"id":"bb62c404-1967-46e3-9e2c-bc23be1e82cd","metadata":{"execution":{"iopub.execute_input":"2025-02-13T22:03:13.523792Z","iopub.status.busy":"2025-02-13T22:03:13.523254Z","iopub.status.idle":"2025-02-13T22:04:00.981199Z","shell.execute_reply":"2025-02-13T22:04:00.980683Z","shell.execute_reply.started":"2025-02-13T22:03:13.523750Z"},"language":"python","scrolled":true,"trusted":true},"outputs":[],"source":"from langchain_community.vectorstores.singlestoredb import SingleStoreDB\n\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import CharacterTextSplitter\nloader = PyPDFLoader(\"EmbedAnything/bench/medicine.pdf\") # use your own document\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\ndocs = text_splitter.split_documents(documents)\nvector_database = SingleStoreDB.from_documents(docs, embedding= embedding_fn, table_name = \"demo_med\") # create your own table"},{"cell_type":"code","execution_count":41,"id":"194dafca-17ba-4766-bcf4-5779adb1937a","metadata":{"execution":{"iopub.execute_input":"2025-02-13T22:28:21.202345Z","iopub.status.busy":"2025-02-13T22:28:21.201882Z","iopub.status.idle":"2025-02-13T22:28:21.414474Z","shell.execute_reply":"2025-02-13T22:28:21.412204Z","shell.execute_reply.started":"2025-02-13T22:28:21.202302Z"},"language":"python","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Figure S1.The growth of the tumor at t = (0,400,800,1200) from group (a) control pmut = 0.1, (b) control pmut = 0.3, (c)\ncontrol pmut = 0.5, (d) hyperfractioned radiotherapy pmut = 0.5, (e) conventional radiotherapy pmut = 0.5, (f) hypofractioned\nradiotherapy pmut = 0.5, (g) conventional radiotherapy pmut = 0.1, (h) conventional radiotherapy pmut = 0.3, (i) targeted\nradiotherapy pmut = 0.5. Blue and red cells are CCs and CSCs, respectively.\n12/12\n.CC-BY 4.0 International licenseavailable under a\n(which was not certified by peer review) is the author/funder, who has granted bioRxiv a license to display the preprint in perpetuity. It is made \nThe copyright holder for this preprintthis version posted January 25, 2025. ; https://doi.org/10.1101/2025.01.23.634433doi: bioRxiv preprint\n"}],"source":"query = \"What is radiotherapy\"\ndocs = vector_database.similarity_search(query)\nprint(docs[0].page_content)"},{"cell_type":"code","execution_count":5,"id":"edf5b87e","metadata":{"execution":{},"language":"python"},"outputs":[],"source":"# import openai\n# from langchain.text_splitter import CharacterTextSplitter\n# from langchain_community.document_loaders import TextLoader\n# from langchain_community.embeddings import OpenAIEmbeddings\n# from langchain_community.vectorstores.singlestoredb import SingleStoreDB\n# from openai import OpenAI\n# import os\n# import pandas as pd\n\n\n# # Load and process documents\n# loader = TextLoader(\"michael_jackson.txt\") # use your own document\n\n# documents = loader.load()\n# text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n# docs = text_splitter.split_documents(documents)\n\n# # Generate embeddings and create a document search database\n# embeddings = OpenAIEmbeddings(api_key=OPENAI_KEY)\n\n# # Create Vector Database\n# vector_database = SingleStoreDB.from_documents(docs, embeddings, table_name=\"mjackson\") # create your own table\n\n# query = \"How old was Michael Jackson when he died?\"\n# docs = vector_database.similarity_search(query)\n# print(docs[0].page_content)"},{"attachments":{},"cell_type":"markdown","id":"a33d3409","metadata":{"execution":{}},"source":"## Retrieval Augmented Generation System\n\nRAG combines large language models with a retrieval mechanism to search a database for relevant information before generating responses. It utilizes real-world data from retrieved documents to ground responses, enhancing factual accuracy and reducing hallucinations. Documents are vectorized using embeddings and stored in a vector database for efficient retrieval. SingleStoreDB serves as a great vector database. The user query is converted into a vector, and a vector search is performed in the database to find documents relevant to that specific query. The system returns the documents with the highest relevance scores, which are then fed to the chatbot for generating informed responses."},{"cell_type":"code","execution_count":44,"id":"9700834d","metadata":{"execution":{"iopub.execute_input":"2025-01-20T19:09:07.695730Z","iopub.status.busy":"2025-01-20T19:09:07.694997Z","iopub.status.idle":"2025-01-20T19:09:07.776897Z","shell.execute_reply":"2025-01-20T19:09:07.775198Z","shell.execute_reply.started":"2025-01-20T19:09:07.695695Z"},"language":"python","trusted":true},"outputs":[{"ename":"RuntimeError","evalue":"Error loading michael_jackson.txt","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/langchain_community/document_loaders/text.py:41\u001b[0m, in \u001b[0;36mTextLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     42\u001b[0m         text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'michael_jackson.txt'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[44], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Load and process documents\u001b[39;00m\n\u001b[1;32m     13\u001b[0m loader \u001b[38;5;241m=\u001b[39m TextLoader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmichael_jackson.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m CharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     16\u001b[0m docs \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(documents)\n","File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/langchain_community/document_loaders/text.py:57\u001b[0m, in \u001b[0;36mTextLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     59\u001b[0m metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path}\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [Document(page_content\u001b[38;5;241m=\u001b[39mtext, metadata\u001b[38;5;241m=\u001b[39mmetadata)]\n","\u001b[0;31mRuntimeError\u001b[0m: Error loading michael_jackson.txt"]}],"source":"import os\nimport openai\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_community.embeddings import OpenAIEmbeddings\nfrom langchain_community.vectorstores.singlestoredb import SingleStoreDB\nfrom openai import OpenAI\n\n# Set up API keys and database URL\nclient = OpenAI(api_key=\"sk-proj--qG1o0v2fJUf8ziux1T41Z3_Xqeld3ElIZu8hr8A\")\n\n# Load and process documents\nloader = TextLoader(\"michael_jackson.txt\")\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\ndocs = text_splitter.split_documents(documents)\n\n# Generate embeddings and create a document search database\nembeddings = OpenAIEmbeddings(OPENAI_KEY)\ndocsearch = SingleStoreDB.from_documents(docs, embeddings, table_name=\"mjackson\")\n\n# Chat loop\nwhile True:\n    # Get user input\n    user_query = input(\"\\nYou: \")\n\n    # Check for exit command\n    if user_query.lower() in ['quit', 'exit']:\n        print(\"Exiting chatbot.\")\n        break\n\n    # Perform similarity search\n    docs = docsearch.similarity_search(user_query)\n    if docs:\n        context = docs[0].page_content\n\n        # Generate response using OpenAI GPT-4\n        response = client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"Context: \" + context},\n                {\"role\": \"user\", \"content\": user_query}\n            ],\n            stream=True,\n            max_tokens=500,\n        )\n\n        # Output the response\n        print(\"AI: \", end=\"\")\n        for chunk in response:\n            if chunk.choices[0].delta.content is not None:\n                print(chunk.choices[0].delta.content, end=\"\")\n\n    else:\n        print(\"AI: Sorry, I couldn't find relevant information.\")"},{"attachments":{},"cell_type":"markdown","id":"983cd74d","metadata":{"execution":{}},"source":"<div id=\"singlestore-footer\" style=\"background-color: rgba(194, 193, 199, 0.25); height:2px; margin-bottom:10px\"></div>\n<div><img src=\"https://raw.githubusercontent.com/singlestore-labs/spaces-notebooks/master/common/images/singlestore-logo-grey.png\" style=\"padding: 0px; margin: 0px; height: 24px\"/></div>"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"singlestore_cell_default_language":"python","singlestore_connection":{"connectionID":"","defaultDatabase":""},"singlestore_row_limit":300},"nbformat":4,"nbformat_minor":5}