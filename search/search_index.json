{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>  Highly Performant, Modular and Memory Safe  Ingestion, Inference and Indexing in Rust \ud83e\udd80 Python docs \u00bb Rust docs \u00bb Benchmarks     \u00b7     FAQ     \u00b7     Adapters     .     Collaborations     .      Notebooks </p> <p>EmbedAnything is a minimalist, yet highly performant, modular, lightning-fast, lightweight, multisource, multimodal, and local embedding pipeline built in Rust. Whether you're working with text, images, audio, PDFs, websites, or other media, EmbedAnything streamlines the process of generating embeddings from various sources and seamlessly streaming (memory-efficient-indexing) them to a vector database. It supports dense, sparse, ONNX, model2vec and late-interaction embeddings, offering flexibility for a wide range of use cases.</p> Table of Contents <ol> <li> About The Project <ul> <li>Built With Rust</li> <li>Why Candle?</li> </ul> </li> <li> Getting Started <ul> <li>Installation</li> </ul> </li> <li>Usage</li> <li>Roadmap</li> <li>Contributing</li> <li>How to add custom model and chunk size</li> </ol>"},{"location":"#key-features","title":"\ud83d\ude80 Key Features","text":"<ul> <li>No Dependency on Pytorch: Easy to deploy on cloud, comes with low memory footprint.</li> <li>Highly Modular : Choose any vectorDB adapter for RAG, with ~~1 line~~ 1 word of code</li> <li>Candle Backend : Supports BERT, Jina, ColPali, Splade, ModernBERT, Reranker, Qwen</li> <li>ONNX Backend: Supports BERT, Jina, ColPali, ColBERT Splade, Reranker, ModernBERT, Qwen</li> <li>Cloud Embedding Models:: Supports OpenAI, Cohere, and Gemini.</li> <li>MultiModality : Works with text sources like PDFs, txt, md, Images JPG and Audio, .WAV</li> <li>GPU support : Hardware acceleration on GPU as well.</li> <li>Chunking : In-built chunking methods like semantic, late-chunking</li> <li>Vector Streaming: Separate file processing, Indexing and Inferencing on different threads, reduces latency.</li> </ul>"},{"location":"#what-is-vector-streaming","title":"\ud83d\udca1What is Vector Streaming","text":"<p>Embedding models are computationally expensive and time-consuming. By separating document preprocessing from model inference, you can significantly reduce pipeline latency and improve throughput.</p> <p>Vector streaming transforms a sequential bottleneck into an efficient, concurrent workflow.</p> <p>The embedding process happens separetly from the main process, so as to maintain high performance enabled by rust MPSC, and no memory leak as embeddings are directly saved to vector database. Find our blog.</p> <p></p>"},{"location":"#why-embed-anything","title":"\ud83e\udd80 Why Embed Anything","text":"<p>\u27a1\ufe0fFaster execution.  \u27a1\ufe0fNo Pytorch Dependency, thus low-memory footprint and easy to deploy on cloud.  \u27a1\ufe0fTrue multithreading  \u27a1\ufe0fRunning embedding models locally and efficiently  \u27a1\ufe0fIn-built chunking methods like semantic, late-chunking  \u27a1\ufe0fSupports range of models, Dense, Sparse, Late-interaction, ReRanker, ModernBert. \u27a1\ufe0fMemory Management: Rust enforces memory management simultaneously, preventing memory leaks and crashes that can plague other languages </p>"},{"location":"#our-past-collaborations","title":"\ud83c\udf53 Our Past Collaborations:","text":"<p>We have collaborated with reputed enterprise like Elastic, Weaviate, SingleStore, Milvus  and Analytics Vidya Datahours</p> <p>You can get in touch with us for further collaborations.</p>"},{"location":"#benchmarks","title":"Benchmarks","text":""},{"location":"#inference-speed-benchmarks","title":"Inference Speed benchmarks.","text":"<p>Only measures embedding model inference speed, on onnx-runtime. Code</p> <p></p> <p>Benchmarks with other fromeworks coming soon!! \ud83d\ude80</p>"},{"location":"#supported-models","title":"\u2b50 Supported Models","text":"<p>We support any hugging-face models on Candle. And We also support ONNX runtime for BERT and ColPali.</p>"},{"location":"#how-to-add-custom-model-on-candle-from_pretrained_hf","title":"How to add custom model on candle: from_pretrained_hf","text":"<pre><code>model = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Bert, model_id=\"model link from huggingface\"\n)\nconfig = TextEmbedConfig(chunk_size=1000, batch_size=32)\ndata = embed_anything.embed_file(\"file_address\", embedder=model, config=config)\n</code></pre> Model HF link Jina Jina Models Bert All Bert based models CLIP openai/clip-* Whisper OpenAI Whisper models ColPali starlight-ai/colpali-v1.2-merged-onnx Colbert answerdotai/answerai-colbert-small-v1, jinaai/jina-colbert-v2 and more Splade Splade Models and other Splade like models Model2Vec model2vec, minishlab/potion-base-8M Qwen3-Embedding Qwen/Qwen3-Embedding-0.6B Reranker Jina Reranker Models, Xenova/bge-reranker, Qwen/Qwen3-Reranker-4B"},{"location":"#splade-models","title":"Splade Models:","text":"<pre><code>model = EmbeddingModel.from_pretrained_hf(\n    WhichModel.SparseBert, \"prithivida/Splade_PP_en_v1\"\n)\n</code></pre>"},{"location":"#onnx-runtime-from_pretrained_onnx","title":"ONNX-Runtime: from_pretrained_onnx","text":""},{"location":"#bert","title":"BERT","text":"<pre><code>model = EmbeddingModel.from_pretrained_onnx(\n  WhichModel.Bert, model_id=\"onnx_model_link\"\n)\n</code></pre>"},{"location":"#colpali","title":"ColPali","text":"<pre><code>model: ColpaliModel = ColpaliModel.from_pretrained_onnx(\"starlight-ai/colpali-v1.2-merged-onnx\", None)\n</code></pre>"},{"location":"#colbert","title":"Colbert","text":"<pre><code>sentences = [\n\"The quick brown fox jumps over the lazy dog\", \n\"The cat is sleeping on the mat\", \"The dog is barking at the moon\", \n\"I love pizza\", \n\"The dog is sitting in the park\"]\n\nmodel = ColbertModel.from_pretrained_onnx(\"jinaai/jina-colbert-v2\", path_in_repo=\"onnx/model.onnx\")\nembeddings = model.embed(sentences, batch_size=2)\n</code></pre>"},{"location":"#rerankers","title":"ReRankers","text":"<pre><code>reranker = Reranker.from_pretrained(\"jinaai/jina-reranker-v1-turbo-en\", dtype=Dtype.F16)\n\nresults: list[RerankerResult] = reranker.rerank([\"What is the capital of France?\"], [\"France is a country in Europe.\", \"Paris is the capital of France.\"], 2)\n</code></pre>"},{"location":"#qwen-3-embedding","title":"Qwen 3 - Embedding","text":"<pre><code># Initialize the model once\nmodel:EmbeddingModel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Qwen3, model_id=\"Qwen/Qwen3-Embedding-0.6B\"\n)\n</code></pre>"},{"location":"#for-semantic-chunking","title":"For Semantic Chunking","text":"<pre><code>model = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Bert, model_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n)\n\n# with semantic encoder\nsemantic_encoder = EmbeddingModel.from_pretrained_hf(WhichModel.Jina, model_id = \"jinaai/jina-embeddings-v2-small-en\")\nconfig = TextEmbedConfig(chunk_size=1000, batch_size=32, splitting_strategy = \"semantic\", semantic_encoder=semantic_encoder)\n</code></pre>"},{"location":"#for-late-chunking","title":"For late-chunking","text":"<pre><code>config = TextEmbedConfig(\n    chunk_size=1000,\n    batch_size=8,\n    splitting_strategy=\"sentence\",\n    late_chunking=True,\n)\n\n# Embed a single file\ndata: list[EmbedData] = model.embed_file(\"test_files/attention.pdf\", config=config)\n</code></pre>"},{"location":"#getting-started","title":"\ud83e\uddd1\u200d\ud83d\ude80 Getting Started","text":""},{"location":"#installation","title":"\ud83d\udc9a Installation","text":"<p><code>pip install embed-anything</code></p> <p>For GPUs and using special models like ColPali </p> <p><code>pip install embed-anything-gpu</code></p> <p>\ud83d\udea7\u274c If it shows cuda error while running on windowns, run the following command:</p> <pre><code>os.add_dll_directory(\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/bin\")\n</code></pre>"},{"location":"#notebooks","title":"\ud83d\udcd2 Notebooks","text":"End-to-End Retrieval and Reranking using VectorDB Adapters ColPali-Onnx Adapters Qwen3- Embedings Benchmarks"},{"location":"#running-embedanything-as-an-openai-compatible-server","title":"\ud83d\ude80 Running EmbedAnything as an OpenAI-Compatible Server","text":"<p>You can run EmbedAnything as an OpenAI-compatible API server using Actix-web. This provides a production-ready, high-performance server for generating embeddings.</p> <p>To learn more about running the server, API endpoints, and usage examples, see Actix Server Guide</p>"},{"location":"#usage","title":"Usage","text":""},{"location":"#usage-for-03-and-later-version","title":"\u27a1\ufe0f Usage For 0.3 and later version","text":"<pre><code>model = EmbeddingModel.from_pretrained_local(\n    WhichModel.Bert, model_id=\"Hugging_face_link\"\n)\ndata = embed_anything.embed_file(\"test_files/test.pdf\", embedder=model)\n</code></pre>"},{"location":"#using-onnx-models","title":"Using ONNX Models","text":"<p>To use ONNX models, you can either use the <code>ONNXModel</code> enum or the <code>model_id</code> from the Hugging Face model.</p> <p>Using the above method is best to ensure that the model works correctly as these models are tested. But if you want to use other models, like finetuned models, you can use the <code>hf_model_id</code> and <code>path_in_repo</code> to load the model like below.</p> <p><pre><code>model = EmbeddingModel.from_pretrained_onnx(\n  WhichModel.Jina, hf_model_id = \"jinaai/jina-embeddings-v2-small-en\", path_in_repo=\"model.onnx\"\n)\n</code></pre> To see all the ONNX models supported with model_name, see here</p>"},{"location":"#faq","title":"\u2049\ufe0fFAQ","text":""},{"location":"#do-i-need-to-know-rust-to-use-or-contribute-to-embedanything","title":"Do I need to know rust to use or contribute to embedanything?","text":"<p>The answer is No. EmbedAnything provides you pyo3 bindings, so you can run any function in python without any issues. To contibute you should check out our guidelines and python folder example of adapters.</p>"},{"location":"#how-is-it-different-from-fastembed","title":"How is it different from fastembed?","text":"<p>We provide both backends, candle and onnx. On top of it we also give an end-to-end pipeline, that is you can ingest different data-types and index to any vector database, and inference any model. Fastembed is just an onnx-wrapper.</p>"},{"location":"#weve-received-quite-a-few-questions-about-why-were-using-candle","title":"We've received quite a few questions about why we're using Candle.","text":"<p>One of the main reasons is that Candle doesn't require any specific ONNX format models, which means it can work seamlessly with any Hugging Face model. This flexibility has been a key factor for us. However, we also recognize that we\u2019ve been compromising a bit on speed in favor of that flexibility.</p>"},{"location":"#contributing-to-embedanything","title":"\ud83d\udea7 Contributing to EmbedAnything","text":"<p>First of all, thank you for taking the time to contribute to this project. We truly appreciate your contributions, whether it's bug reports, feature suggestions, or pull requests. Your time and effort are highly valued in this project. \ud83d\ude80</p> <p>This document provides guidelines and best practices to help you to contribute effectively. These are meant to serve as guidelines, not strict rules. We encourage you to use your best judgment and feel comfortable proposing changes to this document through a pull request.</p> <li>Roadmap</li> <li>Quick Start</li> <li>Guidelines</li>"},{"location":"#roadmap","title":"\ud83c\udfce\ufe0f RoadMap","text":""},{"location":"#accomplishments","title":"Accomplishments","text":"<p>One of the aims of EmbedAnything is to allow AI engineers to easily use state of the art embedding models on typical files and documents. A lot has already been accomplished here and these are the formats that we support right now and a few more have to be done. </p>"},{"location":"#modalities-and-source","title":"\ud83d\uddbc\ufe0f Modalities and Source","text":"<p>We\u2019re excited to share that we've expanded our platform to support multiple modalities, including:</p> <ul> <li> <p> Audio files</p> </li> <li> <p> Markdowns</p> </li> <li> <p> Websites</p> </li> <li> <p> Images</p> </li> <li> <p> Videos</p> </li> <li> <p> Graph</p> </li> </ul> <p>This gives you the flexibility to work with various data types all in one place! \ud83c\udf10 </p>"},{"location":"#performance","title":"\u2699\ufe0f Performance","text":"<p>We now support both candle and Onnx backend \u27a1\ufe0f Support for GGUF models </p>"},{"location":"#embeddings","title":"\ud83e\uded0Embeddings:","text":"<p>We had multimodality from day one for our infrastructure. We have already included it for websites, images and audios but we want to expand it further to.</p> <p>\u27a1\ufe0f Graph embedding -- build deepwalks embeddings depth first and word to vec  \u27a1\ufe0f Video Embedding  \u27a1\ufe0f Yolo Clip </p>"},{"location":"#expansion-to-other-vector-adapters","title":"\ud83c\udf0aExpansion to other Vector Adapters","text":"<p>We currently support a wide range of vector databases for streaming embeddings, including:</p> <ul> <li>Elastic: thanks to amazing and active Elastic team for the contribution </li> <li>Weaviate </li> <li>Pinecone </li> <li>Qdrant </li> <li>Milvus</li> <li>Chroma </li> </ul> <p>How to add an adpters: https://starlight-search.com/blog/2024/02/25/adapter-development-guide.md</p>"},{"location":"#create-wasm-demos-to-integrate-embedanything-directly-to-the-browser","title":"\ud83d\udca5 Create WASM demos to integrate embedanything directly to the browser.","text":""},{"location":"#add-support-for-ingestion-from-remote-sources","title":"\ud83d\udc9c Add support for ingestion from remote sources","text":"<p>\u27a1\ufe0f Support for S3 bucket  \u27a1\ufe0f Support for azure storage  \u27a1\ufe0f Support for google drive/dropbox</p> <p>But we're not stopping there! We're actively working to expand this list.</p> <p>Want to Contribute? If you\u2019d like to add support for your favorite vector database, we\u2019d love to have your help! Check out our contribution.md for guidelines, or feel free to reach out directly turingatverge@gmail.com . Let's build something amazing together! \ud83d\udca1</p>"},{"location":"#a-big-thank-you-to-all-our-stargazers","title":"A big Thank you to all our StarGazers","text":""},{"location":"#star-history","title":"Star History","text":""},{"location":"references/","title":"\ud83d\udcda References","text":"<p>EmbedAnything: A high-performance, multimodal embedding pipeline.</p> <p>This module provides functions and classes for embedding queries, files, and directories using different embedding models. It supports text, images, audio, PDFs, and other media types with various embedding backends (Candle, ONNX, Cloud).</p>"},{"location":"references/#python.python.embed_anything--main-functions","title":"Main Functions:","text":"<ul> <li><code>embed_query</code>: Embeds text queries and returns a list of EmbedData objects.</li> <li><code>embed_file</code>: Embeds a single file and returns a list of EmbedData objects.</li> <li><code>embed_directory</code>: Embeds all files in a directory and returns a list of EmbedData objects.</li> <li><code>embed_image_directory</code>: Embeds all images in a directory.</li> <li><code>embed_audio_file</code>: Embeds audio files using Whisper for transcription.</li> <li><code>embed_webpage</code>: Embeds content from a webpage URL.</li> </ul>"},{"location":"references/#python.python.embed_anything--main-classes","title":"Main Classes:","text":"<ul> <li><code>EmbeddingModel</code>: Main class for loading and using embedding models.</li> <li><code>EmbedData</code>: Represents embedded data with text, embedding vector, and metadata.</li> <li><code>TextEmbedConfig</code>: Configuration for text embedding (chunking, batching, etc.).</li> <li><code>ColpaliModel</code>: Specialized model for document/image-text embedding.</li> <li><code>ColbertModel</code>: Model for late-interaction embeddings.</li> <li><code>Reranker</code>: Model for re-ranking search results.</li> <li><code>AudioDecoderModel</code>: Model for audio transcription (Whisper).</li> </ul>"},{"location":"references/#python.python.embed_anything--usage-examples","title":"Usage Examples:","text":""},{"location":"references/#python.python.embed_anything--text-embedding","title":"Text Embedding","text":"<pre><code>from embed_anything import EmbeddingModel, WhichModel, TextEmbedConfig\nimport embed_anything\n\n# Load a text embedding model\nmodel = EmbeddingModel.from_pretrained_local(\n    WhichModel.Bert, \n    model_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n)\n\n# Configure embedding parameters\nconfig = TextEmbedConfig(\n    chunk_size=1000,              # Characters per chunk\n    batch_size=32,                # Process 32 chunks at once\n    splitting_strategy=\"sentence\"  # Split by sentences\n)\n\n# Embed a PDF file\ndata = embed_anything.embed_file(\"test_files/document.pdf\", embedder=model, config=config)\n\n# Access results\nfor item in data:\n    print(f\"Text: {item.text[:100]}...\")\n    print(f\"Embedding dimension: {len(item.embedding)}\")\n    print(f\"Metadata: {item.metadata}\")\n</code></pre>"},{"location":"references/#python.python.embed_anything--image-embedding","title":"Image Embedding","text":"<pre><code>import embed_anything\nimport numpy as np\nfrom embed_anything import EmbedData, EmbeddingModel, WhichModel\n\n# Load CLIP model for image-text embeddings\nmodel = EmbeddingModel.from_pretrained_local(\n    WhichModel.Clip,\n    model_id=\"openai/clip-vit-base-patch16\"\n)\n\n# Embed all images in a directory\ndata: list[EmbedData] = embed_anything.embed_image_directory(\n    \"test_files\", \n    embedder=model\n)\n\n# Convert to numpy array for similarity search\nembeddings = np.array([item.embedding for item in data])\n\n# Embed a text query\nquery = [\"Photo of a monkey?\"]\nquery_embedding = np.array(\n    embed_anything.embed_query(query, embedder=model)[0].embedding\n)\n\n# Calculate cosine similarity\nsimilarities = np.dot(embeddings, query_embedding)\nmost_similar_idx = np.argmax(similarities)\nprint(f\"Most similar image: {data[most_similar_idx].text}\")\n</code></pre>"},{"location":"references/#python.python.embed_anything--audio-embedding","title":"Audio Embedding","text":"<pre><code>from embed_anything import (\n    AudioDecoderModel,\n    EmbeddingModel,\n    embed_audio_file,\n    TextEmbedConfig,\n    WhichModel\n)\nimport embed_anything\n\n# Load Whisper model for audio transcription\n# Choose from: https://huggingface.co/distil-whisper or \n# https://huggingface.co/collections/openai/whisper-release-6501bba2cf999715fd953013\naudio_decoder = AudioDecoderModel.from_pretrained_hf(\n    \"openai/whisper-tiny.en\", \n    revision=\"main\", \n    model_type=\"tiny-en\", \n    quantized=False\n)\n\n# Load text embedding model for transcribed text\nembedder = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Bert,\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    revision=\"main\",\n)\n\n# Configure text embedding\nconfig = TextEmbedConfig(chunk_size=200, batch_size=32)\n\n# Embed audio file (transcribes then embeds)\ndata = embed_anything.embed_audio_file(\n    \"test_files/audio/samples_hp0.wav\",\n    audio_decoder=audio_decoder,\n    embedder=embedder,\n    text_embed_config=config,\n)\n\n# Access transcribed and embedded segments\nfor item in data:\n    print(f\"Transcribed text: {item.text}\")\n    print(f\"Metadata: {item.metadata}\")\n</code></pre>"},{"location":"references/#python.python.embed_anything--vector-database-integration","title":"Vector Database Integration","text":"<p>Store embeddings directly to a vector database without keeping them in memory:</p> <pre><code>import embed_anything\nimport os\nfrom embed_anything import EmbeddingModel, WhichModel, TextEmbedConfig\nfrom embed_anything.vectordb import PineconeAdapter\n\n# Initialize Pinecone adapter\napi_key = os.environ.get(\"PINECONE_API_KEY\")\npinecone_adapter = PineconeAdapter(api_key)\n\n# Create or use existing index\ntry:\n    pinecone_adapter.delete_index(\"my-index\")\nexcept:\n    pass\n\npinecone_adapter.create_index(\n    dimension=512,      # Embedding dimension\n    metric=\"cosine\",    # Similarity metric\n    index_name=\"my-index\"\n)\n\n# Load embedding model\nmodel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Clip, \n    \"openai/clip-vit-base-patch16\"\n)\n\n# Embed images and stream directly to Pinecone\ndata = embed_anything.embed_image_directory(\n    \"test_files\",\n    embedder=model,\n    adapter=pinecone_adapter,  # Streams to database\n)\n\n# Embeddings are now in Pinecone, not in memory\nprint(\"Embeddings stored in Pinecone!\")\n</code></pre>"},{"location":"references/#python.python.embed_anything--onnx-models-faster-inference","title":"ONNX Models (Faster Inference)","text":"<pre><code>from embed_anything import EmbeddingModel, WhichModel, ONNXModel, Dtype\n\n# Load a pre-configured ONNX model (faster, lower memory)\nmodel = EmbeddingModel.from_pretrained_onnx(\n    WhichModel.Bert,\n    model_id=ONNXModel.BGESmallENV15Q,  # Quantized BGE model\n    dtype=Dtype.Q4F16\n)\n\n# Use like any other model\ndata = embed_anything.embed_file(\"test_files/document.pdf\", embedder=model)\n</code></pre>"},{"location":"references/#python.python.embed_anything--semantic-chunking","title":"Semantic Chunking","text":"<pre><code>from embed_anything import EmbeddingModel, WhichModel, TextEmbedConfig\nimport embed_anything\n\n# Main embedding model\nmodel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Bert,\n    model_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n)\n\n# Semantic encoder for chunk boundaries\nsemantic_encoder = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Jina,\n    model_id=\"jinaai/jina-embeddings-v2-small-en\"\n)\n\n# Configure semantic chunking\nconfig = TextEmbedConfig(\n    chunk_size=1000,\n    batch_size=32,\n    splitting_strategy=\"semantic\",\n    semantic_encoder=semantic_encoder\n)\n\n# Embed with semantic chunking\ndata = embed_anything.embed_file(\"test_files/document.pdf\", embedder=model, config=config)\n</code></pre>"},{"location":"references/#python.python.embed_anything--supported-embedding-models","title":"Supported Embedding Models:","text":"<ul> <li>Text Models: BERT, Jina, Qwen3, Splade, ColBERT, Model2Vec</li> <li>Image Models: CLIP, SigLip</li> <li>Audio Models: Whisper, DistilWhisper</li> <li>Document Models: ColPali</li> <li>Rerankers: Jina Reranker, BGE Reranker, Qwen3 Reranker</li> <li>Cloud Models: OpenAI, Cohere, Gemini</li> </ul> <p>For more examples and detailed documentation, visit: https://embed-anything.com</p>"},{"location":"references/#python.python.embed_anything.Adapter","title":"<code>Adapter</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class Adapter(ABC):\n    def __init__(self, api_key: str):\n        \"\"\"\n        Initializes the Adapter object.\n\n        Args:\n            api_key: The API key for accessing the adapter.\n        \"\"\"\n\n    @abstractmethod\n    def create_index(self, dimension: int, metric: str, index_name: str, **kwargs): ...\n    \"\"\"\n    Creates an index for storing the embeddings.\n\n    Args:\n        dimension: The dimension of the embeddings.\n        metric: The metric for measuring the distance between embeddings.\n        index_name: The name of the index.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    @abstractmethod\n    def delete_index(self, index_name: str):\n        \"\"\"\n        Deletes an index.\n\n        Args:\n            index_name: The name of the index to delete.\n        \"\"\"\n\n    @abstractmethod\n    def convert(self, embeddings: List[List[EmbedData]]) -&gt; List[Dict]:\n        \"\"\"\n        Converts the embeddings to a list of dictionaries.\n\n        Args:\n            embeddings: The list of embeddings.\n\n        Returns:\n            A list of dictionaries.\n        \"\"\"\n\n    @abstractmethod\n    def upsert(self, data: List[Dict]):\n        \"\"\"\n        Upserts the data into the index.\n\n        Args:\n            data: The list of data to upsert.\n        \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.Adapter.__init__","title":"<code>__init__(api_key)</code>","text":"<p>Initializes the Adapter object.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for accessing the adapter.</p> required Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def __init__(self, api_key: str):\n    \"\"\"\n    Initializes the Adapter object.\n\n    Args:\n        api_key: The API key for accessing the adapter.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.Adapter.convert","title":"<code>convert(embeddings)</code>  <code>abstractmethod</code>","text":"<p>Converts the embeddings to a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>List[List[EmbedData]]</code> <p>The list of embeddings.</p> required <p>Returns:</p> Type Description <code>List[Dict]</code> <p>A list of dictionaries.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>@abstractmethod\ndef convert(self, embeddings: List[List[EmbedData]]) -&gt; List[Dict]:\n    \"\"\"\n    Converts the embeddings to a list of dictionaries.\n\n    Args:\n        embeddings: The list of embeddings.\n\n    Returns:\n        A list of dictionaries.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.Adapter.delete_index","title":"<code>delete_index(index_name)</code>  <code>abstractmethod</code>","text":"<p>Deletes an index.</p> <p>Parameters:</p> Name Type Description Default <code>index_name</code> <code>str</code> <p>The name of the index to delete.</p> required Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>@abstractmethod\ndef delete_index(self, index_name: str):\n    \"\"\"\n    Deletes an index.\n\n    Args:\n        index_name: The name of the index to delete.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.Adapter.upsert","title":"<code>upsert(data)</code>  <code>abstractmethod</code>","text":"<p>Upserts the data into the index.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[Dict]</code> <p>The list of data to upsert.</p> required Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>@abstractmethod\ndef upsert(self, data: List[Dict]):\n    \"\"\"\n    Upserts the data into the index.\n\n    Args:\n        data: The list of data to upsert.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.AudioDecoderModel","title":"<code>AudioDecoderModel</code>","text":"<p>Represents an audio decoder model.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>str</code> <p>The ID of the audio decoder model.</p> <code>revision</code> <code>str</code> <p>The revision of the audio decoder model.</p> <code>model_type</code> <code>str</code> <p>The type of the audio decoder model.</p> <code>quantized</code> <code>bool</code> <p>A flag indicating whether the audio decoder model is quantized or not.</p> <p>Example: <pre><code>model = embed_anything.AudioDecoderModel.from_pretrained_hf(\n    model_id=\"openai/whisper-tiny.en\",\n    revision=\"main\",\n    model_type=\"tiny-en\",\n    quantized=False\n)\n</code></pre></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class AudioDecoderModel:\n    \"\"\"\n    Represents an audio decoder model.\n\n    Attributes:\n        model_id: The ID of the audio decoder model.\n        revision: The revision of the audio decoder model.\n        model_type: The type of the audio decoder model.\n        quantized: A flag indicating whether the audio decoder model is quantized or not.\n\n    Example:\n    ```python\n\n    model = embed_anything.AudioDecoderModel.from_pretrained_hf(\n        model_id=\"openai/whisper-tiny.en\",\n        revision=\"main\",\n        model_type=\"tiny-en\",\n        quantized=False\n    )\n    ```\n    \"\"\"\n\n    model_id: str\n    revision: str\n    model_type: str\n    quantized: bool\n\n    def from_pretrained_hf(\n        model_id: str | None = None,\n        revision: str | None = None,\n        model_type: str | None = None,\n        quantized: bool | None = None,\n    ): ...\n</code></pre>"},{"location":"references/#python.python.embed_anything.ColbertModel","title":"<code>ColbertModel</code>","text":"<p>Represents the Colbert model.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class ColbertModel:\n    \"\"\"\n    Represents the Colbert model.\n    \"\"\"\n\n    def __init__(\n        self,\n        hf_model_id: str | None = None,\n        revision: str | None = None,\n        path_in_repo: str | None = None,\n    ):\n        \"\"\"\n        Initializes the ColbertModel object.\n        \"\"\"\n\n    def from_pretrained_onnx(\n        self,\n        hf_model_id: str | None = None,\n        revision: str | None = None,\n        path_in_repo: str | None = None,\n    ) -&gt; ColbertModel:\n        \"\"\"\n        Loads a pre-trained Colbert model from the Hugging Face model hub.\n\n        Attributes:\n            hf_model_id: The ID of the model from Hugging Face.\n            revision: The revision of the model.\n            path_in_repo: The path to the model in the repository.\n\n        Returns:\n            A ColbertModel object.\n        \"\"\"\n\n    def embed(\n        self, text_batch: list[str], batch_size: int | None = None, is_doc: bool = True\n    ) -&gt; list[EmbedData]:\n        \"\"\"\n        Embeds the given text and returns a list of EmbedData objects.\n        \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.ColbertModel.__init__","title":"<code>__init__(hf_model_id=None, revision=None, path_in_repo=None)</code>","text":"<p>Initializes the ColbertModel object.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def __init__(\n    self,\n    hf_model_id: str | None = None,\n    revision: str | None = None,\n    path_in_repo: str | None = None,\n):\n    \"\"\"\n    Initializes the ColbertModel object.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.ColbertModel.embed","title":"<code>embed(text_batch, batch_size=None, is_doc=True)</code>","text":"<p>Embeds the given text and returns a list of EmbedData objects.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed(\n    self, text_batch: list[str], batch_size: int | None = None, is_doc: bool = True\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given text and returns a list of EmbedData objects.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.ColbertModel.from_pretrained_onnx","title":"<code>from_pretrained_onnx(hf_model_id=None, revision=None, path_in_repo=None)</code>","text":"<p>Loads a pre-trained Colbert model from the Hugging Face model hub.</p> <p>Attributes:</p> Name Type Description <code>hf_model_id</code> <p>The ID of the model from Hugging Face.</p> <code>revision</code> <p>The revision of the model.</p> <code>path_in_repo</code> <p>The path to the model in the repository.</p> <p>Returns:</p> Type Description <code>ColbertModel</code> <p>A ColbertModel object.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def from_pretrained_onnx(\n    self,\n    hf_model_id: str | None = None,\n    revision: str | None = None,\n    path_in_repo: str | None = None,\n) -&gt; ColbertModel:\n    \"\"\"\n    Loads a pre-trained Colbert model from the Hugging Face model hub.\n\n    Attributes:\n        hf_model_id: The ID of the model from Hugging Face.\n        revision: The revision of the model.\n        path_in_repo: The path to the model in the repository.\n\n    Returns:\n        A ColbertModel object.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.ColpaliModel","title":"<code>ColpaliModel</code>","text":"<p>Represents the Colpali model.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class ColpaliModel:\n    \"\"\"\n    Represents the Colpali model.\n    \"\"\"\n\n    def __init__(self, model_id: str, revision: str | None = None):\n        \"\"\"\n        Initializes the ColpaliModel object.\n\n        Args:\n            model_id: The ID of the model from Hugging Face.\n            revision: The revision of the model.\n        \"\"\"\n\n    def from_pretrained(model_id: str, revision: str | None = None) -&gt; ColpaliModel:\n        \"\"\"\n        Loads a pre-trained Colpali model from the Hugging Face model hub.\n\n        Args:\n            model_id: The ID of the model from Hugging Face.\n            revision: The revision of the model.\n\n        Returns:\n            A ColpaliModel object.\n        \"\"\"\n\n    def from_pretrained_onnx(\n        model_id: str, revision: str | None = None\n    ) -&gt; ColpaliModel:\n        \"\"\"\n        Loads a pre-trained Colpali model from the Hugging Face model hub.\n\n        Args:\n            model_id: The ID of the model from Hugging Face.\n            revision: The revision of the model.\n\n        Returns:\n            A ColpaliModel object.\n        \"\"\"\n\n    def embed_file(self, file_path: str, batch_size: int | None = 1) -&gt; list[EmbedData]:\n        \"\"\"\n        Embeds the given pdf file and returns a list of EmbedData objects for each page in the file This first convert the pdf file into images and then embed each image.\n\n        Args:\n            file_path: The path to the pdf file to embed.\n            batch_size: The batch size for processing the embeddings. Default is 1.\n\n        Returns:\n            A list of EmbedData objects for each page in the file.\n        \"\"\"\n\n    def embed_query(self, query: str) -&gt; list[EmbedData]:\n        \"\"\"\n        Embeds the given query and returns a list of EmbedData objects.\n\n        Args:\n            query: The query to embed.\n\n        Returns:\n            A list of EmbedData objects.\n\n        \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.ColpaliModel.__init__","title":"<code>__init__(model_id, revision=None)</code>","text":"<p>Initializes the ColpaliModel object.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The ID of the model from Hugging Face.</p> required <code>revision</code> <code>str | None</code> <p>The revision of the model.</p> <code>None</code> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def __init__(self, model_id: str, revision: str | None = None):\n    \"\"\"\n    Initializes the ColpaliModel object.\n\n    Args:\n        model_id: The ID of the model from Hugging Face.\n        revision: The revision of the model.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.ColpaliModel.embed_file","title":"<code>embed_file(file_path, batch_size=1)</code>","text":"<p>Embeds the given pdf file and returns a list of EmbedData objects for each page in the file This first convert the pdf file into images and then embed each image.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the pdf file to embed.</p> required <code>batch_size</code> <code>int | None</code> <p>The batch size for processing the embeddings. Default is 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects for each page in the file.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_file(self, file_path: str, batch_size: int | None = 1) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given pdf file and returns a list of EmbedData objects for each page in the file This first convert the pdf file into images and then embed each image.\n\n    Args:\n        file_path: The path to the pdf file to embed.\n        batch_size: The batch size for processing the embeddings. Default is 1.\n\n    Returns:\n        A list of EmbedData objects for each page in the file.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.ColpaliModel.embed_query","title":"<code>embed_query(query)</code>","text":"<p>Embeds the given query and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to embed.</p> required <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_query(self, query: str) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given query and returns a list of EmbedData objects.\n\n    Args:\n        query: The query to embed.\n\n    Returns:\n        A list of EmbedData objects.\n\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.ColpaliModel.from_pretrained","title":"<code>from_pretrained(model_id, revision=None)</code>","text":"<p>Loads a pre-trained Colpali model from the Hugging Face model hub.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The ID of the model from Hugging Face.</p> required <code>revision</code> <code>str | None</code> <p>The revision of the model.</p> <code>None</code> <p>Returns:</p> Type Description <code>ColpaliModel</code> <p>A ColpaliModel object.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def from_pretrained(model_id: str, revision: str | None = None) -&gt; ColpaliModel:\n    \"\"\"\n    Loads a pre-trained Colpali model from the Hugging Face model hub.\n\n    Args:\n        model_id: The ID of the model from Hugging Face.\n        revision: The revision of the model.\n\n    Returns:\n        A ColpaliModel object.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.ColpaliModel.from_pretrained_onnx","title":"<code>from_pretrained_onnx(model_id, revision=None)</code>","text":"<p>Loads a pre-trained Colpali model from the Hugging Face model hub.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The ID of the model from Hugging Face.</p> required <code>revision</code> <code>str | None</code> <p>The revision of the model.</p> <code>None</code> <p>Returns:</p> Type Description <code>ColpaliModel</code> <p>A ColpaliModel object.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def from_pretrained_onnx(\n    model_id: str, revision: str | None = None\n) -&gt; ColpaliModel:\n    \"\"\"\n    Loads a pre-trained Colpali model from the Hugging Face model hub.\n\n    Args:\n        model_id: The ID of the model from Hugging Face.\n        revision: The revision of the model.\n\n    Returns:\n        A ColpaliModel object.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.DocumentRank","title":"<code>DocumentRank</code>","text":"<p>Represents the rank of a document.</p> <p>Attributes:</p> Name Type Description <code>document</code> <code>str</code> <p>The document to rank.</p> <code>relevance_score</code> <code>float</code> <p>The relevance score of the document.</p> <code>rank</code> <code>int</code> <p>The rank of the document.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class DocumentRank:\n    \"\"\"\n    Represents the rank of a document.\n\n    Attributes:\n        document: The document to rank.\n        relevance_score: The relevance score of the document.\n        rank: The rank of the document.\n    \"\"\"\n\n    document: str\n    relevance_score: float\n    rank: int\n</code></pre>"},{"location":"references/#python.python.embed_anything.Dtype","title":"<code>Dtype</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Represents the data type of the model.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class Dtype(Enum):\n    \"\"\"\n    Represents the data type of the model.\n    \"\"\"\n\n    F16 = \"F16\"\n    INT8 = \"INT8\"\n    Q4 = \"Q4\"\n    UINT8 = \"UINT8\"\n    BNB4 = \"BNB4\"\n    Q4F16 = \"Q4F16\"\n    BF16 = \"BF16\"\n    F32 = \"F32\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbedData","title":"<code>EmbedData</code>","text":"<p>Represents the data of an embedded file.</p> <p>Attributes:</p> Name Type Description <code>embedding</code> <code>list[float]</code> <p>The embedding of the file.</p> <code>text</code> <code>str</code> <p>The text for which the embedding is generated for.</p> <code>metadata</code> <code>dict[str, str]</code> <p>Additional metadata associated with the embedding.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class EmbedData:\n    \"\"\"Represents the data of an embedded file.\n\n    Attributes:\n        embedding: The embedding of the file.\n        text: The text for which the embedding is generated for.\n        metadata: Additional metadata associated with the embedding.\n    \"\"\"\n\n    def __init__(self, embedding: list[float], text: str, metadata: dict[str, str]):\n        self.embedding = embedding\n        self.text = text\n        self.metadata = metadata\n    embedding: list[float]\n    text: str\n    metadata: dict[str, str]\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbeddingModel","title":"<code>EmbeddingModel</code>","text":"<p>Represents an embedding model.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class EmbeddingModel:\n    \"\"\"\n    Represents an embedding model.\n    \"\"\"\n\n    def from_pretrained_hf(\n        model_id: str,\n        revision: str | None = None,\n        token: str | None = None,\n        dtype: Dtype | None = None,\n    ) -&gt; EmbeddingModel:\n        \"\"\"\n        Loads an embedding model from the Hugging Face model hub.\n\n        Attributes:\n            model_id: The ID of the model.\n            revision: The revision of the model.\n            token: The Hugging Face token.\n            dtype: The dtype of the model.\n        Returns:\n            An EmbeddingModel object.\n\n        Example:\n        ```python\n        model = EmbeddingModel.from_pretrained_hf(\n            model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n            revision=\"main\"\n        )\n        ```\n\n        \"\"\"\n\n    def from_pretrained_cloud(\n        model: WhichModel, model_id: str, api_key: str | None = None\n    ) -&gt; EmbeddingModel:\n        \"\"\"\n        Loads an embedding model from a cloud-based service.\n\n        Attributes:\n            model (WhichModel): The cloud service to use. Currently supports WhichModel.OpenAI and WhichModel.Cohere.\n            model_id (str): The ID of the model to use.\n\n                - For OpenAI, see available models at https://platform.openai.com/docs/guides/embeddings/embedding-models\n                - For Cohere, see available models at https://docs.cohere.com/docs/cohere-embed\n                - For CohereVision, see available models at https://docs.cohere.com/docs/cohere-embed\n            api_key (str | None, optional): The API key for accessing the model. If not provided, it is taken from the environment variable:\n\n                - For OpenAI: OPENAI_API_KEY\n                - For Cohere: CO_API_KEY\n\n        Returns:\n            EmbeddingModel: An initialized EmbeddingModel object.\n\n        Raises:\n            ValueError: If an unsupported model is specified.\n\n        Example:\n        ```python\n        # Using Cohere\n        model = EmbeddingModel.from_pretrained_cloud(\n            model=WhichModel.Cohere,\n            model_id=\"embed-english-v3.0\"\n        )\n\n        # Using OpenAI\n        model = EmbeddingModel.from_pretrained_cloud(\n            model=WhichModel.OpenAI,\n            model_id=\"text-embedding-3-small\"\n        )\n        ```\n        \"\"\"\n\n    def from_pretrained_onnx(\n        model: WhichModel,\n        model_name: Optional[ONNXModel] | None = None,\n        hf_model_id: Optional[str] | None = None,\n        revision: Optional[str] | None = None,\n        dtype: Optional[Dtype] | None = None,\n        path_in_repo: Optional[str] | None = None,\n    ) -&gt; EmbeddingModel:\n        \"\"\"\n        Loads an ONNX embedding model.\n\n        Args:\n            model (WhichModel): The architecture of the embedding model to use.\n            model_name (ONNXModel | None, optional): The name of the model. Defaults to None.\n            hf_model_id (str | None, optional): The ID of the model from Hugging Face. Defaults to None.\n            revision (str | None, optional): The revision of the model. Defaults to None.\n            dtype (Dtype | None, optional): The dtype of the model. Defaults to None.\n            path_in_repo (str | None, optional): The path to the model in the repository. Defaults to None.\n        Returns:\n            EmbeddingModel: An initialized EmbeddingModel object.\n\n        Atleast one of the following arguments must be provided:\n            - model_name\n            - hf_model_id\n\n        If hf_model_id is provided, dtype is ignored and the path_in_repo has to be provided pointing to the model file in the repository.\n        If model_name is provided, dtype is used to determine the model file to load.\n\n        Example:\n        ```python\n        model = EmbeddingModel.from_pretrained_onnx(\n            model=WhichModel.Bert,\n            model_name=ONNXModel.BGESmallENV15Q,\n            dtype=Dtype.Q4F16\n        )\n\n        model = EmbeddingModel.from_pretrained_onnx(\n            model=WhichModel.Bert,\n            hf_model_id=\"jinaai/jina-embeddings-v3\",\n            path_in_repo=\"onnx/model_fp16.onnx\"\n        )\n        ```\n\n        Note:\n        This method loads a pre-trained model in ONNX format, which can offer improved inference speed\n        compared to standard PyTorch models. ONNX models are particularly useful for deployment\n        scenarios where performance is critical.\n        \"\"\"\n\n    def embed_file(\n        self,\n        file_path: str,\n        config: TextEmbedConfig | None = None,\n        adapter: Adapter | None = None,\n    ) -&gt; list[EmbedData]:\n        \"\"\"\n        Embeds the given file and returns a list of EmbedData objects.\n\n        Args:\n            file_path: The path to the file to embed.\n            config: The configuration for the embedding.\n            adapter: The adapter for the embedding.\n\n        Returns:\n            A list of EmbedData objects.\n        \"\"\"\n\n    def embed_files_batch(\n        self,\n        files: list[str],\n        config: TextEmbedConfig | None = None,\n        adapter: Adapter | None = None,\n    ) -&gt; list[EmbedData]:\n        \"\"\"\n        Embeds the given files and returns a list of EmbedData objects.\n\n        Args:\n            files: The list of files to embed.\n            config: The configuration for the embedding.\n            adapter: The adapter for the embedding.\n\n        Returns:\n            A list of EmbedData objects.\n        \"\"\"\n\n    def embed_audio_file(\n        self,\n        audio_file: str,\n        audio_decoder: AudioDecoderModel,\n        config: TextEmbedConfig | None = None,\n    ) -&gt; list[EmbedData]:\n        \"\"\"\n        Embeds the given audio file and returns a list of EmbedData objects.\n\n        Args:\n            audio_file: The path to the audio file to embed.\n            audio_decoder: The audio decoder for the audio file.\n            config: The configuration for the embedding.\n\n        Returns:\n            A list of EmbedData objects.\n        \"\"\"\n\n    def embed_query(\n        self,\n        query: list[str],\n        config: TextEmbedConfig | None = None,\n    ) -&gt; list[EmbedData]:\n        \"\"\"\n        Embeds the given list of queries and returns a list of EmbedData objects.\n\n        Args:\n            query: The list of queries to embed.\n            config: The configuration for the embedding.\n\n        Returns:\n            A list of EmbedData objects.\n        \"\"\"\n\n    def embed_webpage(\n        self,\n        url: str,\n        config: TextEmbedConfig | None = None,\n        adapter: Adapter | None = None,\n    ) -&gt; list[EmbedData]:\n        \"\"\"\n        Embeds the given webpage and returns a list of EmbedData objects.\n\n        Args:\n            url: The URL of the webpage to embed.\n            config: The configuration for the embedding.\n            adapter: The adapter for the embedding.\n\n        Returns:\n            A list of EmbedData objects.\n        \"\"\"\n\n    def embed_directory(\n        self,\n        directory: str,\n        config: TextEmbedConfig | None = None,\n        adapter: Adapter | None = None,\n    ) -&gt; list[EmbedData]:\n        \"\"\"\n        Embeds the given directory and returns a list of EmbedData objects.\n\n        Args:\n            directory: The path to the directory to embed.\n            config: The configuration for the embedding.\n            adapter: The adapter for the embedding.\n\n        Returns:\n            A list of EmbedData objects.\n        \"\"\"\n\n    def embed_directory_stream(\n        self,\n        directory: str,\n        config: TextEmbedConfig | None = None,\n        adapter: Adapter | None = None,\n    ) -&gt; list[EmbedData]:\n        \"\"\"\n        Embeds the given directory and returns a list of EmbedData objects.\n\n        Args:\n            directory: The path to the directory to embed.\n            config: The configuration for the embedding.\n            adapter: The adapter for the embedding.\n\n        Returns:\n            A list of EmbedData objects.\n        \"\"\"\n\n    def embed_webpage(\n        self,\n        url: str,\n        config: TextEmbedConfig | None = None,\n        adapter: Adapter | None = None,\n    ) -&gt; list[EmbedData]:\n        \"\"\"\n        Embeds the given webpage and returns a list of EmbedData objects.\n\n        Args:\n            url: The URL of the webpage to embed.\n            config: The configuration for the embedding.\n            adapter: The adapter for the embedding.\n\n        Returns:\n            A list of EmbedData objects.\n        \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbeddingModel.embed_audio_file","title":"<code>embed_audio_file(audio_file, audio_decoder, config=None)</code>","text":"<p>Embeds the given audio file and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>str</code> <p>The path to the audio file to embed.</p> required <code>audio_decoder</code> <code>AudioDecoderModel</code> <p>The audio decoder for the audio file.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_audio_file(\n    self,\n    audio_file: str,\n    audio_decoder: AudioDecoderModel,\n    config: TextEmbedConfig | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given audio file and returns a list of EmbedData objects.\n\n    Args:\n        audio_file: The path to the audio file to embed.\n        audio_decoder: The audio decoder for the audio file.\n        config: The configuration for the embedding.\n\n    Returns:\n        A list of EmbedData objects.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbeddingModel.embed_directory","title":"<code>embed_directory(directory, config=None, adapter=None)</code>","text":"<p>Embeds the given directory and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>The path to the directory to embed.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding.</p> <code>None</code> <code>adapter</code> <code>Adapter | None</code> <p>The adapter for the embedding.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_directory(\n    self,\n    directory: str,\n    config: TextEmbedConfig | None = None,\n    adapter: Adapter | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given directory and returns a list of EmbedData objects.\n\n    Args:\n        directory: The path to the directory to embed.\n        config: The configuration for the embedding.\n        adapter: The adapter for the embedding.\n\n    Returns:\n        A list of EmbedData objects.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbeddingModel.embed_directory_stream","title":"<code>embed_directory_stream(directory, config=None, adapter=None)</code>","text":"<p>Embeds the given directory and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>The path to the directory to embed.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding.</p> <code>None</code> <code>adapter</code> <code>Adapter | None</code> <p>The adapter for the embedding.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_directory_stream(\n    self,\n    directory: str,\n    config: TextEmbedConfig | None = None,\n    adapter: Adapter | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given directory and returns a list of EmbedData objects.\n\n    Args:\n        directory: The path to the directory to embed.\n        config: The configuration for the embedding.\n        adapter: The adapter for the embedding.\n\n    Returns:\n        A list of EmbedData objects.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbeddingModel.embed_file","title":"<code>embed_file(file_path, config=None, adapter=None)</code>","text":"<p>Embeds the given file and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the file to embed.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding.</p> <code>None</code> <code>adapter</code> <code>Adapter | None</code> <p>The adapter for the embedding.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_file(\n    self,\n    file_path: str,\n    config: TextEmbedConfig | None = None,\n    adapter: Adapter | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given file and returns a list of EmbedData objects.\n\n    Args:\n        file_path: The path to the file to embed.\n        config: The configuration for the embedding.\n        adapter: The adapter for the embedding.\n\n    Returns:\n        A list of EmbedData objects.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbeddingModel.embed_files_batch","title":"<code>embed_files_batch(files, config=None, adapter=None)</code>","text":"<p>Embeds the given files and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list[str]</code> <p>The list of files to embed.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding.</p> <code>None</code> <code>adapter</code> <code>Adapter | None</code> <p>The adapter for the embedding.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_files_batch(\n    self,\n    files: list[str],\n    config: TextEmbedConfig | None = None,\n    adapter: Adapter | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given files and returns a list of EmbedData objects.\n\n    Args:\n        files: The list of files to embed.\n        config: The configuration for the embedding.\n        adapter: The adapter for the embedding.\n\n    Returns:\n        A list of EmbedData objects.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbeddingModel.embed_query","title":"<code>embed_query(query, config=None)</code>","text":"<p>Embeds the given list of queries and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>list[str]</code> <p>The list of queries to embed.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_query(\n    self,\n    query: list[str],\n    config: TextEmbedConfig | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given list of queries and returns a list of EmbedData objects.\n\n    Args:\n        query: The list of queries to embed.\n        config: The configuration for the embedding.\n\n    Returns:\n        A list of EmbedData objects.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbeddingModel.embed_webpage","title":"<code>embed_webpage(url, config=None, adapter=None)</code>","text":"<p>Embeds the given webpage and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the webpage to embed.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding.</p> <code>None</code> <code>adapter</code> <code>Adapter | None</code> <p>The adapter for the embedding.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_webpage(\n    self,\n    url: str,\n    config: TextEmbedConfig | None = None,\n    adapter: Adapter | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given webpage and returns a list of EmbedData objects.\n\n    Args:\n        url: The URL of the webpage to embed.\n        config: The configuration for the embedding.\n        adapter: The adapter for the embedding.\n\n    Returns:\n        A list of EmbedData objects.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbeddingModel.from_pretrained_cloud","title":"<code>from_pretrained_cloud(model, model_id, api_key=None)</code>","text":"<p>Loads an embedding model from a cloud-based service.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>WhichModel</code> <p>The cloud service to use. Currently supports WhichModel.OpenAI and WhichModel.Cohere.</p> <code>model_id</code> <code>str</code> <p>The ID of the model to use.</p> <ul> <li>For OpenAI, see available models at https://platform.openai.com/docs/guides/embeddings/embedding-models</li> <li>For Cohere, see available models at https://docs.cohere.com/docs/cohere-embed</li> <li>For CohereVision, see available models at https://docs.cohere.com/docs/cohere-embed</li> </ul> <code>api_key</code> <code>str | None</code> <p>The API key for accessing the model. If not provided, it is taken from the environment variable:</p> <ul> <li>For OpenAI: OPENAI_API_KEY</li> <li>For Cohere: CO_API_KEY</li> </ul> <p>Returns:</p> Name Type Description <code>EmbeddingModel</code> <code>EmbeddingModel</code> <p>An initialized EmbeddingModel object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported model is specified.</p> <p>Example: <pre><code># Using Cohere\nmodel = EmbeddingModel.from_pretrained_cloud(\n    model=WhichModel.Cohere,\n    model_id=\"embed-english-v3.0\"\n)\n\n# Using OpenAI\nmodel = EmbeddingModel.from_pretrained_cloud(\n    model=WhichModel.OpenAI,\n    model_id=\"text-embedding-3-small\"\n)\n</code></pre></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def from_pretrained_cloud(\n    model: WhichModel, model_id: str, api_key: str | None = None\n) -&gt; EmbeddingModel:\n    \"\"\"\n    Loads an embedding model from a cloud-based service.\n\n    Attributes:\n        model (WhichModel): The cloud service to use. Currently supports WhichModel.OpenAI and WhichModel.Cohere.\n        model_id (str): The ID of the model to use.\n\n            - For OpenAI, see available models at https://platform.openai.com/docs/guides/embeddings/embedding-models\n            - For Cohere, see available models at https://docs.cohere.com/docs/cohere-embed\n            - For CohereVision, see available models at https://docs.cohere.com/docs/cohere-embed\n        api_key (str | None, optional): The API key for accessing the model. If not provided, it is taken from the environment variable:\n\n            - For OpenAI: OPENAI_API_KEY\n            - For Cohere: CO_API_KEY\n\n    Returns:\n        EmbeddingModel: An initialized EmbeddingModel object.\n\n    Raises:\n        ValueError: If an unsupported model is specified.\n\n    Example:\n    ```python\n    # Using Cohere\n    model = EmbeddingModel.from_pretrained_cloud(\n        model=WhichModel.Cohere,\n        model_id=\"embed-english-v3.0\"\n    )\n\n    # Using OpenAI\n    model = EmbeddingModel.from_pretrained_cloud(\n        model=WhichModel.OpenAI,\n        model_id=\"text-embedding-3-small\"\n    )\n    ```\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbeddingModel.from_pretrained_hf","title":"<code>from_pretrained_hf(model_id, revision=None, token=None, dtype=None)</code>","text":"<p>Loads an embedding model from the Hugging Face model hub.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <p>The ID of the model.</p> <code>revision</code> <p>The revision of the model.</p> <code>token</code> <p>The Hugging Face token.</p> <code>dtype</code> <p>The dtype of the model.</p> <p>Returns:     An EmbeddingModel object.</p> <p>Example: <pre><code>model = EmbeddingModel.from_pretrained_hf(\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    revision=\"main\"\n)\n</code></pre></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def from_pretrained_hf(\n    model_id: str,\n    revision: str | None = None,\n    token: str | None = None,\n    dtype: Dtype | None = None,\n) -&gt; EmbeddingModel:\n    \"\"\"\n    Loads an embedding model from the Hugging Face model hub.\n\n    Attributes:\n        model_id: The ID of the model.\n        revision: The revision of the model.\n        token: The Hugging Face token.\n        dtype: The dtype of the model.\n    Returns:\n        An EmbeddingModel object.\n\n    Example:\n    ```python\n    model = EmbeddingModel.from_pretrained_hf(\n        model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n        revision=\"main\"\n    )\n    ```\n\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbeddingModel.from_pretrained_onnx","title":"<code>from_pretrained_onnx(model, model_name=None, hf_model_id=None, revision=None, dtype=None, path_in_repo=None)</code>","text":"<p>Loads an ONNX embedding model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>WhichModel</code> <p>The architecture of the embedding model to use.</p> required <code>model_name</code> <code>ONNXModel | None</code> <p>The name of the model. Defaults to None.</p> <code>None</code> <code>hf_model_id</code> <code>str | None</code> <p>The ID of the model from Hugging Face. Defaults to None.</p> <code>None</code> <code>revision</code> <code>str | None</code> <p>The revision of the model. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>Dtype | None</code> <p>The dtype of the model. Defaults to None.</p> <code>None</code> <code>path_in_repo</code> <code>str | None</code> <p>The path to the model in the repository. Defaults to None.</p> <code>None</code> <p>Returns:     EmbeddingModel: An initialized EmbeddingModel object.</p> Atleast one of the following arguments must be provided <ul> <li>model_name</li> <li>hf_model_id</li> </ul> <p>If hf_model_id is provided, dtype is ignored and the path_in_repo has to be provided pointing to the model file in the repository. If model_name is provided, dtype is used to determine the model file to load.</p> <p>Example: <pre><code>model = EmbeddingModel.from_pretrained_onnx(\n    model=WhichModel.Bert,\n    model_name=ONNXModel.BGESmallENV15Q,\n    dtype=Dtype.Q4F16\n)\n\nmodel = EmbeddingModel.from_pretrained_onnx(\n    model=WhichModel.Bert,\n    hf_model_id=\"jinaai/jina-embeddings-v3\",\n    path_in_repo=\"onnx/model_fp16.onnx\"\n)\n</code></pre></p> <p>Note: This method loads a pre-trained model in ONNX format, which can offer improved inference speed compared to standard PyTorch models. ONNX models are particularly useful for deployment scenarios where performance is critical.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def from_pretrained_onnx(\n    model: WhichModel,\n    model_name: Optional[ONNXModel] | None = None,\n    hf_model_id: Optional[str] | None = None,\n    revision: Optional[str] | None = None,\n    dtype: Optional[Dtype] | None = None,\n    path_in_repo: Optional[str] | None = None,\n) -&gt; EmbeddingModel:\n    \"\"\"\n    Loads an ONNX embedding model.\n\n    Args:\n        model (WhichModel): The architecture of the embedding model to use.\n        model_name (ONNXModel | None, optional): The name of the model. Defaults to None.\n        hf_model_id (str | None, optional): The ID of the model from Hugging Face. Defaults to None.\n        revision (str | None, optional): The revision of the model. Defaults to None.\n        dtype (Dtype | None, optional): The dtype of the model. Defaults to None.\n        path_in_repo (str | None, optional): The path to the model in the repository. Defaults to None.\n    Returns:\n        EmbeddingModel: An initialized EmbeddingModel object.\n\n    Atleast one of the following arguments must be provided:\n        - model_name\n        - hf_model_id\n\n    If hf_model_id is provided, dtype is ignored and the path_in_repo has to be provided pointing to the model file in the repository.\n    If model_name is provided, dtype is used to determine the model file to load.\n\n    Example:\n    ```python\n    model = EmbeddingModel.from_pretrained_onnx(\n        model=WhichModel.Bert,\n        model_name=ONNXModel.BGESmallENV15Q,\n        dtype=Dtype.Q4F16\n    )\n\n    model = EmbeddingModel.from_pretrained_onnx(\n        model=WhichModel.Bert,\n        hf_model_id=\"jinaai/jina-embeddings-v3\",\n        path_in_repo=\"onnx/model_fp16.onnx\"\n    )\n    ```\n\n    Note:\n    This method loads a pre-trained model in ONNX format, which can offer improved inference speed\n    compared to standard PyTorch models. ONNX models are particularly useful for deployment\n    scenarios where performance is critical.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.ImageEmbedConfig","title":"<code>ImageEmbedConfig</code>","text":"<p>Represents the configuration for the Image Embedding model.</p> <p>Attributes:</p> Name Type Description <code>buffer_size</code> <code>int | None</code> <p>The buffer size for the Image Embedding model. Default is 100.</p> <code>batch_size</code> <code>int | None</code> <p>The batch size for processing the embeddings. Default is 32. Based on the memory, you can increase or decrease the batch size.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class ImageEmbedConfig:\n    \"\"\"\n    Represents the configuration for the Image Embedding model.\n\n    Attributes:\n        buffer_size: The buffer size for the Image Embedding model. Default is 100.\n        batch_size: The batch size for processing the embeddings. Default is 32. Based on the memory, you can increase or decrease the batch size.\n    \"\"\"\n\n    def __init__(self, buffer_size: int | None = None, batch_size: int | None = None):\n        self.buffer_size = buffer_size\n        self.batch_size = batch_size\n    buffer_size: int | None\n    batch_size: int | None\n</code></pre>"},{"location":"references/#python.python.embed_anything.ONNXModel","title":"<code>ONNXModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum representing various ONNX models.</p> <pre><code>| Enum Variant                     | Description                                      |\n|----------------------------------|--------------------------------------------------|\n| `AllMiniLML6V2`                  | sentence-transformers/all-MiniLM-L6-v2           |\n| `AllMiniLML6V2Q`                 | Quantized sentence-transformers/all-MiniLM-L6-v2 |\n| `AllMiniLML12V2`                 | sentence-transformers/all-MiniLM-L12-v2          |\n| `AllMiniLML12V2Q`                | Quantized sentence-transformers/all-MiniLM-L12-v2|\n| `ModernBERTBase`                 | nomic-ai/modernbert-embed-base                   |\n| `ModernBERTLarge`                | nomic-ai/modernbert-embed-large                  |\n| `BGEBaseENV15`                   | BAAI/bge-base-en-v1.5                            |\n| `BGEBaseENV15Q`                  | Quantized BAAI/bge-base-en-v1.5                  |\n| `BGELargeENV15`                  | BAAI/bge-large-en-v1.5                           |\n| `BGELargeENV15Q`                 | Quantized BAAI/bge-large-en-v1.5                 |\n| `BGESmallENV15`                  | BAAI/bge-small-en-v1.5 - Default                 |\n| `BGESmallENV15Q`                 | Quantized BAAI/bge-small-en-v1.5                 |\n| `NomicEmbedTextV1`               | nomic-ai/nomic-embed-text-v1                     |\n| `NomicEmbedTextV15`              | nomic-ai/nomic-embed-text-v1.5                   |\n| `NomicEmbedTextV15Q`             | Quantized nomic-ai/nomic-embed-text-v1.5         |\n| `ParaphraseMLMiniLML12V2`        | sentence-transformers/paraphrase-MiniLM-L6-v2    |\n| `ParaphraseMLMiniLML12V2Q`       | Quantized sentence-transformers/paraphrase-MiniLM-L6-v2 |\n| `ParaphraseMLMpnetBaseV2`        | sentence-transformers/paraphrase-mpnet-base-v2   |\n| `BGESmallZHV15`                  | BAAI/bge-small-zh-v1.5                           |\n| `MultilingualE5Small`            | intfloat/multilingual-e5-small                   |\n| `MultilingualE5Base`             | intfloat/multilingual-e5-base                    |\n| `MultilingualE5Large`            | intfloat/multilingual-e5-large                   |\n| `MxbaiEmbedLargeV1`              | mixedbread-ai/mxbai-embed-large-v1               |\n| `MxbaiEmbedLargeV1Q`             | Quantized mixedbread-ai/mxbai-embed-large-v1     |\n| `GTEBaseENV15`                   | Alibaba-NLP/gte-base-en-v1.5                     |\n| `GTEBaseENV15Q`                  | Quantized Alibaba-NLP/gte-base-en-v1.5           |\n| `GTELargeENV15`                  | Alibaba-NLP/gte-large-en-v1.5                    |\n| `GTELargeENV15Q`                 | Quantized Alibaba-NLP/gte-large-en-v1.5          |\n| `JINAV2SMALLEN`                  | jinaai/jina-embeddings-v2-small-en               |\n| `JINAV2BASEEN`                   | jinaai/jina-embeddings-v2-base-en                |\n| `JINAV3`                         | jinaai/jina-embeddings-v3                        |\n| `SPLADEPPENV1`                   | prithivida/Splade_PP_en_v1                      |\n| `SPLADEPPENV2`                   | prithivida/Splade_PP_en_v2                      |\n| `ModernBERTBase`                 | nomic-ai/modernbert-embed-base                   |\n</code></pre> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class ONNXModel(Enum):\n    \"\"\"\n    Enum representing various ONNX models.\n\n    ```markdown\n    | Enum Variant                     | Description                                      |\n    |----------------------------------|--------------------------------------------------|\n    | `AllMiniLML6V2`                  | sentence-transformers/all-MiniLM-L6-v2           |\n    | `AllMiniLML6V2Q`                 | Quantized sentence-transformers/all-MiniLM-L6-v2 |\n    | `AllMiniLML12V2`                 | sentence-transformers/all-MiniLM-L12-v2          |\n    | `AllMiniLML12V2Q`                | Quantized sentence-transformers/all-MiniLM-L12-v2|\n    | `ModernBERTBase`                 | nomic-ai/modernbert-embed-base                   |\n    | `ModernBERTLarge`                | nomic-ai/modernbert-embed-large                  |\n    | `BGEBaseENV15`                   | BAAI/bge-base-en-v1.5                            |\n    | `BGEBaseENV15Q`                  | Quantized BAAI/bge-base-en-v1.5                  |\n    | `BGELargeENV15`                  | BAAI/bge-large-en-v1.5                           |\n    | `BGELargeENV15Q`                 | Quantized BAAI/bge-large-en-v1.5                 |\n    | `BGESmallENV15`                  | BAAI/bge-small-en-v1.5 - Default                 |\n    | `BGESmallENV15Q`                 | Quantized BAAI/bge-small-en-v1.5                 |\n    | `NomicEmbedTextV1`               | nomic-ai/nomic-embed-text-v1                     |\n    | `NomicEmbedTextV15`              | nomic-ai/nomic-embed-text-v1.5                   |\n    | `NomicEmbedTextV15Q`             | Quantized nomic-ai/nomic-embed-text-v1.5         |\n    | `ParaphraseMLMiniLML12V2`        | sentence-transformers/paraphrase-MiniLM-L6-v2    |\n    | `ParaphraseMLMiniLML12V2Q`       | Quantized sentence-transformers/paraphrase-MiniLM-L6-v2 |\n    | `ParaphraseMLMpnetBaseV2`        | sentence-transformers/paraphrase-mpnet-base-v2   |\n    | `BGESmallZHV15`                  | BAAI/bge-small-zh-v1.5                           |\n    | `MultilingualE5Small`            | intfloat/multilingual-e5-small                   |\n    | `MultilingualE5Base`             | intfloat/multilingual-e5-base                    |\n    | `MultilingualE5Large`            | intfloat/multilingual-e5-large                   |\n    | `MxbaiEmbedLargeV1`              | mixedbread-ai/mxbai-embed-large-v1               |\n    | `MxbaiEmbedLargeV1Q`             | Quantized mixedbread-ai/mxbai-embed-large-v1     |\n    | `GTEBaseENV15`                   | Alibaba-NLP/gte-base-en-v1.5                     |\n    | `GTEBaseENV15Q`                  | Quantized Alibaba-NLP/gte-base-en-v1.5           |\n    | `GTELargeENV15`                  | Alibaba-NLP/gte-large-en-v1.5                    |\n    | `GTELargeENV15Q`                 | Quantized Alibaba-NLP/gte-large-en-v1.5          |\n    | `JINAV2SMALLEN`                  | jinaai/jina-embeddings-v2-small-en               |\n    | `JINAV2BASEEN`                   | jinaai/jina-embeddings-v2-base-en                |\n    | `JINAV3`                         | jinaai/jina-embeddings-v3                        |\n    | `SPLADEPPENV1`                   | prithivida/Splade_PP_en_v1                      |\n    | `SPLADEPPENV2`                   | prithivida/Splade_PP_en_v2                      |\n    | `ModernBERTBase`                 | nomic-ai/modernbert-embed-base                   |\n    ```\n    \"\"\"\n\n    AllMiniLML6V2 = \"AllMiniLML6V2\"\n\n    AllMiniLML6V2Q = \"AllMiniLML6V2Q\"\n\n    AllMiniLML12V2 = \"AllMiniLML12V2\"\n\n    AllMiniLML12V2Q = \"AllMiniLML12V2Q\"\n\n    ModernBERTBase = \"ModernBERTBase\"\n\n    ModernBERTLarge = \"ModernBERTLarge\"\n\n    BGEBaseENV15 = \"BGEBaseENV15\"\n\n    BGEBaseENV15Q = \"BGEBaseENV15Q\"\n\n    BGELargeENV15 = \"BGELargeENV15\"\n\n    BGELargeENV15Q = \"BGELargeENV15Q\"\n\n    BGESmallENV15 = \"BGESmallENV15\"\n\n    BGESmallENV15Q = \"BGESmallENV15Q\"\n\n    NomicEmbedTextV1 = \"NomicEmbedTextV1\"\n\n    NomicEmbedTextV15 = \"NomicEmbedTextV15\"\n\n    NomicEmbedTextV15Q = \"NomicEmbedTextV15Q\"\n\n    ParaphraseMLMiniLML12V2 = \"ParaphraseMLMiniLML12V2\"\n\n    ParaphraseMLMiniLML12V2Q = \"ParaphraseMLMiniLML12V2Q\"\n\n    ParaphraseMLMpnetBaseV2 = \"ParaphraseMLMpnetBaseV2\"\n\n    BGESmallZHV15 = \"BGESmallZHV15\"\n\n    MultilingualE5Small = \"MultilingualE5Small\"\n\n    MultilingualE5Base = \"MultilingualE5Base\"\n\n    MultilingualE5Large = \"MultilingualE5Large\"\n\n    MxbaiEmbedLargeV1 = \"MxbaiEmbedLargeV1\"\n\n    MxbaiEmbedLargeV1Q = \"MxbaiEmbedLargeV1Q\"\n\n    GTEBaseENV15 = \"GTEBaseENV15\"\n\n    GTEBaseENV15Q = \"GTEBaseENV15Q\"\n\n    GTELargeENV15 = \"GTELargeENV15\"\n\n    GTELargeENV15Q = \"GTELargeENV15Q\"\n\n    JINAV2SMALLEN = \"JINAV2SMALLEN\"\n\n    JINAV2BASEEN = \"JINAV2BASEEN\"\n\n    JINAV3 = \"JINAV3\"\n\n    SPLADEPPENV1 = \"SPLADEPPENV1\"\n\n    SPLADEPPENV2 = \"SPLADEPPENV2\"\n\n    ModernBERTBase = \"ModernBERTBase\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.Reranker","title":"<code>Reranker</code>","text":"<p>Represents the Reranker model.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class Reranker:\n    \"\"\"\n    Represents the Reranker model.\n    \"\"\"\n\n    def __init__(\n        self, model_id: str, revision: str | None = None, dtype: Dtype | None = None, path_in_repo: str | None = None\n    ):\n        \"\"\"\n        Initializes the Reranker object.\n        \"\"\"\n\n    def from_pretrained(\n        model_id: str, revision: str | None = None, dtype: Dtype | None = None, path_in_repo: str | None = None\n    ) -&gt; Reranker:\n        \"\"\"\n        Loads a pre-trained Reranker model from the Hugging Face model hub.\n\n        Args:\n            model_id: The ID of the model from Hugging Face.\n            revision: The revision of the model.\n            dtype: The dtype of the model.\n            path_in_repo: The path to the model in the repository.\n\n        \"\"\"\n\n    def rerank(\n        self, query: list[str], documents: list[str], batch_size: int\n    ) -&gt; RerankerResult:\n        \"\"\"\n        Reranks the given documents for the query and returns a list of RerankerResult objects.\n\n        Args:\n            query: The query to rerank.\n            documents: The list of documents to rerank.\n            batch_size: The number of documents to process per batch.\n\n        Returns:\n            A list of RerankerResult objects.\n        \"\"\"\n\n    def compute_scores(\n        self, query: list[str], documents: list[str], batch_size: int\n    ) -&gt; list[list[float]]:\n        \"\"\"\n        Computes the scores for the given query and documents.\n\n        Args:\n            query: The query to compute the scores for.\n            documents: The list of documents to compute the scores for.\n            batch_size: The batch size for processing the scores.\n\n        Returns:\n            A list of scores for the given query and documents.\n        \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.Reranker.__init__","title":"<code>__init__(model_id, revision=None, dtype=None, path_in_repo=None)</code>","text":"<p>Initializes the Reranker object.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def __init__(\n    self, model_id: str, revision: str | None = None, dtype: Dtype | None = None, path_in_repo: str | None = None\n):\n    \"\"\"\n    Initializes the Reranker object.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.Reranker.compute_scores","title":"<code>compute_scores(query, documents, batch_size)</code>","text":"<p>Computes the scores for the given query and documents.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>list[str]</code> <p>The query to compute the scores for.</p> required <code>documents</code> <code>list[str]</code> <p>The list of documents to compute the scores for.</p> required <code>batch_size</code> <code>int</code> <p>The batch size for processing the scores.</p> required <p>Returns:</p> Type Description <code>list[list[float]]</code> <p>A list of scores for the given query and documents.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def compute_scores(\n    self, query: list[str], documents: list[str], batch_size: int\n) -&gt; list[list[float]]:\n    \"\"\"\n    Computes the scores for the given query and documents.\n\n    Args:\n        query: The query to compute the scores for.\n        documents: The list of documents to compute the scores for.\n        batch_size: The batch size for processing the scores.\n\n    Returns:\n        A list of scores for the given query and documents.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.Reranker.from_pretrained","title":"<code>from_pretrained(model_id, revision=None, dtype=None, path_in_repo=None)</code>","text":"<p>Loads a pre-trained Reranker model from the Hugging Face model hub.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The ID of the model from Hugging Face.</p> required <code>revision</code> <code>str | None</code> <p>The revision of the model.</p> <code>None</code> <code>dtype</code> <code>Dtype | None</code> <p>The dtype of the model.</p> <code>None</code> <code>path_in_repo</code> <code>str | None</code> <p>The path to the model in the repository.</p> <code>None</code> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def from_pretrained(\n    model_id: str, revision: str | None = None, dtype: Dtype | None = None, path_in_repo: str | None = None\n) -&gt; Reranker:\n    \"\"\"\n    Loads a pre-trained Reranker model from the Hugging Face model hub.\n\n    Args:\n        model_id: The ID of the model from Hugging Face.\n        revision: The revision of the model.\n        dtype: The dtype of the model.\n        path_in_repo: The path to the model in the repository.\n\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.Reranker.rerank","title":"<code>rerank(query, documents, batch_size)</code>","text":"<p>Reranks the given documents for the query and returns a list of RerankerResult objects.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>list[str]</code> <p>The query to rerank.</p> required <code>documents</code> <code>list[str]</code> <p>The list of documents to rerank.</p> required <code>batch_size</code> <code>int</code> <p>The number of documents to process per batch.</p> required <p>Returns:</p> Type Description <code>RerankerResult</code> <p>A list of RerankerResult objects.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def rerank(\n    self, query: list[str], documents: list[str], batch_size: int\n) -&gt; RerankerResult:\n    \"\"\"\n    Reranks the given documents for the query and returns a list of RerankerResult objects.\n\n    Args:\n        query: The query to rerank.\n        documents: The list of documents to rerank.\n        batch_size: The number of documents to process per batch.\n\n    Returns:\n        A list of RerankerResult objects.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.RerankerResult","title":"<code>RerankerResult</code>","text":"<p>Represents the result of the reranking process.</p> <p>Attributes:</p> Name Type Description <code>query</code> <code>str</code> <p>The query to rerank.</p> <code>documents</code> <code>list[DocumentRank]</code> <p>The list of documents to rerank.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class RerankerResult:\n    \"\"\"\n    Represents the result of the reranking process.\n\n    Attributes:\n        query: The query to rerank.\n        documents: The list of documents to rerank.\n    \"\"\"\n\n    query: str\n    documents: list[DocumentRank]\n</code></pre>"},{"location":"references/#python.python.embed_anything.TextEmbedConfig","title":"<code>TextEmbedConfig</code>","text":"<p>Represents the configuration for the Text Embedding model.</p> <p>Attributes:</p> Name Type Description <code>chunk_size</code> <code>int | None</code> <p>The chunk size for the Text Embedding model. Default is 1000 Characters.</p> <code>batch_size</code> <code>int | None</code> <p>The batch size for processing the embeddings. Default is 32. Based on the memory, you can increase or decrease the batch size.</p> <code>buffer_size</code> <code>int | None</code> <p>The buffer size for the Text Embedding model. Default is 100.</p> <code>late_chunking</code> <code>bool | None</code> <p>A flag indicating whether to use late chunking for the Text Embedding model. Use late chunking to increase the context that is taken into account for each chunk.  Default is False.</p> <code>splitting_strategy</code> <code>str | None</code> <p>The strategy to use for splitting the text into chunks. Default is \"sentence\". If semantic splitting is used, semantic_encoder is required.</p> <code>semantic_encoder</code> <code>EmbeddingModel | None</code> <p>The semantic encoder for the Text Embedding model. Default is None.</p> <code>use_ocr</code> <code>bool | None</code> <p>A flag indicating whether to use OCR for the Text Embedding model. Default is False.</p> <code>tesseract_path</code> <code>str | None</code> <p>The path to the Tesseract OCR executable. Default is None and uses the system path.</p> <code>pdf_backend</code> <code>str | None</code> <p>The backend to use for PDF text extraction. Currently only <code>lopdf</code> is supported. Default is <code>lopdf</code>.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class TextEmbedConfig:\n    \"\"\"\n    Represents the configuration for the Text Embedding model.\n\n    Attributes:\n        chunk_size: The chunk size for the Text Embedding model. Default is 1000 Characters.\n        batch_size: The batch size for processing the embeddings. Default is 32. Based on the memory, you can increase or decrease the batch size.\n        buffer_size: The buffer size for the Text Embedding model. Default is 100.\n        late_chunking: A flag indicating whether to use late chunking for the Text Embedding model. Use late chunking to increase the context that is taken into account for each chunk.  Default is False.\n        splitting_strategy: The strategy to use for splitting the text into chunks. Default is \"sentence\". If semantic splitting is used, semantic_encoder is required.\n        semantic_encoder: The semantic encoder for the Text Embedding model. Default is None.\n        use_ocr: A flag indicating whether to use OCR for the Text Embedding model. Default is False.\n        tesseract_path: The path to the Tesseract OCR executable. Default is None and uses the system path.\n        pdf_backend: The backend to use for PDF text extraction. Currently only `lopdf` is supported. Default is `lopdf`.\n    \"\"\"\n\n    def __init__(\n        self,\n        chunk_size: int | None = 1000,\n        overlap_ratio: float | None = 0.0,\n        batch_size: int | None = 32,\n        late_chunking: bool | None = False,\n        buffer_size: int | None = 100,\n        splitting_strategy: str | None = \"sentence\",\n        semantic_encoder: EmbeddingModel | None = None,\n        use_ocr: bool | None = False,\n        tesseract_path: str | None = None,\n        pdf_backend: str | None = \"lopdf\",\n    ):\n        self.chunk_size = chunk_size\n        self.overlap_ratio = overlap_ratio\n        self.batch_size = batch_size\n        self.late_chunking = late_chunking\n        self.buffer_size = buffer_size\n        self.splitting_strategy = splitting_strategy\n        self.semantic_encoder = semantic_encoder\n        self.use_ocr = use_ocr\n        self.tesseract_path = tesseract_path\n        self.pdf_backend = pdf_backend\n    chunk_size: int | None\n    overlap_ratio: float | None\n    batch_size: int | None\n    late_chunking: bool | None\n    buffer_size: int | None\n    splitting_strategy: str | None\n    semantic_encoder: EmbeddingModel | None\n    use_ocr: bool | None\n    tesseract_path: str | None\n    pdf_backend: str | None\n</code></pre>"},{"location":"references/#python.python.embed_anything.embed_audio_file","title":"<code>embed_audio_file(file_path, audio_decoder, embedder, text_embed_config=TextEmbedConfig(chunk_size=1000, batch_size=32))</code>","text":"<p>Embeds the given audio file and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the audio file to embed.</p> required <code>audio_decoder</code> <code>AudioDecoderModel</code> <p>The audio decoder model to use.</p> required <code>embedder</code> <code>EmbeddingModel</code> <p>The embedding model to use.</p> required <code>text_embed_config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding model.</p> <code>TextEmbedConfig(chunk_size=1000, batch_size=32)</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> <p>Example: <pre><code>import embed_anything\naudio_decoder = embed_anything.AudioDecoderModel.from_pretrained_hf(\n    \"openai/whisper-tiny.en\", revision=\"main\", model_type=\"tiny-en\", quantized=False\n)\n\nembedder = embed_anything.EmbeddingModel.from_pretrained_hf(\n    embed_anything.WhichModel.Bert,\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    revision=\"main\",\n)\n\nconfig = embed_anything.TextEmbedConfig(chunk_size=1000, batch_size=32)\ndata = embed_anything.embed_audio_file(\n    \"test_files/audio/samples_hp0.wav\",\n    audio_decoder=audio_decoder,\n    embedder=embedder,\n    text_embed_config=config,\n)\n</code></pre></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_audio_file(\n    file_path: str,\n    audio_decoder: AudioDecoderModel,\n    embedder: EmbeddingModel,\n    text_embed_config: TextEmbedConfig | None = TextEmbedConfig(\n        chunk_size=1000, batch_size=32\n    ),\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given audio file and returns a list of EmbedData objects.\n\n    Args:\n        file_path: The path to the audio file to embed.\n        audio_decoder: The audio decoder model to use.\n        embedder: The embedding model to use.\n        text_embed_config: The configuration for the embedding model.\n\n    Returns:\n        A list of EmbedData objects.\n\n    Example:\n    ```python\n\n    import embed_anything\n    audio_decoder = embed_anything.AudioDecoderModel.from_pretrained_hf(\n        \"openai/whisper-tiny.en\", revision=\"main\", model_type=\"tiny-en\", quantized=False\n    )\n\n    embedder = embed_anything.EmbeddingModel.from_pretrained_hf(\n        embed_anything.WhichModel.Bert,\n        model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n        revision=\"main\",\n    )\n\n    config = embed_anything.TextEmbedConfig(chunk_size=1000, batch_size=32)\n    data = embed_anything.embed_audio_file(\n        \"test_files/audio/samples_hp0.wav\",\n        audio_decoder=audio_decoder,\n        embedder=embedder,\n        text_embed_config=config,\n    )\n    ```\n\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.embed_directory","title":"<code>embed_directory(file_path, embedder, extensions, config=None, adapter=None)</code>","text":"<p>Embeds the files in the given directory and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the directory containing the files to embed.</p> required <code>embedder</code> <code>EmbeddingModel</code> <p>The embedding model to use.</p> required <code>extensions</code> <code>list[str]</code> <p>The list of file extensions to consider for embedding.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding model.</p> <code>None</code> <code>adapter</code> <code>Adapter | None</code> <p>The adapter to use for storing the embeddings in a vector database.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> <p>Example: <pre><code>import embed_anything\nmodel = embed_anything.EmbeddingModel.from_pretrained_hf(\n    embed_anything.WhichModel.Bert,\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    revision=\"main\",\n)\ndata = embed_anything.embed_directory(\"test_files\", embedder=model, extensions=[\".pdf\"])\n</code></pre></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_directory(\n    file_path: str,\n    embedder: EmbeddingModel,\n    extensions: list[str],\n    config: TextEmbedConfig | None = None,\n    adapter: Adapter | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the files in the given directory and returns a list of EmbedData objects.\n\n    Args:\n        file_path: The path to the directory containing the files to embed.\n        embedder: The embedding model to use.\n        extensions: The list of file extensions to consider for embedding.\n        config: The configuration for the embedding model.\n        adapter: The adapter to use for storing the embeddings in a vector database.\n\n    Returns:\n        A list of EmbedData objects.\n\n    Example:\n    ```python\n    import embed_anything\n    model = embed_anything.EmbeddingModel.from_pretrained_hf(\n        embed_anything.WhichModel.Bert,\n        model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n        revision=\"main\",\n    )\n    data = embed_anything.embed_directory(\"test_files\", embedder=model, extensions=[\".pdf\"])\n    ```\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.embed_file","title":"<code>embed_file(file_path, embedder, config=None, adapter=None)</code>","text":"<p>Embeds the given file and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the file to embed.</p> required <code>embedder</code> <code>EmbeddingModel</code> <p>The embedding model to use.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding model.</p> <code>None</code> <code>adapter</code> <code>Adapter | None</code> <p>The adapter to use for storing the embeddings in a vector database.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> <p>Example: <pre><code>import embed_anything\nmodel = embed_anything.EmbeddingModel.from_pretrained_hf(\n    embed_anything.WhichModel.Bert,\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    revision=\"main\",\n)\ndata = embed_anything.embed_file(\"test_files/test.pdf\", embedder=model)\n</code></pre></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_file(\n    file_path: str,\n    embedder: EmbeddingModel,\n    config: TextEmbedConfig | None = None,\n    adapter: Adapter | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given file and returns a list of EmbedData objects.\n\n    Args:\n        file_path: The path to the file to embed.\n        embedder: The embedding model to use.\n        config: The configuration for the embedding model.\n        adapter: The adapter to use for storing the embeddings in a vector database.\n\n    Returns:\n        A list of EmbedData objects.\n\n    Example:\n    ```python\n    import embed_anything\n    model = embed_anything.EmbeddingModel.from_pretrained_hf(\n        embed_anything.WhichModel.Bert,\n        model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n        revision=\"main\",\n    )\n    data = embed_anything.embed_file(\"test_files/test.pdf\", embedder=model)\n    ```\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.embed_files_batch","title":"<code>embed_files_batch(files, embedder, config=None, adapter=None)</code>","text":"<p>Embeds the given files and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list[str]</code> <p>The list of files to embed.</p> required <code>embedder</code> <code>EmbeddingModel</code> <p>The embedding model to use.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding model.</p> <code>None</code> <code>adapter</code> <code>Adapter | None</code> <p>The adapter to use for storing the embeddings in a vector database.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> <p>Example: <pre><code>import embed_anything\nmodel = embed_anything.EmbeddingModel.from_pretrained_hf(\n    embed_anything.WhichModel.Bert,\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    revision=\"main\",\n)\ndata = embed_anything.embed_files_batch(\n    [\"test_files/test.pdf\", \"test_files/test.txt\"],\n    embedder=model,\n    config=embed_anything.TextEmbedConfig(),\n    adapter=None,\n)\n</code></pre></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_files_batch(\n    files: list[str],\n    embedder: EmbeddingModel,\n    config: TextEmbedConfig | None = None,\n    adapter: Adapter | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given files and returns a list of EmbedData objects.\n\n    Args:\n        files: The list of files to embed.\n        embedder: The embedding model to use.\n        config: The configuration for the embedding model.\n        adapter: The adapter to use for storing the embeddings in a vector database.\n\n    Returns:\n        A list of EmbedData objects.\n\n    Example:\n    ```python\n    import embed_anything\n    model = embed_anything.EmbeddingModel.from_pretrained_hf(\n        embed_anything.WhichModel.Bert,\n        model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n        revision=\"main\",\n    )\n    data = embed_anything.embed_files_batch(\n        [\"test_files/test.pdf\", \"test_files/test.txt\"],\n        embedder=model,\n        config=embed_anything.TextEmbedConfig(),\n        adapter=None,\n    )\n    ```\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.embed_html","title":"<code>embed_html(file_name, embedder, origin=None, config=None, adapter=None)</code>","text":"<p>Embeds the given HTML file and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The path to the HTML file to embed.</p> required <code>embedder</code> <code>EmbeddingModel</code> <p>The embedding model to use.</p> required <code>origin</code> <code>str | None</code> <p>The origin of the HTML file.</p> <code>None</code> <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding model.</p> <code>None</code> <code>adapter</code> <code>Adapter | None</code> <p>The adapter to use for storing the embeddings.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> <p>Example: <pre><code>import embed_anything\nmodel = embed_anything.EmbeddingModel.from_pretrained_hf(\n    embed_anything.WhichModel.Bert,\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    revision=\"main\",\n)\ndata = embed_anything.embed_html(\n    \"test_files/test.html\", embedder=model, origin=\"https://www.akshaymakes.com/\"\n)\n</code></pre></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_html(\n    file_name: str,\n    embedder: EmbeddingModel,\n    origin: str | None = None,\n    config: TextEmbedConfig | None = None,\n    adapter: Adapter | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given HTML file and returns a list of EmbedData objects.\n\n    Args:\n        file_name: The path to the HTML file to embed.\n        embedder: The embedding model to use.\n        origin: The origin of the HTML file.\n        config: The configuration for the embedding model.\n        adapter: The adapter to use for storing the embeddings.\n\n    Returns:\n        A list of EmbedData objects.\n\n    Example:\n    ```python\n    import embed_anything\n    model = embed_anything.EmbeddingModel.from_pretrained_hf(\n        embed_anything.WhichModel.Bert,\n        model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n        revision=\"main\",\n    )\n    data = embed_anything.embed_html(\n        \"test_files/test.html\", embedder=model, origin=\"https://www.akshaymakes.com/\"\n    )\n    ```\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.embed_image_directory","title":"<code>embed_image_directory(file_path, embedder, config=None, adapter=None)</code>","text":"<p>Embeds the images in the given directory and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the directory containing the images to embed.</p> required <code>embedder</code> <code>EmbeddingModel</code> <p>The embedding model to use.</p> required <code>config</code> <code>ImageEmbedConfig | None</code> <p>The configuration for the embedding model.</p> <code>None</code> <code>adapter</code> <code>Adapter | None</code> <p>The adapter to use for storing the embeddings in a vector database.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_image_directory(\n    file_path: str,\n    embedder: EmbeddingModel,\n    config: ImageEmbedConfig | None = None,\n    adapter: Adapter | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the images in the given directory and returns a list of EmbedData objects.\n\n    Args:\n        file_path: The path to the directory containing the images to embed.\n        embedder: The embedding model to use.\n        config: The configuration for the embedding model.\n        adapter: The adapter to use for storing the embeddings in a vector database.\n\n    Returns:\n        A list of EmbedData objects.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.embed_query","title":"<code>embed_query(query, embedder, config=None)</code>","text":"<p>Embeds the given query and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>list[str]</code> <p>The query to embed.</p> required <code>embedder</code> <code>EmbeddingModel</code> <p>The embedding model to use.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding model.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> <p>Example:</p> <pre><code>import embed_anything\nmodel = embed_anything.EmbeddingModel.from_pretrained_hf(\n    embed_anything.WhichModel.Bert,\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    revision=\"main\",\n)\n</code></pre> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_query(\n    query: list[str], embedder: EmbeddingModel, config: TextEmbedConfig | None = None\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given query and returns a list of EmbedData objects.\n\n    Args:\n        query: The query to embed.\n        embedder: The embedding model to use.\n        config: The configuration for the embedding model.\n\n    Returns:\n        A list of EmbedData objects.\n\n    Example:\n\n    ```python\n    import embed_anything\n    model = embed_anything.EmbeddingModel.from_pretrained_hf(\n        embed_anything.WhichModel.Bert,\n        model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n        revision=\"main\",\n    )\n    ```\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.embed_webpage","title":"<code>embed_webpage(url, embedder, config, adapter)</code>","text":"<p>Embeds the webpage at the given URL and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the webpage to embed.</p> required <code>embedder</code> <code>EmbeddingModel</code> <p>The name of the embedding model to use. Choose between \"OpenAI\", \"Jina\", \"Bert\"</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding model.</p> required <code>adapter</code> <code>Adapter | None</code> <p>The adapter to use for storing the embeddings.</p> required <p>Returns:</p> Type Description <code>list[EmbedData] | None</code> <p>A list of EmbedData objects</p> <p>Example: <pre><code>import embed_anything\n\nconfig = embed_anything.EmbedConfig(\n    openai_config=embed_anything.OpenAIConfig(model=\"text-embedding-3-small\")\n)\ndata = embed_anything.embed_webpage(\n    \"https://www.akshaymakes.com/\", embedder=\"OpenAI\", config=config\n)\n</code></pre></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_webpage(\n    url: str,\n    embedder: EmbeddingModel,\n    config: TextEmbedConfig | None,\n    adapter: Adapter | None,\n) -&gt; list[EmbedData] | None:\n    \"\"\"Embeds the webpage at the given URL and returns a list of EmbedData\n    objects.\n\n    Args:\n        url: The URL of the webpage to embed.\n        embedder: The name of the embedding model to use. Choose between \"OpenAI\", \"Jina\", \"Bert\"\n        config: The configuration for the embedding model.\n        adapter: The adapter to use for storing the embeddings.\n\n    Returns:\n        A list of EmbedData objects\n\n    Example:\n    ```python\n    import embed_anything\n\n    config = embed_anything.EmbedConfig(\n        openai_config=embed_anything.OpenAIConfig(model=\"text-embedding-3-small\")\n    )\n    data = embed_anything.embed_webpage(\n        \"https://www.akshaymakes.com/\", embedder=\"OpenAI\", config=config\n    )\n    ```\n    \"\"\"\n</code></pre>"},{"location":"blog/","title":"\ud83d\udcf0 All Posts","text":""},{"location":"blog/2024/03/31/gemini/","title":"Gemini as a Research Agent: Fusion Deep Research for Intelligent Information Retrieval","text":"<p>In today's data-saturated world, effective research isn't just about accessing information\u2014it's about finding the right information efficiently. Let's explore how Gemini can function as a sophisticated research agent, using a fusion deep research approach to intelligently determine what information is necessary and when to search external sources.</p> <p>Please find the repository here.</p>"},{"location":"blog/2024/03/31/gemini/#the-fusion-deep-research-architecture","title":"The Fusion Deep Research Architecture","text":"<p>The system I've been building employs Gemini 2.0 Flash as an intelligent intermediary between user queries and multiple information sources. This fusion approach combines:</p> <ol> <li>Local vector database knowledge</li> <li>Web-based information retrieval</li> <li>Intelligent query reformulation</li> <li>Comprehensive information evaluation</li> </ol>"},{"location":"blog/2024/03/31/gemini/#purpose-of-done-flag","title":"Purpose of done flag","text":"<p>The <code>\"done\": false</code> flag is a crucial control mechanism in the Gemini research agent system. It indicates whether the research process should continue or if sufficient information has been gathered. Here's a deeper breakdown of how it works: Purpose of the \"done\" Flag The \"done\" flag serves as the agent's decision-making output that determines whether:</p> <pre><code>More research iterations are needed (false)\nEnough information has been collected (true)\n</code></pre> <p></p>"},{"location":"blog/2024/03/31/gemini/#how-the-agent-sets-this-flag","title":"How the Agent Sets This Flag","text":"<p>After each research iteration, Gemini evaluates the collected information against specific criteria: CopyBased on this observations, you have two options: <code>1. Find knowledge gaps that still need to be explored and write 3 different queries that explore different perspectives of the topic. If this is the case set the done flag to False. 2. If there are no more knowledge gaps and you have enough information related to the topic, you dont have to provide any more queries and you can set the done flag to True. When Gemini sets \"done\": false, it's essentially saying: \"I've analyzed what we know so far, and there are still important aspects of this topic we haven't covered adequately.\" Evaluation Criteria The system uses sophisticated criteria to make this determination: CopyBefore setting the done flag to true, make sure that the following conditions are met:  1. You have explored different perspectives of the topic 2. You have collected some opposing views 3. You have collected some supporting views 4. You have collected some views that are not directly related to the topic but can be used to</code></p>"},{"location":"blog/2024/03/31/gemini/#how-fusion-deep-research-works","title":"How Fusion Deep Research Works","text":"<p>Let's examine a specific example where a user asks: \"What are the differences between SSM models and transformer models?\"</p>"},{"location":"blog/2024/03/31/gemini/#step-1-query-diversification","title":"Step 1: Query Diversification","text":"<p>When receiving this query, Gemini first analyzes it to create focused search terms. Instead of using the raw query, it generates three distinct perspectives to explore:</p> <pre><code>{\n  \"querys\": [\n    \"Key architectural differences between SSM models and transformer models\",\n    \"Computational efficiency comparison between SSM models and transformer models\",\n    \"When to use SSM models versus transformer models\"\n  ],\n  \"done\": false\n}\n</code></pre> <p>This diversification ensures comprehensive coverage of the topic from multiple angles\u2014a key principle of fusion deep research.</p>"},{"location":"blog/2024/03/31/gemini/#step-2-multi-source-information-gathering","title":"Step 2: Multi-Source Information Gathering","text":"<p>The agent uses a fusion approach to information retrieval, intelligently determining when to: - Query the local vector database for trusted information - Expand to web sources when local information is insufficient</p> <pre><code>def get_observations(queries: List[str]) -&gt; List[str]:\n    local_observations = []\n    web_observations = []\n    for query in queries:\n        local_observation = my_store.forward(query)\n        local_observations.extend(list(local_observation))\n\n        if web_search:\n            web_result = exa.search_and_contents(query, type=\"auto\", text=True, num_results=3)\n            # Process web results...\n</code></pre> <p>This fusion of information sources creates a more robust research foundation.</p>"},{"location":"blog/2024/03/31/gemini/#step-3-continuous-knowledge-gap-analysis","title":"Step 3: Continuous Knowledge Gap Analysis","text":"<p>After each search iteration, Gemini evaluates the collected information against specific criteria:</p> <pre><code>Step Number: 1\nSearching with queries: \nKey architectural differences between SSM models and transformer models\nComputational efficiency comparison between SSM models and transformer models\nWhen to use SSM models versus transformer models\nDone: False\n</code></pre> <p>Importantly, the system doesn't just blindly collect information\u2014it actively identifies knowledge gaps:</p> <pre><code>Based on this observations, you have two options:\n1. Find knowledge gaps that still need to be explored and write 3 different queries that explore different perspectives of the topic. If this is the case set the done flag to False.\n2. If there are no more knowledge gaps and you have enough information related to the topic, you dont have to provide any more queries and you can set the done flag to True.\n</code></pre> <p>This continuous evaluation represents the \"deep\" aspect of fusion deep research\u2014digging beyond surface-level information to ensure comprehensive understanding.</p>"},{"location":"blog/2024/03/31/gemini/#step-4-multi-perspective-completion-criteria","title":"Step 4: Multi-Perspective Completion Criteria","text":"<p>Perhaps most impressively, Gemini autonomously determines when sufficient information has been gathered based on robust criteria:</p> <pre><code>Before setting the done flag to true, make sure that the following conditions are met: \n1. You have explored different perspectives of the topic\n2. You have collected some opposing views\n3. You have collected some supporting views\n4. You have collected some views that are not directly related to the topic but can be used to explore the topic further.\n</code></pre> <p>This creates a research process that is both broad (multiple perspectives) and deep (thorough exploration of each perspective).</p>"},{"location":"blog/2024/03/31/gemini/#step-5-information-synthesis-and-report-generation","title":"Step 5: Information Synthesis and Report Generation","text":"<p>Once fusion deep research is complete, Gemini synthesizes the collected information into a cohesive report, intelligently blending insights from all sources without explicitly highlighting their origins:</p> <pre><code>Do not explicitly mention if the output is from local or web observations. Just write the report as if you have all the information available.\n</code></pre> <p>This seamless integration of multiple information sources is the culmination of the fusion deep research approach.</p>"},{"location":"blog/2024/03/31/gemini/#why-fusion-deep-research-matters","title":"Why Fusion Deep Research Matters","text":"<p>Traditional research methods often: - Rely too heavily on a single information source - Miss important perspectives - Require manual reformulation of queries - Don't know when \"enough is enough\" - Need Evaluation setup manually</p> <p>By implementing fusion deep research with Gemini as an intelligent agent, we transform research from a mechanical process into an adaptive methodology that:</p> <ol> <li>Ensures comprehensive coverage through multiple information sources</li> <li>Identifies and fills knowledge gaps through intelligent query reformulation</li> <li>Balances efficiency and thoroughness by prioritizing local knowledge before web searches</li> <li>Delivers multi-perspective insights rather than single-viewpoint information</li> <li>Knows when to stop searching based on sophisticated completion criteria</li> </ol>"},{"location":"blog/2024/03/31/gemini/#building-your-own-fusion-deep-research-system","title":"Building Your Own Fusion Deep Research System","text":"<p>The implementation requires:</p> <ol> <li>A vector database for local document storage</li> <li>Access to Gemini API (specifically gemini-2.0-flash)</li> <li>A web search API (the example uses Exa)</li> <li>Clear evaluation criteria for information sufficiency</li> </ol> <p>The most critical component is the prompt engineering that enables Gemini to make intelligent decisions about information adequacy and query formulation within the fusion deep research framework.</p>"},{"location":"blog/2024/03/31/gemini/#conclusion","title":"Conclusion","text":"<p>As information continues to expand exponentially, fusion deep research represents the future of intelligent information retrieval. By combining multiple information sources with sophisticated evaluation criteria and autonomous decision-making about research sufficiency, we can dramatically improve both the efficiency and quality of our research processes.</p> <p>Gemini's ability to act as an intelligent intermediary in the fusion deep research process\u2014reformulating queries, evaluating information completeness across sources, and knowing when to stop\u2014transforms it from a mere language model into a true research assistant. As these systems evolve, they promise to redefine how we interact with the growing sea of information around us, making fusion deep research an essential tool for knowledge workers across all domains.</p> <p>Please find the repository here.</p> <p>Please give a \u2b50 to our repo.</p>"},{"location":"blog/2024/12/15/embed-anything/","title":"The path ahead of EmbedAnything","text":"<p>In March, we set out to build a local file search app. We aimed to create a tool that would make file searching faster, more innovative, and more efficient. However, we quickly hit a roadblock: no high-performance backend fit our needs.</p> <p></p>"},{"location":"blog/2024/12/15/embed-anything/#short-of-backend","title":"Short of backend","text":"<p>Initially, we experimented with LlamaIndex, hoping it would provide the required speed and reliability. Unfortunately, it fell short. Its performance didn\u2019t meet our expectations, and its heavy dependencies added unnecessary complexity to our stack. We realized we needed a better solution.</p> <p>Around the same time, we discovered Candle, a Rust-based framework for transformer model inference. Candle stood out with its remarkable speed and minimal dependency footprint. It was exactly what we were looking for a high-performing, lightweight backend that aligned with our vision for a seamless file search experience.</p>"},{"location":"blog/2024/12/15/embed-anything/#experimentation-and-breakthroughs","title":"Experimentation and Breakthroughs","text":"<p>Excited by Candle\u2019s potential, we experimented to see how well it could handle our use case. The results were outstanding. Candle\u2019s blazing-fast inference speeds and low resource demands enabled us to build a prototype that surpassed our initial performance goals.</p> <p>With a working prototype, we decided to share it with the world. We knew a compelling demonstration could capture attention and validate our efforts. The next step was to make a splash with our launch.</p>"},{"location":"blog/2024/12/15/embed-anything/#demo-released","title":"Demo Released","text":"<p>On April 2nd, we unveiled our demo online, carefully choosing the date to avoid confusion with April Fool\u2019s Day. We created an engaging demo video to highlight the app\u2019s capabilities and shared it on Twitter. What happened next exceeded all our expectations.</p> <p>The demo received an overwhelming response. What began as a simple showcase of our prototype transformed into a pivotal moment for our project. In the next 30 days, we released it as an open-source project, seeing the demand and people\u2019s interest.</p> <p>Demo</p>"},{"location":"blog/2024/12/15/embed-anything/#02-released","title":"0.2 released","text":"<p>Since then, we have never looked back. We kept embedding anything better and better. In the next three months, we released a more stable version, 0.2, with all the Python versions. It was running amazingly on AWS and could support multimodality.</p> <p>At the same time, we realized that people wanted an end-to-end solution, not just an embedding generation platform. So we tried to integrate a vector database, but we realized that it would just make our library heavier and not give the value we were looking for, which was confirmed by this discussion opened on our GitHub.</p> <p>\u2014GitHub discussion</p> <p>Akshay started looking for ways to index embeddings without being dependent on vector databases as a dependency, and he came up with a brilliant method that enhanced performance and made indexing extremely memory efficient.</p> <p>And thus, vector streaming was born.</p> <p>\u2014 vector streaming blog</p>"},{"location":"blog/2024/12/15/embed-anything/#03-release","title":"0.3 release","text":"<p>It's time to release 0.3 because we underwent major code refactoring. All the major functions are refactored, making calling models more intuitive and optimized. Check out our docs and usage. We also added audio modality and different types of ingestions.</p> <p>We only supported dense, so we expanded the types of embedding we could support. We went for sparse and started supporting ColPali, ColBert, ModernBert, Reranker, Jina V3.</p>"},{"location":"blog/2024/12/15/embed-anything/#what-we-got-right","title":"What We Got Right","text":"<p>We actively listened to our community and prioritized their needs in the library's development. When users requested support for sparse matrices in hybrid models, we delivered. When they wanted advanced indexing, we made it happen. During the critical three-month period between versions 0.2 and 0.4, our efforts were laser-focused on enhancing the product to meet and exceed expectations. </p> <p>We also released benches comparing it with other inference and to our suprise it's faster than libraries like sentence transformer and fastembed. Check out Benches.</p> <p>We presented Embedanything at many conferences, like Pydata Global, Elastic, voxel 51 meetups, AI builders, etc. Additionally, we forged collaborations with major brands like Weaviate and Elastic, a strategy we\u2019re excited to continue expanding in 2025.</p> <p>Elastic Collab</p>"},{"location":"blog/2024/12/15/embed-anything/#what-we-initially-got-wrong","title":"What We Initially Got Wrong","text":"<p>In hindsight, one significant mistake was prematurely releasing the library before it was ready for production. As the saying goes, \u201cYou never get a second chance to make a first impression,\u201d and this holds true even for open-source projects.</p> <p>The library was unusable on macOS for the first three months, and we only released compatibility with Python 10. We didn\u2019t focus enough on how we were rolling out updates, partly because we never anticipated the overwhelming rate of experimentation and interest it would receive right from the start.</p> <p>I intended to foster a \u201cbuild in public\u201d project, encouraging collaboration and rapid iteration. I wanted to showcase how quickly we could improve and refine this amazing library. </p>"},{"location":"blog/2024/12/15/embed-anything/#in-the-year-2025","title":"In the year 2025","text":"<p>We are committed to applying everything we\u2019ve learned from this journey and doubling down on what truly matters: our hero, the product. In the grand scheme of things, nothing else is as important. Moving forward, we\u2019re also excited to announce even more collaborations with amazing brands, further expanding the impact and reach of our work.</p> <p>Heartfelt thanks to all our amazing contributors and stargazers for your unwavering support and dedication to embedanything. Your continuous experimentation and feedback inspire us to keep refining and enhancing the library with every iteration. We deeply appreciate your efforts in making this journey truly collaborative. Let\u2019s go from 100k+ to a million downloads this year!</p>"},{"location":"blog/2026/01/11/release-notes-7/","title":"Release Notes 0.7","text":"<p>0.7 is all about making deployment easy and integrations to data sources. Including Prebuilt Docker Image,SearchR1 example to improve context for Agents and AWS S3 bucket Integration.</p>"},{"location":"blog/2026/01/11/release-notes-7/#summary-of-changes-068-070","title":"\ud83d\udccb Summary of Changes (0.6.8 \u2192 0.7.0)","text":"<p>This release represents a significant milestone in making EmbedAnything easier to deploy and more powerful for production use. Here's a summary of the key improvements and additions since version 0.6.8:</p>"},{"location":"blog/2026/01/11/release-notes-7/#major-features","title":"\ud83d\ude80 Major Features","text":"<ul> <li>Prebuilt Docker Image: Production-ready Docker image available for immediate deployment</li> <li>AWS S3 Integration: Direct support for fetching and embedding files from S3 buckets</li> <li>SearchR1 Agent: Advanced agent framework for interweaving retrieved results to improve context</li> </ul>"},{"location":"blog/2026/01/11/release-notes-7/#bug-fixes-stability","title":"\ud83d\udc1b Bug Fixes &amp; Stability","text":"<ul> <li>Fixed Parameter Mismatching in Rerank: Resolved issues with reranking function parameters</li> <li>Fixed Qwen3Embed Concurrency Panic: Addressed panic issues when using Qwen3 embeddings concurrently</li> <li>Fixed File Extension Handling: Improved error handling for files without extensions</li> <li>Enhanced Error Handling: Better error messages and recovery mechanisms</li> </ul>"},{"location":"blog/2026/01/11/release-notes-7/#prebuilt-docker-image","title":"\ud83d\udc33 Prebuilt Docker Image","text":"<p>We're excited to announce that a prebuilt Docker image is now available! You can now pull the image and spin up the server without building from source. This makes deployment significantly easier and faster.</p>"},{"location":"blog/2026/01/11/release-notes-7/#quick-start-with-docker","title":"Quick Start with Docker","text":""},{"location":"blog/2026/01/11/release-notes-7/#pull-the-prebuilt-image","title":"Pull the Prebuilt Image","text":"<pre><code>docker pull starlightsearch/embedanything-server:latest\n</code></pre>"},{"location":"blog/2026/01/11/release-notes-7/#run-the-container","title":"Run the Container","text":"<pre><code>docker run -p 8080:8080 starlightsearch/embedanything-server:latest\n</code></pre> <p>The server will start on <code>http://0.0.0.0:8080</code>.</p>"},{"location":"blog/2026/01/11/release-notes-7/#building-from-source","title":"Building from Source","text":"<p>If you prefer to build the Docker image yourself, you can use the provided Dockerfile:</p> <pre><code>docker build -f server.Dockerfile -t embedanything-server .\ndocker run -p 8080:8080 embedanything-server\n</code></pre>"},{"location":"blog/2026/01/11/release-notes-7/#server-features","title":"Server Features","text":"<p>The Actix server provides an OpenAI-compatible API for generating embeddings. We chose Actix Server for:</p> <ol> <li>Blazing fast: Consistently ranks among the fastest web frameworks in benchmarks like TechEmpower</li> <li>Asynchronous by default: Built on Rust's async/await, enabling efficient I/O-bound workloads</li> <li>Lightweight &amp; modular: Minimal core with extensible middleware, plugins, and integrations</li> <li>Type-safe: Strong type guarantees ensure fewer runtime surprises</li> <li>Production-ready: Stable, mature, and already used in industries like fintech, IoT, and SaaS platforms</li> </ol> <p>For benchmarks between Python and Rust servers, check out this blog: https://www.jonvet.com/blog/benchmarking-python-rust-web-servers</p>"},{"location":"blog/2026/01/11/release-notes-7/#api-usage","title":"API Usage","text":""},{"location":"blog/2026/01/11/release-notes-7/#create-embeddings","title":"Create Embeddings","text":"<p>Endpoint: <code>POST /v1/embeddings</code></p> <p>Request: <pre><code>{\n  \"model\": \"sentence-transformers/all-MiniLM-L12-v2\",\n  \"input\": [\"The quick brown fox jumps over the lazy dog\"]\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [0.0023064255, -0.009327292, ...]\n    }\n  ],\n  \"model\": \"sentence-transformers/all-MiniLM-L12-v2\",\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"total_tokens\": 9\n  }\n}\n</code></pre></p>"},{"location":"blog/2026/01/11/release-notes-7/#health-check","title":"Health Check","text":"<p>Endpoint: <code>GET /health_check</code></p> <p>Returns a 200 OK status if the server is running.</p>"},{"location":"blog/2026/01/11/release-notes-7/#example-usage-with-curl","title":"Example Usage with curl","text":"<pre><code># Create embeddings\ncurl -X POST http://localhost:8080/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"sentence-transformers/all-MiniLM-L12-v2\",\n    \"input\": [\"Hello world\", \"How are you?\"]\n  }'\n\n# Health check\ncurl http://localhost:8080/health_check\n</code></pre>"},{"location":"blog/2026/01/11/release-notes-7/#example-usage-with-python","title":"Example Usage with Python","text":"<pre><code>import requests\n\n# Create embeddings\nresponse = requests.post(\n    \"http://localhost:8080/v1/embeddings\",\n    json={\n        \"model\": \"sentence-transformers/all-MiniLM-L12-v2\",\n        \"input\": [\"The quick brown fox jumps over the lazy dog\"]\n    }\n)\n\nif response.status_code == 200:\n    data = response.json()\n    print(f\"Generated {len(data['data'])} embeddings\")\n    print(f\"First embedding dimension: {len(data['data'][0]['embedding'])}\")\nelse:\n    print(f\"Error: {response.json()}\")\n</code></pre>"},{"location":"blog/2026/01/11/release-notes-7/#error-handling","title":"Error Handling","text":"<p>The API returns OpenAI-compatible error responses:</p> <pre><code>{\n  \"error\": {\n    \"message\": \"Error description\",\n    \"type\": \"error_type\",\n    \"code\": \"error_code\"\n  }\n}\n</code></pre> <p>For more details, see the Actix Server Guide.</p>"},{"location":"blog/2026/01/11/release-notes-7/#searchr1-agent-integration","title":"\ud83d\udd0d SearchR1 Agent Integration","text":"<p>We've included the SearchR1 agent, which is a powerful method of interweaving retrieved results to improve context. This agent enables more sophisticated reasoning by dynamically integrating search results into the generation process.</p>"},{"location":"blog/2026/01/11/release-notes-7/#how-searchr1-works","title":"How SearchR1 Works","text":"<p>SearchR1 uses a unique approach where: - The model conducts reasoning inside <code>&lt;think&gt;</code> tags - When knowledge gaps are identified, it calls a search engine via <code>&lt;search&gt; query &lt;/search&gt;</code> tags - Search results are returned between <code>&lt;information&gt;</code> and <code>&lt;/information&gt;</code> tags - The agent can search multiple times, iteratively refining its understanding - Once sufficient information is gathered, it provides the answer inside <code>&lt;answer&gt;</code> tags</p> <p>This interweaving of retrieved results with reasoning creates a more contextually aware and accurate response generation process. The agent actively identifies knowledge gaps and explores different perspectives of a topic before providing a final answer.</p>"},{"location":"blog/2026/01/11/release-notes-7/#example-usage","title":"Example Usage","text":"<p>The SearchR1 agent is available in our examples. Check out <code>examples/SearchAgent/</code> for complete implementation examples showing how to integrate SearchR1 with EmbedAnything's retrieval capabilities using LanceDB.</p>"},{"location":"blog/2026/01/11/release-notes-7/#direct-aws-s3-bucket-integration","title":"\u2601\ufe0f Direct AWS S3 Bucket Integration","text":"<p>We've added direct integration with AWS S3 buckets, allowing you to fetch and embed files directly from your S3 storage without manual downloads.</p>"},{"location":"blog/2026/01/11/release-notes-7/#features","title":"Features","text":"<ul> <li>Fetch files directly from S3 buckets</li> <li>Support for explicit credentials or environment variables</li> <li>Seamless integration with EmbedAnything's embedding pipeline</li> <li>Save files locally or work with them in memory</li> </ul>"},{"location":"blog/2026/01/11/release-notes-7/#usage","title":"Usage","text":""},{"location":"blog/2026/01/11/release-notes-7/#using-explicit-credentials","title":"Using Explicit Credentials","text":"<pre><code>from embed_anything import S3Client, EmbeddingModel, WhichModel, TextEmbedConfig\n\n# Create S3Client with credentials\ns3_client = S3Client(\n    access_key_id=\"your-access-key-id\",\n    secret_access_key=\"your-secret-access-key\",\n    region=\"us-east-1\"\n)\n\n# Fetch a file from S3\nfile = s3_client.get_file_from_s3(\n    bucket_name=\"your-bucket-name\", \n    key=\"path/to/your/file.txt\"\n).save_file()\n\n# Embed the file\nembedder = EmbeddingModel.from_pretrained_hf(\n    model_id=\"jinaai/jina-embeddings-v2-small-en\"\n)\nembeddings = embedder.embed_file(\n    file, \n    config=TextEmbedConfig(\n        chunk_size=1000, \n        batch_size=32, \n        splitting_strategy=\"sentence\"\n    )\n)\n</code></pre>"},{"location":"blog/2026/01/11/release-notes-7/#using-environment-variables","title":"Using Environment Variables","text":"<pre><code>from embed_anything import S3Client\n\n# Create S3Client from environment variables\n# Reads from: AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION\ns3_client = S3Client.from_env()\n\n# Fetch and use files as above\nfile = s3_client.get_file_from_s3(\n    bucket_name=\"your-bucket-name\", \n    key=\"path/to/your/file.pdf\"\n).save_file()\n</code></pre>"},{"location":"blog/2026/01/11/release-notes-7/#s3client-methods","title":"S3Client Methods","text":"<ul> <li><code>get_file_from_s3(bucket_name, key)</code>: Fetches a file from S3 and returns an <code>S3File</code> object</li> <li><code>S3File.save_file(file_path)</code>: Saves the file to the local filesystem (optional path parameter)</li> <li><code>S3File.bytes</code>: Access file contents as bytes</li> <li><code>S3File.key</code>: Get the S3 key/path</li> </ul> <p>For a complete example, see <code>examples/s3_example.py</code>.</p> <p>We're excited about these improvements and look forward to seeing how you use them in your projects! For questions or feedback, please open an issue on our GitHub repository.</p>"},{"location":"blog/2025/05/25/release-notes-6/","title":"Release Notes 6.0","text":"<p>Super Excited to share the latest development in our library, which essentially giving you more embedding choices -- Cohere and siglip, new chunking method-- late chunking and more crates that facilitates amazing modality and maintainability for our rust codebase, --processor crate. so let's dive in.</p>"},{"location":"blog/2025/05/25/release-notes-6/#late-chunking","title":"Late Chunking","text":"<p>The new 0.5.6 version adds Late Chunking to EmbedAnything, a technique introduced by Jina AI and Weaviate.  Here's how we've implemented Late Chunking in EA:</p> <p>\ud835\uddd5\ud835\uddee\ud835\ude01\ud835\uddf0\ud835\uddf5 \ud835\uddee\ud835\ude00 \ud835\uddd6\ud835\uddf5\ud835\ude02\ud835\uddfb\ud835\uddf8 \ud835\uddda\ud835\uddff\ud835\uddfc\ud835\ude02\ud835\uddfd: In EmbedAnything, with late chunking enabled, the batch size determines the number of neighboring chunks that will be processed together.</p> <p>\ud835\udddd\ud835\uddfc\ud835\uddf6\ud835\uddfb\ud835\ude01 \ud835\uddd8\ud835\uddfa\ud835\uddef\ud835\uddf2\ud835\uddf1\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4: The grouped chunks are fed into the embedding model as a single, larger input. This allows the model to capture relationships and dependencies between adjacent chunks.</p> <p>\ud835\uddd8\ud835\uddfa\ud835\uddef\ud835\uddf2\ud835\uddf1\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\udde6\ud835\uddfd\ud835\uddf9\ud835\uddf6\ud835\ude01: After embedding, the combined output is divided back into the embeddings for the original, individual chunks.</p> <p>\ud835\udde0\ud835\uddf2\ud835\uddee\ud835\uddfb \ud835\udde3\ud835\uddfc\ud835\uddfc\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf4 (\ud835\uddfd\ud835\uddf2\ud835\uddff \ud835\uddd6\ud835\uddf5\ud835\ude02\ud835\uddfb\ud835\uddf8): Mean pooling is then applied to each individual chunk's embedding, incorporating the contextual information learned during the joint embedding phase.</p> <p>\ud835\udc3e\ud835\udc52\ud835\udc66 \ud835\udc35\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc53\ud835\udc56\ud835\udc61\ud835\udc60:</p> <p>\ud835\uddd6\ud835\uddfc\ud835\uddfb\ud835\ude01\ud835\uddf2\ud835\ude05\ud835\ude01-\ud835\uddd4\ud835\ude04\ud835\uddee\ud835\uddff\ud835\uddf2 \ud835\uddd8\ud835\uddfa\ud835\uddef\ud835\uddf2\ud835\uddf1\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4\ud835\ude00: By embedding neighboring chunks together, we capture crucial contextual information that would be lost with independent chunking.</p> <p>\ud835\udde2\ud835\uddfd\ud835\ude01\ud835\uddf6\ud835\uddfa\ud835\uddf6\ud835\ude07\ud835\uddf2\ud835\uddf1 \ud835\udde5\ud835\uddf2\ud835\ude01\ud835\uddff\ud835\uddf6\ud835\uddf2\ud835\ude03\ud835\uddee\ud835\uddf9 \ud835\udde3\ud835\uddf2\ud835\uddff\ud835\uddf3\ud835\uddfc\ud835\uddff\ud835\uddfa\ud835\uddee\ud835\uddfb\ud835\uddf0\ud835\uddf2: Expect a significant improvement in the accuracy and relevance of your search results.</p> <pre><code>model:EmbeddingModel = EmbeddingModel.from_pretrained_onnx(\n    WhichModel.Jina, hf_model_id=\"jinaai/jina-embeddings-v2-small-en\", path_in_repo=\"model.onnx\"\n)\nconfig = TextEmbedConfig(\n    chunk_size=1000,\n    batch_size=8,\n    splitting_strategy=\"sentence\",\n    late_chunking=True,\n)\n\n# Embed a single file\ndata: list[EmbedData] = model.embed_file(\"test_files/attention.pdf\", config=config)\n</code></pre>"},{"location":"blog/2025/05/25/release-notes-6/#cohere-embed-4","title":"Cohere Embed 4:","text":"<p>\ud83e\uddca Single embedding per document, even for multimodal inputs \ud83d\udcda Handles up to 128K tokens \u2013 perfect for long-form business documents \ud83d\uddc3\ufe0f Supports compressed vector formats (int8, binary) for real-world scalability \ud83c\udf10 Multilingual across 100+ languages</p> <p>The catch? It\u2019s not open-source\u2014and even if it were, the model would be quite hefty to run locally. But if you\u2019re already using cloud-based embeddings like OpenAI\u2019s, Embed v4 is worth testing.</p> <pre><code># Initialize the model once\nmodel: EmbeddingModel = EmbeddingModel.from_pretrained_cloud(\n    WhichModel.CohereVision, model_id=\"embed-v4.0\"\n)\n</code></pre>"},{"location":"blog/2025/05/25/release-notes-6/#siglip","title":"SigLIP","text":"<p>We already had Clip support but many of you asked for siglip support. It out performs clip for zero shot classification for smaller batch. It also has better memory efficinecy.</p> <pre><code># Load the model.\nmodel = embed_anything.EmbeddingModel.from_pretrained_hf(\n    embed_anything.WhichModel.Clip,\n    model_id=\"google/siglip-base-patch16-224\",\n)\n</code></pre>"},{"location":"blog/2025/05/25/release-notes-6/#processor-crate","title":"Processor Crate:","text":"<p>This crate contains various \"processors\" that accepts files and produces a chunked, metadata-rich document description. This is especially helpful for retrieval-augmented generation! </p> <p>We have also received some additional cool feature requests on GitHub, which we would like to implement. If you want to help out please check out EmbedAnything on GitHub. We would love to have a contribution. \ud83d\ude80</p>"},{"location":"blog/2025/01/25/smolagent/","title":"In-and-Out of domain query with EmbedAnything and SmolAgent","text":"<p>When working with domain-specific queries, we often struggle with the challenge of balancing in-domain and out-of-domain requests. But not anymore! With embedanything, you can leverage fine-tuned, domain-focused models while smolagent takes the lead in smart decision-making. Whether you're handling queries from different domains or need to combine their insights seamlessly, smolagent ensures smooth collaboration, merging responses for a unified, accurate answer.</p> <p>But first let\u2019s discuss what is SmolAgent and then we can discuss each retrieval :</p> <p>According to Hugging-face\u2019s official release agents are:</p> <pre><code>AI Agents are\u00a0**programs where LLM outputs control the workflow**.\n</code></pre> <p>Any system leveraging LLMs will integrate the LLM outputs into code. The influence of the LLM\u2019s input on the code workflow is the level of agency of LLMs in the system.</p>"},{"location":"blog/2025/01/25/smolagent/#an-example","title":"An Example:","text":"<p>This agentic system runs in a loop, executing a new action at each step (the action can involve calling some pre-determined\u00a0tools\u00a0that are just functions), until its observations make it apparent that a satisfactory state has been reached to solve the given task. Here\u2019s an example of how a multi-step agent can solve a simple math question:</p> <p></p>"},{"location":"blog/2025/01/25/smolagent/#how-its-working-with-embedanything","title":"How it\u2019s working with EmbedAnything","text":"<p>Embedanything is used to take docs from source, to inference domain specific and general models and generate embeddings. It takes care of chunking and cleaning for parsing documents.</p> <p>Embed anything is a rust-based framework for the Ingestion of any file type in any modality, Inference of any model present in HF with their HF link using a candle and some Onnx models, and then indexing them to the vector database. It excels at three core functions:</p> <ol> <li>Document Processing: Automatically handles document intake from various sources, cleaning the text and removing irrelevant content to ensure quality input.</li> <li>Intelligent Chunking: Breaks down documents into optimal segments while preserving context and meaning, ensuring the resulting embeddings capture the full semantic value of the content.</li> <li>Flexible Model Integration: Seamlessly works with both general-purpose language models and specialized domain-specific models, allowing users to generate embeddings that best suit their specific use case.</li> </ol> <p>This streamlined pipeline eliminates the usual complexity of document embedding workflows, making it easier to prepare data for downstream tasks like semantic search, document retrieval, or content recommendation systems.</p> <p></p>"},{"location":"blog/2025/01/25/smolagent/#lets-get-into-the-code","title":"Let\u2019s get into the code:","text":"<p>In the accompanying diagram, we showcase two distinct folders containing different types of documents: one for general information and the other for domain-specific content\u2014for example, medicine-related documents.</p> <p>For domain-specific queries, we use a PubMed fine-tuned model, while for general queries, we rely on an ONNX model through embedanything. When a query is received, smolagent intelligently decides which tool to use based on the query's nature. It then processes the relevant parts of the query, performs retrieval, and rephrases the results to deliver a final, cohesive answer.</p> <p>Now, let\u2019s dive into the retrieval code and explore how this process works behind the scenes!</p> <pre><code>class RetrieverTool(Tool):\n    name = \"retriever\"\n    description = \"Uses semantic search to retrieve policies about india that could be most relevant to answer your query.\"\n    inputs = {\n        \"query\": {\n            \"type\": \"string\",\n            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n        }\n    }\n    output_type = \"string\"\n\n    def __init__(self, directory, **kwargs):\n        super().__init__(**kwargs)\n        self.model = EmbeddingModel.from_pretrained_onnx(WhichModel.Bert, ONNXModel.AllMiniLML6V2Q)\n        self.connection = lancedb.connect(\"tmp/general\")\n        if \"docs\" in self.connection.table_names():\n            self.table = self.connection.open_table(\"docs\")\n        else:\n            self.embeddings = embed_anything.embed_directory(directory, embedder = self.model)\n            docs = []\n            for e in self.embeddings:\n                docs.append({\n                    \"vector\": e.embedding,\n                    \"text\": e.text,\n                    \"id\": str(uuid4())\n                })\n            self.table = self.connection.create_table(\"docs\", docs)\n\n    def forward(self, query: str) -&gt; str:\n        assert isinstance(query, str), \"Your search query must be a string\"\n\n        query_vec = embed_anything.embed_query([query], embedder = self.model)[0].embedding\n        docs = self.table.search(query_vec).limit(5).to_pandas()[\"text\"]\n        return \"\\nRetrieved documents:\\n\" + \"\".join(\n            [f\"\\n\\n===== Document {str(i)} =====\\n\" + doc for i, doc in enumerate(docs)]\n        )\n</code></pre> <p>Let\u2019s begin by setting up a general query retrieval tool. This process generates embeddings through embed anything using an ONNX model for inference and then saves these embeddings to a LanceDB database. One key point to keep in mind: ensure you create separate tables for different domains in LanceDB. This structure allows for better organization and efficient retrieval. For each domain, you can also fine-tune different models tailored to that specific domain. In the next steps, we\u2019ll explore how to effectively handle in-domain queries to achieve precise and context-aware results.</p>"},{"location":"blog/2025/01/25/smolagent/#for-domain-specific-models","title":"For Domain Specific models","text":"<p>For domain-specific models, we are using Candle because it allows any fine-tuned model to run if it has a similar architecture. Three things have changed.</p> <ol> <li> <p>Model used is 'NeuML/pubmedbert-base-embeddings'</p> </li> <li> <p>EmbedAnything function: from pretrained hf</p> </li> <li>Lance DB table: tmp/medical</li> </ol> <p>``</p> <pre><code>self.model =EmbeddingModel.from_pretrained_hf(WhichModel.Bert, model_id='NeuML/pubmedbert-base-embeddings')\nself.connection = lancedb.connect(\"tmp/medical\")\n</code></pre>"},{"location":"blog/2025/01/25/smolagent/#run-smolagent","title":"Run SmolAgent","text":"<p>Finally, you'll need to provide the necessary tools and downloaded folders to smolagent. Here's an example: when given a single sentence containing two distinct queries\u2014one general and the other specific to radiology\u2014smolagent breaks it down intelligently. It retrieves answers for the general query from policy-related documents and addresses the radiology-specific query using relevant medical documents. Once processed, it seamlessly merges the results into a cohesive and accurate response.</p> <pre><code>retriever_tool = RetrieverTool(\"downloaded_docs\")\nmedical_tool = MedicalRetrieverTool(\"medical_docs\")\n\nagent = CodeAgent(\n    tools=[retriever_tool, medical_tool],\n    model=OpenAIServerModel(model_id = \"gpt-4o-mini\", api_base = \"https://api.openai.com/v1/\", api_key = api_key),\n    verbosity_level=2,\n)\n\nagent_output = agent.run(\"What are the different policies for indian manufacturing and what are the medical risks of radiotherapy?\")\n</code></pre>"},{"location":"blog/2025/01/25/smolagent/#output","title":"Output","text":"<p>The output generated involves multiple well-defined steps:</p> <ol> <li>Query Breakdown: The system first analyzes the query and breaks it into relevant components. </li> <li>Understanding Retrieval Needs: It identifies that the first part of the query requires general retrieval. </li> <li>Running General Retrieval Tools: The system runs general retrieval tools to gather context for the general query.</li> <li>Domain-Specific Retrieval: After obtaining the general context, it processes the second part of the query, performing retrieval on domain-specific data to gather the necessary insights.</li> <li>Answer Merging: Finally, it combines the results from both retrievals into a unified, coherent answer.</li> </ol> <p>This seamless workflow ensures precise handling of complex, multi-domain queries while maintaining context relevance across all steps.</p> <p></p> <p>Check out our </p> <p></p>"},{"location":"blog/2025/02/25/vector%20database/","title":"How to write adapters for your vector database.","text":"<p>We have received multiple requests to add different vector database for our vector streaming. So We have decided to put a detailed guide for different vector databases out there. We are happy to accept pull-request.</p>"},{"location":"blog/2025/02/25/vector%20database/#creating-custom-adapters-for-embedanything-a-step-by-step-guide","title":"Creating Custom Adapters for EmbedAnything: A Step-by-Step Guide","text":"<p>In the world of machine learning and natural language processing, working with embeddings has become a fundamental task. The EmbedAnything library simplifies the process of generating embeddings from various data sources, but what if you want to store these embeddings in a specific database or service? This is where adapters come in. In this blog post, we'll walk through the process of creating a custom adapter for the EmbedAnything library, using the Pinecone vector database as an example.</p>"},{"location":"blog/2025/02/25/vector%20database/#understanding-adapters-in-embedanything","title":"Understanding Adapters in EmbedAnything","text":"<p>Adapters serve as bridges between the EmbedAnything library and external services or databases. They handle the conversion and storage of embeddings, allowing you to seamlessly integrate EmbedAnything with your preferred storage solution.</p>"},{"location":"blog/2025/02/25/vector%20database/#the-anatomy-of-an-adapter","title":"The Anatomy of an Adapter","text":"<p>Before diving into the code, let's understand the key components of an adapter:</p> <ol> <li>Initialization: Setting up the connection to the external service</li> <li>Index Management: Creating and deleting indices in the external service</li> <li>Data Conversion: Transforming EmbedAnything's embedding format to the format required by the external service</li> <li>Data Storage: Storing the converted embeddings in the external service</li> </ol>"},{"location":"blog/2025/02/25/vector%20database/#creating-a-pinecone-adapter-step-by-step","title":"Creating a Pinecone Adapter: Step by Step","text":"<p>Let's break down the process of creating a Pinecone adapter for EmbedAnything:</p>"},{"location":"blog/2025/02/25/vector%20database/#step-1-set-up-the-basic-class-structure","title":"Step 1: Set Up the Basic Class Structure","text":"<pre><code>from embed_anything import EmbedData, EmbeddingModel, WhichModel, TextEmbedConfig\n\nclass PineconeAdapter(Adapter):\n    \"\"\"\n    Adapter class for interacting with Pinecone, a vector database service.\n    \"\"\"\n    def __init__(self, api_key: str):\n        \"\"\"\n        Initializes a new instance of the PineconeAdapter class.\n\n        Args:\n            api_key (str): The API key for accessing the Pinecone service.\n        \"\"\"\n        super().__init__(api_key)\n        self.pc = Pinecone(api_key=self.api_key)\n        self.index_name = None\n</code></pre> <p>In this step, we're: - Inheriting from the base <code>Adapter</code> class provided by EmbedAnything - Initializing the Pinecone client using the provided API key - Setting up an attribute to track the current index name</p>"},{"location":"blog/2025/02/25/vector%20database/#step-2-implement-index-management-methods","title":"Step 2: Implement Index Management Methods","text":"<pre><code>def create_index(\n    self, \n    dimension: int, \n    metric: str = \"cosine\", \n    index_name: str = \"anything\",\n    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n):\n    \"\"\"\n    Creates a new index in Pinecone.\n\n    Args:\n        dimension (int): The dimensionality of the embeddings.\n        metric (str, optional): The distance metric to use for similarity search. Defaults to \"cosine\".\n        index_name (str, optional): The name of the index. Defaults to \"anything\".\n        spec (ServerlessSpec, optional): The serverless specification for the index. Defaults to AWS in us-east-1 region.\n    \"\"\"\n    self.index_name = index_name\n    self.pc.create_index(\n        name=index_name,\n        dimension=dimension,\n        metric=metric,\n        spec=spec\n    )\n\ndef delete_index(self, index_name: str):\n    \"\"\"\n    Deletes an existing index from Pinecone.\n\n    Args:\n        index_name (str): The name of the index to delete.\n    \"\"\"\n    self.pc.delete_index(name=index_name)\n</code></pre> <p>These methods handle: - Creating a new index in Pinecone with the specified dimensions and distance metric - Deleting an existing index when needed - Storing the current index name for later use</p>"},{"location":"blog/2025/02/25/vector%20database/#step-3-implement-data-conversion-logic","title":"Step 3: Implement Data Conversion Logic","text":"<pre><code>def convert(self, embeddings: List[EmbedData]) -&gt; List[Dict]:\n    \"\"\"\n    Converts a list of embeddings into the required format for upserting into Pinecone.\n\n    Args:\n        embeddings (List[EmbedData]): The list of embeddings to convert.\n\n    Returns:\n        List[Dict]: The converted data in the required format for upserting into Pinecone.\n    \"\"\"\n    data_emb = []\n    for embedding in embeddings:\n        data_emb.append(\n            {\n                \"id\": str(uuid.uuid4()),\n                \"values\": embedding.embedding,\n                \"metadata\": {\n                    \"text\": embedding.text,\n                    \"file\": re.split(\n                        r\"/|\\\\\", embedding.metadata.get(\"file_name\", \"\")\n                    )[-1],\n                },\n            }\n        )\n    return data_emb\n</code></pre> <p>This method: - Takes a list of <code>EmbedData</code> objects from EmbedAnything - Converts each embedding into the format expected by Pinecone - Generates a unique ID for each embedding - Extracts and formats metadata from the original embedding</p>"},{"location":"blog/2025/02/25/vector%20database/#step-4-implement-storage-logic","title":"Step 4: Implement Storage Logic","text":"<pre><code>def upsert(self, data: List[Dict]):\n    \"\"\"\n    Upserts data into the specified index in Pinecone.\n\n    Args:\n        data (List[Dict]): The data to upsert into Pinecone.\n\n    Raises:\n        ValueError: If the index has not been created before upserting data.\n    \"\"\"\n    data = self.convert(data)\n    if not self.index_name:\n        raise ValueError(\"Index must be created before upserting data\")\n    self.pc.Index(name=self.index_name).upsert(data)\n</code></pre> <p>This method: - Converts the input data using the <code>convert</code> method - Checks if an index has been created before attempting to upsert data - Upserts the converted data into the specified Pinecone index</p>"},{"location":"blog/2025/02/25/vector%20database/#using-your-custom-adapter","title":"Using Your Custom Adapter","text":"<p>Once you've created your adapter, you can use it with EmbedAnything like this:</p> <pre><code># Initialize the PineconeEmbedder class\napi_key = os.environ.get(\"PINECONE_API_KEY\")\nindex_name = \"anything\"\npinecone_adapter = PineconeAdapter(api_key)\n\n# Delete existing index if it exists\ntry:\n    pinecone_adapter.delete_index(\"anything\")\nexcept:\n    pass\n\n# Create a new index\npinecone_adapter.create_index(dimension=512, metric=\"cosine\")\n\n# Initialize the embedding model\nmodel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Bert, \n    model_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n)\n\n# Embed a PDF file\ndata = embed_anything.embed_file(\n    \"test-file\",\n    embedder=model,\n    adapter=pinecone_adapter,\n)\n\n# Embed all images in a directory\ndata = embed_anything.embed_image_directory(\n    \"test_files\",\n    embedder=model,\n    adapter=pinecone_adapter\n)\n\nprint(data)\n</code></pre>"},{"location":"blog/2025/02/25/vector%20database/#conclusion","title":"Conclusion","text":"<p>Creating custom adapters for EmbedAnything allows you to seamlessly integrate the library with your preferred storage solutions. By following the step-by-step guide and best practices outlined in this blog post, you can create robust and efficient adapters that enhance your embedding workflow.</p> <p>Remember, the key to a good adapter is clear documentation, robust error handling, and efficient data conversion. With these principles in mind, you can extend EmbedAnything to work with virtually any storage solution.</p> <p>Happy coding!</p>"},{"location":"blog/2025/12/11/Year-in-review-2025/","title":"Beyond EmbedAnything: A Year of Growth Beyond Expectations!!","text":"<p>Reflecting on EmbedAnything: A Year Later</p> <p>A year ago, I shared the story behind EmbedAnything\u2014how we built and scaled an embedding infrastructure that has since been loved by developers at Microsoft, Meta, Tencent, AWS, ByteDance and RedHat. In that post, I documented our journey: the technical decisions that enabled massive scale, the enterprise collaborations that shaped our direction, and our unwavering commitment to building best-in-class infrastructure for RAG and agentic systems.</p> <p></p> <p>I also made a point to be transparent about our mistakes alongside our successes. Building great products requires learning from what doesn't work as much as celebrating what does. If you're interested in the technical architecture, scaling challenges, or the candid lessons we learned along the way, https://embed-anything.com/blog/2024/12/15/embed-anything/.</p>"},{"location":"blog/2025/12/11/Year-in-review-2025/#a-year-of-growth-beyond-expectations","title":"A Year of Growth Beyond Expectations","text":"<p>This past year exceeded my expectations in ways I hadn't anticipated. </p> <p>By the numbers: We grew 3X, and our GitHub star count increased from 200 to 870+. But the metrics that matter most can't be captured in downloads alone.</p> <p>What truly stands out is the organic community that's emerged around EmbedAnything. Developers are actively discussing the project on social media and at conferences, leading to meaningful contributions and technical discussions on our GitHub repository. This kind of engagement is what makes open-source work rewarding.</p> <p></p> <p>I've been fortunate to witness this enthusiasm firsthand. Over the past year, I've had the opportunity to present EmbedAnything at Berlin Buzzwords, PyCon DE, GDG Berlin, and DevFest</p> <p>I've also shared our work in the offices of Google, Deutsche Bank, JetBrains, and Zed, engaging directly with teams using similar technologies and facing comparable challenges.</p> <p>These conversations\u2014whether at conferences, in office visits, or through GitHub issues\u2014have been invaluable in shaping our roadmap and understanding real-world use cases.</p>"},{"location":"blog/2025/12/11/Year-in-review-2025/#some-of-the-contributions-i-want-to-highlight","title":"Some of the contributions I want to highlight","text":"<ol> <li>Jack Bosewell: Processor Crate:  which takes files of different types and produces metadata-rich descriptions, extremely useful for RAG.</li> <li>Taradeepan\u2019s AWS: Now you can embed a file directly from your AWS S3 bucket</li> <li>Contributions from Milvus, Qdrant, and SingleStore: Making us support almost all the vector databases as adapters for vector streaming, making as the standard Infrastructure for RAG.</li> </ol> <p>Others, like adding HF home, endpoint, fixing Qwen instruction prompt, fixing versions, etc</p>"},{"location":"blog/2025/12/11/Year-in-review-2025/#project-highlights","title":"Project highlights","text":"<p>I am only mentioning a few of them that I really liked 1. FogX-Store is a dataset store service that collects and serves large robotics datasets:\u00a0https://github.com/J-HowHuang/FogX-Store \\n 2. A Rust-based cursor like chat with your codebase tool:\u00a0https://github.com/timpratim/cargo-chat \\n 3. Semantic file tracker in CLI operated through a daemon built with rust.:\u00a0https://github.com/sam-salehi/sophist \\n</p> <p>Find the full list here: https://github.com/StarlightSearch/EmbedAnything?tab=readme-ov-file#awesome-projects-built-on-embedanything</p>"},{"location":"blog/2025/12/11/Year-in-review-2025/#things-that-we-did-right","title":"Things that we did right:","text":"<ol> <li> <p>StarlightSearch Discussions, community feedback has always been our compass. It's what enabled us to build solutions that truly differentiate us in the market. Our GitHub discussions are a testament to this\u2014they reflect real conversations with real developers solving real problems, and they directly inform our roadmap.</p> </li> <li> <p>Becoming the Standard Infrastructure for RAG with vector Streaming. One of the most validating aspects of EmbedAnything's growth has been the collaboration with the vector database ecosystem. Nearly every major vector database provider has contributed adapters to our library, giving developers unprecedented modularity in their tech stack choices.</p> </li> </ol> <p></p> <p>Milvus even published a dedicated blog post about EmbedAnything, highlighting how our library integrates with their platform. This kind of recognition from established players in the space validates both our technical approach and the value we're providing to the developer community.</p> <p>Expanding into Agents and Reinforcement Learning</p> <p>This year, you asked for agentic capabilities, and we delivered\u2014built in Rust for performance and reliability. As the Model Context Protocol (MCP) gained traction and context engineering became increasingly critical, we focused our efforts on two key areas:</p> <ol> <li>CodeAct implementation for more sophisticated agent behaviors</li> <li>Reinforcement learning training for domain-specific agents, including self-improvement capabilities based on SearchR1 architectures</li> </ol> <p>First Real-World Validation: Our First On-Premise Deployment</p> <p>This year, we deployed our search product on-premises at the Serpentine, student tech group at TU/e to test scalability and real-time response capabilities under production conditions.</p> <p>The results exceeded our expectations. Seeing StarlightSearch perform in a live environment\u2014handling real queries, scaling to actual demand, and delivering answers in real-time\u2014validated everything we've built.</p> <p>What we've created isn't just technically sound on paper. It works, it scales, and it delivers results that feel transformative in practice.</p> <p>This deployment gave us invaluable insights into production requirements and confirmed that our architecture can handle enterprise-scale workloads. More importantly, it demonstrated that the performance characteristics we've prioritized\u2014speed, accuracy, and reliability\u2014translate directly into user value.</p> <p>Writing More Technical Contents</p> <p>After every talk I presented, I found out some knowledge gap, that still exists in people's mind and wrote a technical blog about it. Read our blogs on memory leak, configuring textembedconfig for embedanything, ColPali and Fusion DeepSearch --&gt; Here.</p> <p>And Subscribe to our newletter to get notification for upcoming blogs on Agents and Reinformcement Learning.</p>"},{"location":"blog/2025/12/11/Year-in-review-2025/#a-difficult-decision-closing-lumo","title":"A Difficult Decision: Closing Lumo","text":"<p>We made the difficult decision to close-source Lumo, our agentic AI framework. This decision wasn't made lightly, and I want to be transparent about why.</p> <p>EmbedAnything has been extensively copied by well-funded companies\u2014organizations with substantial resources but limited interest in genuine innovation or community engagement. Some have replicated not just technical concepts, but marketing materials verbatim. While I'm choosing not to name them now, I'm documenting these instances(unethical practices) and may address them directly in the future.</p> <p>Why This Matters</p> <p>The open-source ecosystem thrives on good-faith collaboration and attribution. When companies with significant funding simply replicate rather than innovate, it creates a challenging environment for original creators operating with minimal resources (our entire investment in EmbedAnything remains \u20ac30 in stickers).</p> <p>Moving Forward</p> <p>This situation has been challenging, but it hasn't stopped our momentum. We remain committed to:</p> <ul> <li>Building differentiated products that solve real problems</li> <li>Maintaining transparency with our community</li> <li>Continuing to listen and respond to developer needs</li> </ul> <p>I'm asking the open-source community to support original projects and creators. creativity, empathy, and genuine community engagement can't be purchased or copied. The organisation who copied us will always remain a second-class copy of embedanything.</p>"},{"location":"blog/2025/12/11/Year-in-review-2025/#mistakes-and-learnings","title":"Mistakes and learnings.","text":"<p>My biggest mistake this year was delaying our sales efforts. Without the cushion of venture funding, we need revenue to sustain our open-source work\u2014it's that simple. While we entered the agentic AI market later than some competitors, we're strategically positioned in reinforcement learning, where we believe the real differentiation lies.</p> <p>The Rust Advantage\u2014and Its Challenges</p> <p>Building in Rust has given us significant advantages, particularly the ability to compile for any hardware architecture. This opens multiple market opportunities: consumer edge applications, robotics, and physical AI systems.</p> <p>However, each market presents unique challenges:</p> <ul> <li>Consumer applications require substantial investment in habit formation and user education. The common objection\u2014\"Why not just use OpenAI?\"\u2014misses the point about cost optimization and data privacy, but overcoming that perception requires resources we're currently building toward.</li> <li>Robotics and physical AI are compelling, but require deep vertical focus.</li> </ul> <p>As the saying goes: when you build for everyone, you build for no one. We're focused on identifying our core audience\u2014developers and organizations that prioritize high-performance infrastructure and understand its value. Not every team needs what we've built, and that's perfectly fine. Our goal is to serve those who do exceptionally well.</p> <p>The Incorporation Timing Mistake</p> <p>I delayed incorporating StarlightSearch, waiting to secure our first customer before making it official. In retrospect, this was backward thinking. When you're gaining significant community traction and industry attention, incorporation provides credibility and trust that can actually accelerate customer acquisition.</p>"},{"location":"blog/2025/12/11/Year-in-review-2025/#moving-forward-starlightsearch-is-ready","title":"Moving Forward: StarlightSearch is Ready","text":"<p>I'm pleased to announce that StarlightSearch is in the process of incorporation and ready to serve high-performance Infrastructure.</p> <p>I'm driven by a genuine passion for building exceptional products, advancing AI capabilities, and embracing emerging technologies. While I may not have a traditional business school background or claim any particular title like a product \u201cleader\u201d, my approach is grounded in a few core principles:</p> <p></p> <p>Transparency: I believe in open communication about our direction, decisions, and even our mistakes. You'll always know where we stand.</p> <p>Collaboration: Your input matters. I'm committed to actively listening to your needs, feedback, and ideas. Collaborate with enterprises, small or large, and have their inputs reach to you.</p> <p>Excellence: Our products will reflect our dedication to quality and innovation. This isn't just a promise\u2014it's a standard I hold myself accountable to every day.</p> <p>I'm undeterred in this mission and focused on what truly matters: delivering value and building something meaningful together.</p> <p>What do you need from us? I'm listening\u2026</p>"},{"location":"blog/2024/12/31/colpali-vision-rag/","title":"Optimize VLM Tokens with EmbedAnything x ColPali","text":"<p>ColPali, a late-interaction vision model, leverages this power to enable text searches within images. This means you can pinpoint the exact pages in a PDF containing relevant text, even if the text exists only as part of an image. For example, suppose you have hundreds of pages in a PDF and even hundreds of PDFs. In that case, ColPali can identify the specific pages matching a query\u2014an impressive feat for streamlining information retrieval. This system is widely come to be known as Vision RAG. </p> <p></p> <p>However, due to its computational demands, running the ColPali model directly on a local machine might not always be feasible. To address this, we developed a quantized version of ColPali. Quantization reduces the precision of the model's weights, significantly lowering computational and memory requirements. Despite this optimization, the quantized model maintains performance nearly equivalent to the original.</p>"},{"location":"blog/2024/12/31/colpali-vision-rag/#what-is-vision-rag","title":"What is Vision RAG?","text":"<p>Let\u2019s look a bit deeper into what Vision RAG is. Traditional RAG methods use text throughout the pipeline. They store text chunks and their embeddings in a vector database and then retrieve these chunks for further downstream tasks. A simplest / naive RAG attaches these chunks as context to the original query and aims to provide more information to the model. There are two problems here. One is that getting text from many data sources may not be possible. Think about scanned PDFs or documents with many graphics, like design pamphlets, etc. The traditional RAG falls apart if any documents you work with are like this. A bandaid to the problem is to use OCR engines to somehow extract text. This adds additional moving parts to the process, and OCR engines are pretty fragile.  The second problem, even if you manage to get the text, is the chunking process. Again, how do you decide what the chunk size should be and what the overlap should be? Even if you find optimal parameters for a few documents, will they hold for new ones? All these parameters add to the design space, and the RAG performance needs to be continuously evaluated based on these design choices. Vision RAG tries to solve this by removing the whole chunking process from the system and instead storing the image as a multi-vector embedding in the database. When there is a query, a Late Interaction Score (LIS), similar to the classical cosine similarity but for multi-vector, is measured, and the DB returns the document pages with the highest LIS scores. These documents can now be sent to a Vision Language Model (VLM) along with the original query to get the answer to the questions. The image below shows this process from start to end. Since vision language models are more expensive than text models, Vision RAG is even more important because you don\u2019t have to send complete PDFs to the model. You are just sending the relevant pages. This can save a lot of costs. The document embedding generation happens offline and is taken care of by EmbedAnything. One drawback with this approach is that not all vector databases today support storing multi-vectors. A few that support these are Qdrant and Vespa. </p> <p></p> <p>Let us look at how you can use Colpali models with EmbedAnything and convert PDFs into multi-vector embeddings. In this example, we will not use a vector database but find the late interaction score of the query against all the pages. </p>"},{"location":"blog/2024/12/31/colpali-vision-rag/#step-1-install-the-dependencies","title":"Step 1: Install the dependencies","text":"<p>Since we are going to convert pdfs into images, we need poppler-utils. </p> <p>EmbedAnything requires poppler to convert pdfs to images. So make sure you have it installed.</p> <ul> <li>For Linux:</li> </ul> <pre><code>apt install poppler-utils\n</code></pre> <ul> <li>For Mac</li> </ul> <pre><code>brew install poppler\n</code></pre> <ul> <li>For Windows</li> </ul> <p>https://github.com/oschwartz10612/poppler-windows/releases/tag/v24.08.0-0 Download the binary from here, unzip it and add the <code>bin</code> folder to your system path.</p> <p>Using the GPU version of EmbedAnything is highly recommended because ColPali is based on paligemma and requires a computation like any other small language model. </p> <pre><code>pip install embed-anything-gpu tabulate openai\n</code></pre> <p>Let\u2019s import EmbedAnything and the other dependencies:</p> <pre><code>import base64\nfrom embed_anything import EmbedData, ColpaliModel\nimport numpy as np\nfrom tabulate import tabulate\nfrom pathlib import Path\nfrom PIL import Image\nimport io\nimport matplotlib.pyplot as plt\nimport openai\nimport os\n</code></pre>"},{"location":"blog/2024/12/31/colpali-vision-rag/#step-2-get-the-files-that-need-to-be-indexed","title":"Step 2: Get the files that need to be indexed","text":"<p>For this demo, we will clone the EmbedAnything repo which has some test pdfs with the \u201cAttention is all you need\u201d and a Mistral paper. </p> <pre><code>if not os.path.exists(\"EmbedAnything\"):\n  !git clone https://github.com/StarlightSearch/EmbedAnything.gi\n</code></pre>"},{"location":"blog/2024/12/31/colpali-vision-rag/#step-3-load-the-colpali-onnx-model","title":"Step 3 : Load the ColPali Onnx Model","text":"<p>Use the <code>embed_anything</code> function with <code>from_pretrained_onnx</code> to load the ColPali Onnx model from the specified link. This initializes the model for embedding tasks. If you are using a python notebook, this can take some time because the model is being downloaded. Unfortunately, the progress bar is not visible on a notebook. You can also load the original Colpali model and not the <code>onnx</code> model using the <code>from_pretrained_hf</code> function. </p> <pre><code>model: ColpaliModel = ColpaliModel.from_pretrained_onnx(\"starlight-ai/colpali-v1.2-merged-onnx\", None)\n</code></pre>"},{"location":"blog/2024/12/31/colpali-vision-rag/#step-4-load-the-files-and-embed-them","title":"Step 4: Load the files and embed them.","text":"<p>Now, we just load all the files from the directory with a PDF extension. Then, for each file, we run the <code>embed_file</code> function with a <code>batch_size</code> of 1. You can increase the batch size if you have higher VRAM, but one works well. </p> <pre><code>directory = Path(\"EmbedAnything/test_files\")\nfiles = list(directory.glob(\"*.pdf\"))\nfile_embed_data: list[EmbedData] = []\nfor file in files:\n    try:\n        embedding: list[EmbedData] = model.embed_file(str(file), batch_size=1)\n        file_embed_data.extend(embedding)\n    except Exception as e:\n        print(f\"Error embedding file {file}: {e}\")\nfile_embeddings = np.array([e.embedding for e in file_embed_data])\nprint(\"Embedded Files: \", files)\n</code></pre> <p><code>file_embeddings</code> is a list of <code>EmbedData</code> object which contains other metadata along with the embeddings like page number, file name and the image of the page in string base64 format. You can now store these embeddings in a vector database of choice. </p>"},{"location":"blog/2024/12/31/colpali-vision-rag/#step-5-process-the-query","title":"Step 5: Process the query","text":"<p>We do the same for the query as well using <code>embed_query</code> function. </p> <pre><code>query = \"What is positional encoding?\"\nquery_embedding = model.embed_query(query)\nquery_embeddings = np.array([e.embedding for e in query_embedding])\n</code></pre>"},{"location":"blog/2024/12/31/colpali-vision-rag/#step-6-compute-similarity-scores","title":"Step 6: Compute Similarity Scores","text":"<p>We can calculate the Late Interaction Score between query and file embeddings using the Einstein summation function. This identifies the most relevant pages based on the highest scores. Extract the top 3 pages for further processing. We also take out the <code>image</code> field from the <code>EmbedData</code> object of the embeddings. This is a base64 string representation of the image that will send to GPT. </p> <pre><code>def score(query_embeddings, file_embed_data):\n    file_embeddings = np.array([e.embedding for e in file_embed_data])\n    scores = np.einsum(\"bnd,csd-&gt;bcns\", query_embeddings, file_embeddings).max(axis=3).sum(axis=2).squeeze()\n\n    # Get top pages\n    top_pages = np.argsort(scores)[::-1][:3]\n\n    # Extract file names and page numbers\n    table = [\n        [file_embed_data[page].metadata[\"file_path\"].split(\"/\")[-1], file_embed_data[page].metadata[\"page_number\"]]\n        for page in top_pages\n    ]\n\n    # Print the results in a table\n    print(tabulate(table, headers=[\"File Name\", \"Page Number\"], tablefmt=\"grid\"))\n    results_str = tabulate(table, headers=[\"File Name\", \"Page Number\"], tablefmt=\"grid\")\n\n    images = [file_embed_data[page].metadata[\"image\"] for page in top_pages]\n    images_pil = [Image.open(io.BytesIO(base64.b64decode(image))) for image in images]\n    return images_pil, results_str, images_str\n</code></pre> <p>The result will look something like this:</p> <pre><code>+----------------------------------------+---------------+\n| File Name                              |   Page Number |\n+========================================+===============+\n| EmbedAnything/test_files/attention.pdf |             6 |\n+----------------------------------------+---------------+\n| EmbedAnything/test_files/attention.pdf |             9 |\n+----------------------------------------+---------------+\n| EmbedAnything/test_files/linear.pdf    |            34 |\n+----------------------------------------+---------------+\n| EmbedAnything/test_files/attention.pdf |             3 |\n+----------------------------------------+---------------+\n| EmbedAnything/test_files/attention.pdf |            15 |\n+----------------------------------------+---------------+\n</code></pre> <p>We can visualize the top 3 pages using this command </p> <p></p>"},{"location":"blog/2024/12/31/colpali-vision-rag/#step-7-sent-these-images-to-openai","title":"Step 7: Sent these images to OpenAI","text":"<p>Now we can send these top 3 retrieved images to OpenAI gpt-4o-mini model along with the original query. You can add further instructions for the model here as per your needs. Don\u2019t forget to add your OpenAI key to the client. </p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(api_key = &lt;openai-key&gt; )\n\nimage_contents = [\n    {\n        \"type\": \"image_url\",\n        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_str}\"}\n    }\n    for image_str in images_str\n]\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": query},\n            ] + image_contents,\n        }\n    ],\n\n)\n</code></pre> <p>The output looks like this</p> <pre><code>Positional encoding is a critical concept in transformer models, which addresses the inherent limitation of self-attention mechanisms: they do not consider the order of input tokens. Since transformers process all tokens simultaneously, they require a way to encode the order of tokens in a sequence to maintain their relative positions.\n\n### Key Aspects of Positional Encoding:\n\n1. **Purpose**: It helps the model understand the sequence of data since transformers lack recurrence or convolution that traditionally encode this information.\n\n2. **Method**: \n   - Positional encodings are added to the input embeddings of tokens.\n   - A common approach is to use sine and cosine functions of different frequencies, defined mathematically as:\n\n     \\[\n     PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n     \\]\n     \\[\n     PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n     \\]\n\n   - Here, \\( pos \\) is the position of the token, \\( i \\) is the dimension, and \\( d_{model} \\) is the dimensionality of the embedding.\n\n3. **Frequency**: The functions allow for various wavelengths, making it possible to learn relationships at different scales, which enables the model to understand both short-range and long-range dependencies in the sequence.\n\n4. **Alternatives**: While sinusoidal encodings are widely used, learned positional embeddings can also be employed, which allows the model to learn the optimal way to encode positions during training.\ny\nIn summary, positional encoding is vital for allowing transformer models to grasp the order of tokens in sequences, facilitating effective learning from sequential data.\n</code></pre> <p>This response used a total of 2500 tokens which translates to $0.006. If we would have sent the entire <code>pdf</code> of 15 pages, without retrieval to the model, it would have cost about 12,500 tokens which is five times higher than this system. And this is assuming we know which <code>pdf</code> to send. Also the response may not be accurate because the model has too much unnecessary information to filter out.</p> <p>Check out the demo notebook at </p> <p></p>"},{"location":"blog/2025/09/15/semantic-late-chunking/","title":"How to Configure TextEmbedConfig in EmbedAnything","text":"<p>After presenting at Google, PyCon DE, Berlin Buzzwords, and GDG Berlin, I was surprised by how many people approached me with questions about writing configurations, chunk sizes, and batch sizes for EmbedAnything. Since I had never specifically covered this topic in my talks or blog posts, I decided to create this comprehensive guide to clarify these concepts and explain how we handle your chunking strategy with vector streaming.</p>"},{"location":"blog/2025/09/15/semantic-late-chunking/#understanding-textembedconfig","title":"Understanding TextEmbedConfig","text":"<p>TextEmbedConfig consists of three essential components that work together to optimize your text embedding process:</p> <ol> <li>The embedding model - defines how text is converted to vectors</li> <li>Splitting strategy with chunk size - determines how documents are divided</li> <li>Batch size - controls vector streaming performance</li> </ol> <p>Let's explore each component in detail.</p>"},{"location":"blog/2025/09/15/semantic-late-chunking/#setting-up-the-embedding-model","title":"Setting Up the Embedding Model","text":"<p>The foundation of any embedding configuration is the model itself. Here's how to initialize an embedding model:</p> <pre><code>model = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Bert, \n    model_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n)\n</code></pre> <p>This example uses a BERT-based model from Hugging Face, but you can choose from various architectures depending on your specific needs.</p>"},{"location":"blog/2025/09/15/semantic-late-chunking/#splitting-strategies-and-chunk-size","title":"Splitting Strategies and Chunk Size","text":"<p>EmbedAnything offers multiple splitting strategies, each designed for different use cases. The two primary approaches are semantic chunking and sentence-based splitting.</p>"},{"location":"blog/2025/09/15/semantic-late-chunking/#semantic-chunking","title":"Semantic Chunking","text":"<p>Semantic chunking groups similar content together based on meaning rather than arbitrary boundaries. This approach requires a semantic encoder to determine content similarity.</p> <p>First, set up your semantic encoder:</p> <pre><code>semantic_encoder = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Jina, \n    model_id=\"jinaai/jina-embeddings-v2-small-en\"\n)\n</code></pre> <p>Then configure TextEmbedConfig with semantic chunking:</p> <pre><code>config = TextEmbedConfig(\n    chunk_size=1000,\n    batch_size=32,\n    splitting_strategy=\"semantic\",\n    semantic_encoder=semantic_encoder\n)\n</code></pre>"},{"location":"blog/2025/09/15/semantic-late-chunking/#late-chunking-strategy","title":"Late Chunking Strategy","text":"<p>Late chunking is an advanced technique that preserves contextual relationships throughout the entire document during the embedding process. The document is embedded as a whole first, then divided into chunks while maintaining rich contextual information.</p> <pre><code>config = TextEmbedConfig(\n    chunk_size=1000,\n    batch_size=8,\n    splitting_strategy=\"sentence\",\n    late_chunking=True\n)\n</code></pre> <p>Key benefits of late chunking: - Maintains contextual relationships across the entire document - Produces more meaningful embeddings for each chunk - Particularly effective for longer documents with complex relationships</p>"},{"location":"blog/2025/09/15/semantic-late-chunking/#understanding-key-configuration-parameters","title":"Understanding Key Configuration Parameters","text":""},{"location":"blog/2025/09/15/semantic-late-chunking/#chunk-size","title":"Chunk Size","text":"<p>The <code>chunk_size</code> parameter defines the maximum number of characters (or tokens, depending on the model) allowed in each chunk. Consider these factors when setting chunk size:</p> <ul> <li>Smaller chunks: Better for precise retrieval, more granular search results</li> <li>Larger chunks: Better for maintaining context, fewer total chunks to process</li> <li>Model limitations: Ensure chunk size doesn't exceed your embedding model's maximum input length</li> </ul>"},{"location":"blog/2025/09/15/semantic-late-chunking/#batch-size-for-vector-streaming","title":"Batch Size for Vector Streaming","text":"<p>Batch size controls how many chunks the embedding model processes simultaneously. This directly impacts performance and memory usage:</p> <pre><code># Conservative approach for limited resources\nconfig = TextEmbedConfig(chunk_size=1000, batch_size=8, splitting_strategy=\"sentence\")\n\n# Aggressive approach for high-performance systems\nconfig = TextEmbedConfig(chunk_size=1000, batch_size=32, splitting_strategy=\"semantic\")\n</code></pre> <p>Choosing the right batch size: - Smaller batches (4-8): Lower memory usage, more stable processing - Larger batches (16-32): Faster processing, higher memory requirements - Experimentation is key: Test different batch sizes with your specific documents and hardware</p>"},{"location":"blog/2025/09/15/semantic-late-chunking/#splitting-strategy-options","title":"Splitting Strategy Options","text":"<p>EmbedAnything supports several splitting strategies:</p> <ul> <li>\"semantic\": Groups content by meaning (requires semantic encoder)</li> <li>\"sentence\": Splits at sentence boundaries</li> <li>Custom strategies: Can be implemented for specialized use cases</li> </ul>"},{"location":"blog/2025/09/15/semantic-late-chunking/#putting-it-all-together","title":"Putting It All Together","text":"<p>Here's a complete example showing different configuration approaches:</p> <pre><code># Basic semantic chunking configuration\nsemantic_encoder = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Jina, \n    model_id=\"jinaai/jina-embeddings-v2-small-en\"\n)\n\nconfig_semantic = TextEmbedConfig(\n    chunk_size=1000,\n    batch_size=16,\n    splitting_strategy=\"semantic\",\n    semantic_encoder=semantic_encoder\n)\n\n# Late chunking configuration for complex documents\nconfig_late_chunking = TextEmbedConfig(\n    chunk_size=1500,\n    batch_size=8,\n    splitting_strategy=\"sentence\",\n    late_chunking=True\n)\n\n# Simple sentence-based chunking\nconfig_simple = TextEmbedConfig(\n    chunk_size=800,\n    batch_size=24,\n    splitting_strategy=\"sentence\"\n)\n</code></pre>"},{"location":"blog/2025/09/15/semantic-late-chunking/#performance-optimization-tips","title":"Performance Optimization Tips","text":"<ol> <li>Start with default values (chunk_size=1000, batch_size=8) and adjust based on your specific use case</li> <li>Monitor memory usage when increasing batch size</li> <li>Consider your documents' structure when choosing splitting strategy</li> <li>Test retrieval quality with different chunk sizes</li> <li>Profile your pipeline to find the optimal batch size for your hardware</li> </ol>"},{"location":"blog/2025/09/15/semantic-late-chunking/#best-practices","title":"Best Practices","text":"<ul> <li>Use semantic chunking for documents where meaning preservation is crucial</li> <li>Implement late chunking for complex documents with intricate relationships</li> <li>Adjust batch size based on your available memory and processing requirements</li> <li>Regularly evaluate your configuration's impact on both performance and retrieval quality</li> </ul> <p>By understanding these configuration options and their trade-offs, you can optimize EmbedAnything for your specific use case, whether you're processing technical documentation, literary texts, or any other type of content that requires intelligent chunking and embedding.</p>"},{"location":"blog/2024/03/31/embed-anything/","title":"About Embed Anything","text":"<p>EmbedAnything is an open-source Rust/Python framework that lets you generate vector embeddings for any data (text, images, audio) with minimal code. It's blazing fast, memory-efficient, and can handle massive datasets through vector streaming - meaning you can process 10GB+ files without running out of RAM. Whether you're building a search engine or recommendation system, you can start with <code>pip install embed-anything</code> and a few lines of Python. </p> <p></p>"},{"location":"blog/2024/03/31/embed-anything/#introduction","title":"Introduction","text":"<p>Embedding models are essential today. They have become extremely important due to their use in Retrieval-Augmented Generation (RAG), Image Search, Recommendation Systems, and many other applications. EmbedAnything provides a way to embed data from different modalities ranging from audio, images and text in an fast and efficient way. The library is completely written in Rust and leverages the Huggingface Candle library to serve highly performant embedding models like Jina, ColPali, and others. The best part is that there is no dependence on PyTorch or libtorch, making deploying your applications very easy. In this article, we will look at more features EmbedAnything offers. But first, let's see what motivated us to make this library. </p>"},{"location":"blog/2024/03/31/embed-anything/#motivation","title":"Motivation","text":"<p>The AI landscape today is fantastic. New cutting-edge models pop up almost every month. They are more efficient than ever and have excellent capabilities. However, deploying these large models is quite a pain, and there are several frameworks like VLLM, LitServe, and more to make serving LLMS easy. However, most of these solutions do not provide a way to efficiently serve embedding models. Embedding models are challenging because they require a lot of pre-processing even to start the embedding process. For example, embedding a PDF requires extracting the text, chunking it,  embedding it, adding the required metadata, and then pushing these embeddings to a vector database. Solutions like Ollama and FastEmbed exist, which provide embedding features but have drawbacks. Other solutions require PyTorch or Libtorch, which makes the application footprint quite heavy, and thus, deployment is more complicated. Moreover, currently, there are no existing solutions to extract embeddings and custom metadata from various file formats. LangChain offers some solutions, but it is a bulky package, and extracting only the embedding data is difficult. Moreover, LangChain is not very suitable for vision-related tasks. </p> <p>This is where EmbedAnything comes in. It is a lightweight library that allows you to generate embeddings from different file formats and modalities. Currently, EmbedAnything supports text documents, images and audio, with many more formats like video in the pipeline. The idea is to provide an end-to-end solution where you can give the file and get the embeddings with the appropriate metadata.</p> <p>Development of EmbedAnything started with these goals in mind:</p> <ol> <li>Compatibility with Local and Cloud Models: Seamless integration with local and cloud-based embedding models.</li> <li>High-Speed Performance: Fast processing to meet demanding application requirements.</li> <li>Multimodal Capability: Flexibility to handle various modalities.</li> <li>CPU and GPU Compatibility: Performance optimization for both CPU and GPU environments.</li> <li>Lightweight Design: Minimized footprint for efficient resource utilization.</li> </ol>"},{"location":"blog/2024/03/31/embed-anything/#our-solution","title":"Our Solution","text":"<p>Now, let us look at the different ways we are tackling the problems associated with embedding data.</p>"},{"location":"blog/2024/03/31/embed-anything/#keeping-it-local","title":"Keeping it Local","text":"<p>While cloud-based embedding services like OpenAI, Jina, and Mistral offer convenience, many users require the flexibility and control of local embedding models. Here's why local models are crucial for some use cases:</p> <ul> <li>Cost-Effectiveness:\u00a0Cloud services often charge per API call or model usage. Running embeddings locally on your own hardware can significantly reduce costs, especially for projects with frequent or high-volume embedding needs.</li> <li>Data Privacy:\u00a0Certain data, like medical records or financial documents, might be too sensitive to upload to the cloud. Local embedding keeps your data confidential and under your control.</li> <li>Offline Functionality:\u00a0An internet connection isn't always guaranteed. Local models ensure your embedding tasks can run uninterrupted even without an internet connection.</li> </ul>"},{"location":"blog/2024/03/31/embed-anything/#performance","title":"Performance","text":"<p>EmbedAnything is built with Rust. This makes it faster and provides type safety and a much better development experience. But why is speed so crucial in this process?</p> <p>The need for speed:</p> <p>Creating embeddings from files involves two steps that demand significant computational power:</p> <ol> <li>Extracting Text from Files, Especially PDFs:\u00a0Text can exist in different formats such as markdown, PDFs, and Word documents. However, extracting text from PDFs can be challenging and often causes slowdowns. It is especially difficult to extract text in manageable batches as embedding models have a context limit. Breaking the text into paragraphs containing focused information can help. This task is even more compute intensive which using OCR models like Tesseract to extract text.</li> <li>Inferencing on the Transformer Embedding Model:\u00a0The transformer model is usually at the core of the embedding process, but it is known for being computationally expensive. To address this, EmbedAnything utilizes the Candle Framework by Hugging Face, a machine-learning framework built entirely in Rust for optimized performance.</li> </ol>"},{"location":"blog/2024/03/31/embed-anything/#the-benefit-of-rust-for-speed","title":"The Benefit of Rust for Speed","text":"<p>By leveraging Rust for its core functionalities, EmbedAnything offers significant speed advantages: </p> <ul> <li>Rust is Compiled: Unlike Python, Rust compiles directly to machine code, resulting in faster execution.</li> <li>Efficient Memory Management: Working with embedding models requires careful memory usage when handling models and embeddings. Rust's efficient data structures make this possible.</li> <li>True Concurrency: Rust enables genuine multi-threading and asynchronous programming. As illustrated in the image below, we can simultaneously extract text, split content, generate embeddings, and push data to the database like in the Vector Streaming feature discussed below.</li> </ul> <p>The image shows the speed of embedding documents with EmbedAnything compared to other libraries. You can find the source for the benchmark here. </p> <p></p>"},{"location":"blog/2024/03/31/embed-anything/#what-does-candle-bring-to-the-table","title":"What does Candle bring to the table?","text":"<p>Running language models or embedding models locally can be difficult, especially when you want to deploy a product that utilizes these models. If you use the transformers library from Hugging Face in Python, you will depend on PyTorch for tensor operations. This, in turn, depends on Libtorch, meaning you must include the entire Libtorch library with your product. Also, Candle allows inferences on CUDA-enabled GPUs right out of the box.</p>"},{"location":"blog/2024/03/31/embed-anything/#multiple-modalities","title":"Multiple Modalities","text":"<p>EmbedAnything supports different modalities. You can embed text documents like HTML Pages, PDFs, and Markdowns using text embedding models like Jina, AllMiniLM, and others. You can also embed images using CLIP. </p> <p>Audio files can also be embedded using Whisper. The best part about EmbedAnything for audio embedding is that you can connect a text embedding model with Whisper. Using this, you can embed the text that Whisper decodes in parallel. The metadata includes the time stamps of the texts. You can see this in action in this Huggingface Space. </p> <p>Moreover, with EmbedAnything, you can use late-interaction models like ColPali, which remove the need to do OCR or chunking by embedding the PDF pages as a whole and retrieving the relevant PDF and pages against a query. This has enormous potential. For example, you can reduce the number of tokens that a Vision Language Model uses for document reading by retrieving only the valid pages and then showing them to large VLMs like GPT-4o and Gemini Flash. This can save a lot of cost and time. </p>"},{"location":"blog/2024/03/31/embed-anything/#vector-streaming","title":"Vector Streaming","text":"<p>Vector streaming allows you to create an asynchronous chunking and embedding task. We can effectively spawn threads to handle this task using Rust's concurrency patterns and thread safety. This is done using Rust's MPSC (Multi-producer Single Consumer) module, which passes messages between threads. Thus, this creates a stream of chunks passed into the embedding thread with a buffer. Once the buffer is complete, it embeds the chunks and sends the embeddings back to the main thread, where they are sent to the vector database. This ensures time is well spent on a single operation and no bottlenecks. Moreover, only the chunks and embeddings in the buffer are stored in the system memory. They are erased from the memory once moved to the vector database.</p> <p>Thus, it effectively solves the problem by making many tasks done asynchronously and thus improving efficiency. </p> <p></p>"},{"location":"blog/2024/03/31/embed-anything/#real-world-use-cases","title":"Real-world Use Cases","text":"<ul> <li>Vector Streaming: This involves streaming live information from videos by breaking them into frames and generating embeddings of the images using multimodal embedding models like CLIP. This technique is particularly useful for live camera feeds, such as CCTV, to detect malicious activities or traffic violations.</li> <li>Search Applications: We generate embeddings from PDFs and enable direct searches for page numbers based on user queries, utilizing ColPali. All of this is integrated into our pipeline.</li> <li>Classification Problems: Embeddings can be employed for classification tasks and can scale through metric learning, allowing for the easy addition of new classes.</li> <li>RAG Applications: We view EmbedAnything as an ingestion pipeline for vector databases, which can be extensively utilized for Retrieval-Augmented Generation (RAG) in chatbots.</li> </ul>"},{"location":"blog/2024/03/31/embed-anything/#how-to-get-started","title":"How to get started?","text":"<p>To install our library, all you have to do is </p> <pre><code>pip install embed-anythin\n</code></pre> <p>To improve the speed further and to use large models like ColPali, use the GPU version of embed anything by running. </p> <pre><code>pip install embed-anything-gpu\n</code></pre> <p>Then, with just a few lines of code, you can embed any files and directories. </p> <p><pre><code>model = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Bert, model_id=\"model link from huggingface\"\n)\nconfig = TextEmbedConfig(chunk_size=200, batch_size=32)\ndata = embed_anything.embed_file(\"file_address\", embedder=model, config=config)\n</code></pre> You can check out the documentation at  https://starlight-search.com/references/ </p>"},{"location":"blog/2024/03/31/embed-anything/#whats-next","title":"What\u2019s Next","text":"<p>Future work includes expanding our modalities and vector streaming adapters. We currently support unstructured sources like images, audio, and texts from different sources like PDFs, markdown, and jpegs, but we would also like to expand to graph embeddings and video embeddings. We currently support Weaviate and Elastic Cloud to stream the vectors, which we will expand to other vector databases. </p> <p>Moreover, we will also release methods to fine-tune embedding models easily using Candle, similar to how Sentence Transformers does, but with the speed and memory efficiency of Rust. </p> <p>With this, I would like to conclude this article on this amazing new library that I am building, and I hope to receive some great feedback from the readers. Try it out now and check out the GitHub Repo. </p>"},{"location":"blog/2025/05/01/mcp%2C%20agent/","title":"Easy MCP integration to our agentic framework; LUMO","text":""},{"location":"blog/2025/05/01/mcp%2C%20agent/#building-a-server-with-lumo-a-step-by-step-guide-to-mcp-integration","title":"Building a Server with Lumo: A Step-by-Step Guide to MCP Integration","text":"<p>Lumo, a powerful Rust-based agent, offers seamless integration with MCPs (Modular Control Protocols) and remarkable flexibility in implementation. While Lumo can be used as a library, CLI tool, or server, this guide will focus specifically on deploying Lumo in server mode for optimal MCP integration.</p>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#what-is-an-mcp","title":"What is an MCP?","text":"<p>Modular Control Protocol (MCP) is a standardized communication framework that allows different components of a system to interact efficiently. MCPs enable modular applications to communicate through a structured protocol, making it easier to build scalable, maintainable systems where components can be swapped or upgraded without disrupting the entire architecture.</p>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#architecture-of-mcp","title":"Architecture of MCP","text":"<p>MCP follows a client-server architecture with clearly defined roles:</p> <ul> <li>Hosts: LLM applications (like Claude Desktop or integrated development environments) that initiate connections</li> <li>Clients: Components that maintain one-to-one connections with servers inside the host application</li> <li>Servers: Systems that provide context, tools, and prompts to clients</li> </ul> <p>This architecture is built around three main concepts:</p> <ol> <li>Resources: Similar to GET endpoints, resources load information into the LLM's context</li> <li>Tools: Functioning like POST endpoints, tools execute code or produce side effects</li> <li>Prompts: Reusable templates that define interaction patterns for LLM communications</li> </ol> <p></p>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#setting-up-a-lumo-server-with-mcp-integration","title":"Setting Up a Lumo Server with MCP Integration","text":""},{"location":"blog/2025/05/01/mcp%2C%20agent/#server-usage","title":"\ud83d\udda5\ufe0f Server Usage","text":"<p>Lumo can also be run as a server, providing a REST API for agent interactions.</p>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#starting-the-server","title":"Starting the Server","text":"<pre><code>cargo install --git https://github.com/StarlightSearch/lumo.git --branch new-updates --features mcp lumo-server\n</code></pre>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#using-binary","title":"Using Binary","text":"<pre><code># Start the server (default port: 8080)\nlumo-server\n</code></pre>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#using-docker","title":"Using Docker","text":"<pre><code># Build the image\ndocker build -f server.Dockerfile -t lumo-server .\n\n# Run the container with required API keys\ndocker run -p 8080:8080 \\\n  -e OPENAI_API_KEY=your-openai-key \\\n  -e GOOGLE_API_KEY=your-google-key \\\n  -e GROQ_API_KEY=your-groq-key \\\n  -e ANTHROPIC_API_KEY=your-anthropic-key \\\n  -e EXA_API_KEY=your-exa-key \\\n  lumo-server\n</code></pre> <p>You can also use the pre-built image: <pre><code>docker pull akshayballal95/lumo-server:latest\n</code></pre></p>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#server-configuration","title":"Server Configuration","text":"<p>You can configure multiple servers in the configuration file for MCP agent usage. The configuration file location varies by operating system:</p> <pre><code>Linux: ~/.config/lumo-cli/servers.yaml\nmacOS: ~/Library/Application Support/lumo-cli/servers.yaml\nWindows: %APPDATA%\\Roaming\\lumo\\lumo-cli\\servers.yaml```\n\nExample config: \n\n```exa-search:\n  command: npx\n  args:\n    - \"exa-mcp-server\"\n  env: \n    EXA_API_KEY: \"your-api-key\"\n\nfetch:\n  command: uvx\n  args:\n    - \"mcp_server_fetch\"\n\nsystem_prompt: |-\n  You are a powerful agentic AI assistant...\n</code></pre>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#api-endpoints","title":"API Endpoints","text":""},{"location":"blog/2025/05/01/mcp%2C%20agent/#health-check","title":"Health Check","text":"<pre><code>curl http://localhost:8080/health_check\n</code></pre>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#run-task","title":"Run Task","text":"<pre><code>curl -X POST http://localhost:8080/run \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"task\": \"What are the files in the folder?\",\n    \"model\": \"gpt-4o-mini\",\n    \"base_url\": \"https://api.openai.com/v1/chat/completions\",\n    \"tools\": [\"DuckDuckGo\", \"VisitWebsite\"],\n    \"max_steps\": 5,\n    \"agent_type\": \"mcp\"\n  }'\n</code></pre>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#request-body-parameters","title":"Request Body Parameters","text":"<ul> <li><code>task</code> (required): The task to execute</li> <li><code>model</code> (required): Model ID (e.g., \"gpt-4\", \"qwen2.5\", \"gemini-2.0-flash\")</li> <li><code>base_url</code> (required): Base URL for the API</li> <li><code>tools</code> (optional): Array of tool names to use</li> <li><code>max_steps</code> (optional): Maximum number of steps to take</li> <li><code>agent_type</code> (optional): Type of agent to use (\"function-calling\" or \"mcp\")</li> <li><code>history</code> (optional): Array of previous messages for context</li> </ul>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#mcp-vs-traditional-function-calling","title":"MCP vs. Traditional Function-Calling","text":"<p>While both MCP and traditional function-calling allow LLMs to interact with external tools, they differ in several important ways:</p> <p>The only difference between them is that, in traditional function calling, you need to define the processes and then LLM chooses which is the right option for the given job. It\u2019s main purpose is to translate natural language into JSON format function calls. Meanwhile, MCP is the protocol that standardized the resources and tool calls for the LLM, that is why even though LLM still makes the decision to choose which MCP, it\u2019s the standard calls that makes it highly scalable</p>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#benefits-of-lumo-over-other-agentic-systems","title":"Benefits of lumo over other agentic systems","text":"<ol> <li>MCP Agent Support for multi-agent coordination</li> <li>Multi-modal support, can easily use OpenAI, Google or Anthropic</li> <li>Asynchronous tool calling.</li> <li>In-built Observability with langfuse</li> </ol> <p>Open discussions if you have any doubt and give us a star at repo.</p>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#conclusion","title":"Conclusion","text":"<p>As agents evolve, standardized protocols like MCP will become increasingly important for enabling sophisticated AI applications. By providing a common language for AI systems to interact with external tools and data sources, MCP helps bridge the gap between powerful language models and the specific capabilities needed for real-world applications.</p> <p>For developers working with AI, understanding and adopting MCP offers a more sustainable, future-proof approach to building AI integrations compared to platform-specific function-calling implementations.</p>"},{"location":"blog/2025/11/01/memory_leak/","title":"Memory Leak Explained!","text":"<p>Vector Streaming is popular for its low latency and high modularity features. But the best of it all is the memory leak that we are trying to bring it but haven\u2019t been able to click with the audience yet. Why we named it vector streaming and how it plays a major role in memory management.</p>"},{"location":"blog/2025/11/01/memory_leak/#why-memory-management-matters-when-working-with-embeddings","title":"Why Memory Management Matters When Working with Embeddings","text":"<p>When you run inference on an embedding model, you're converting text chunks into high-dimensional vector representations. In typical implementations, these embeddings accumulate in RAM during processing, which creates a significant risk of memory leaks\u2014especially when dealing with large document collections or long-running processes.</p>"},{"location":"blog/2025/11/01/memory_leak/#the-problem-with-traditional-approaches","title":"The Problem with Traditional Approaches","text":"<p>Consider a standard embedding pipeline: you load documents, chunk them, generate embeddings, and then... what happens to those embeddings while you're processing the next batch? They sit in memory. And if your indexing process is slow or if there's an interruption, those embeddings can pile up, consuming RAM until your application crashes or becomes unresponsive.</p> <p></p>"},{"location":"blog/2025/11/01/memory_leak/#what-is-vector-streaming","title":"What is vector streaming?","text":"<p>Embedding models are computationally expensive and time-consuming. By separating document preprocessing from model inference, you can significantly reduce pipeline latency and improve throughput.</p> <p>Vector streaming transforms a sequential bottleneck into an efficient, concurrent workflow.</p> <p>The embedding process occurs separately from the main process, allowing for high performance enabled by Rust's MPSC and preventing memory leaks, as embeddings are directly saved to a vector database. Find our\u00a0blog.</p>"},{"location":"blog/2025/11/01/memory_leak/#the-solution-vector-streaming","title":"The Solution: Vector Streaming","text":"<p>The most effective way to prevent these memory issues is to stream embeddings directly to your vector database rather than buffering them in RAM. This approach ensures that once an embedding is generated, it's immediately persisted and can be freed from memory.</p> <p>This is exactly what EmbedAnything accomplishes through its vector streaming architecture. By separating document preprocessing, model inference, and indexing onto different threads using Rust's MPSC (multi-producer, single-consumer) channels, embeddings flow directly from the model to your vector database without accumulating in memory.</p>"},{"location":"blog/2025/11/01/memory_leak/#benefits-of-this-approach","title":"Benefits of This Approach","text":"<ul> <li>No memory leaks: Embeddings are persisted immediately rather than queued in RAM</li> <li>Reduced latency: Concurrent processing of file parsing, embedding generation, and indexing</li> <li>Better throughput: Transform a sequential bottleneck into an efficient, parallel workflow</li> <li>Production-ready: Rust's memory safety guarantees prevent the crashes and leaks common in other languages</li> </ul> <p>Vector streaming transforms what would typically be a memory-intensive operation into a lightweight, efficient pipeline\u2014perfect for production environments where reliability and performance are critical.</p> <p>HOW vector streaming:</p> <p>The best way to create an inference pipeline with no memory leak is to save the embeddings generated directly on the vector database, rather than in RAM.</p> <p>How do we do it? also extremely unique.</p> <p>We set a buffer size that you can customize according to your hardware. And send each chunk, (to know how to set chunksize and buffer size because your retrieved results are highly based on these two factors, refer to this blog.)</p> <p>Inference that is the conversation of chunks to embeddings and storage to vector databases and all other process happens on different threads, that gives us a unique way to be able to store the embeddings not on RAM but on to the vector database itself.</p> <p></p> <p>Results of Vector Streaming have been tested on standard text data, with our Elastic Adapter available. We also support Weaviate, Milvus, Qdrant, Lance and many more and it\u2019s super easy to add your favourite vector database, just read our blog.</p> <p>We also have </p> <p></p>"},{"location":"blog/2025/11/01/memory_leak/#how-to-write-it-an-example","title":"How to write it, an example.","text":"<p>There are three parts of writing vector streaming with an already existing adapter.</p> <pre><code># Initialize the embedding model\n\nmodel = EmbeddingModel.from_pretrained_hf(\n\nWhichModel.Bert,\n\nmodel_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n\n)\n\n# Embed a PDF file\n\ndata = embed_anything.embed_file(\n\n\"test-file\",\n\nembedder=model,\n\nadapter=pinecone_adapter,\n\n)\n\n# Embed all images in a directory\n\ndata = embed_anything.embed_image_directory(\n\n\"test_files\",\n\nembedder=model,\n\nadapter=pinecone_adapter\n\n)\n</code></pre> <p>We also released an Actix Server that can be used directly, more on that later. I hope you found this blog useful in understanding how memory leaks work and how our design helps in overcoming this issue.</p>"},{"location":"blog/2025/04/01/embed-anything/","title":"PyCon Germany","text":"<p>The 2025 PyCon DE event highlighted a growing but cautious interest in AI agents among the Python community. While agent technology received significant attention, many speakers and attendees expressed skepticism about their practical utility in real-world applications.</p> <p>A notable trend was the gap between theoretical potential and actual implementation challenges. Blue Yonder teams, who work in supply chain management, shared how they overcome challenges with deploying agent-based solutions in production environments. Leonardo from Hugging Face presented a compelling mathematical limitation: even with models achieving 90% accuracy, a 20-step agent workflow would result in only about 12% end-to-end accuracy (due to compounding errors at each step). This mathematical reality presents a significant obstacle for complex agent workflows.</p> <p>Alexander Hendorf raised essential questions about the actual use cases where agents provide meaningful value in practical applications, suggesting that the technology might still be searching for its most effective applications.</p> <p>The conference reflected a Python community that remains interested in agent technology but is increasingly focused on practical implementation challenges rather than theoretical possibilities.</p> <p></p>"},{"location":"blog/2025/04/01/embed-anything/#community-culture","title":"Community &amp; Culture","text":"<ul> <li>Valerio Maggio moderated lightning talks described as \"playful chaos\" covering diverse topics.</li> <li>A Fireside Chat emphasized that technology should solve real problems, not exist for its own sake, and contrasted American startup culture with Germany's focus on sustainable development.</li> <li>Feminist AI Lan party, I had a boxing workshop, along with Ines and others. It was fun and frankly much needed after all the serious talks.</li> </ul> <p> The conference reflected a Python community balancing excitement about new technology</p> <p>PyConDE 2025: Building the Future of AI Agents - A Report from the Trenches</p>"},{"location":"blog/2025/04/01/embed-anything/#beyond-agents-what-ai-strategy-really-needs-in-2025","title":"Beyond Agents: What AI Strategy Really Needs in 2025","text":"<p>I was at a talk by Alexander C.S. Hendorf \ud83d\udc4b where he shared his insights on Nvidia GTC and shed light on a bunch of amazing topics , robotics , simulation, and open source. I appreciate some of the points he made regarding the commercialization of open-source software. And a deep question: \"Are agents actually helpful?\"</p>"},{"location":"blog/2025/04/01/embed-anything/#beyond-brute-force-smarter-data-ingestion-for-rag-systems","title":"Beyond Brute Force: Smarter Data Ingestion for RAG Systems","text":"<p>The first talk that really grabbed my attention was \"PDFs - When a thousand words are worth more than a picture (or table).\" The presenter highlighted a critical bottleneck in RAG (Retrieval Augmented Generation) systems: the inherent limitations of PDF parsing. We often take for granted the visual fidelity of PDFs, but their structure presents a real challenge for computers trying to extract meaningful information. Tables and figures, in particular, become nightmares for traditional parsers, leading to unreliable knowledge being fed into vector databases and ultimately, unreliable outputs from our RAG systems.</p> <p>The solution? Embracing multimodal models. The speaker advocated for decomposing tables and figures into plain language descriptions, mimicking how a human would explain them. This approach, focusing on semantics rather than visual representation, promises to significantly improve retrieval accuracy and pave the way for more robust RAG-based agents. The key takeaway here is that building effective agents requires intelligent data ingestion strategies that go beyond simple text extraction.</p>"},{"location":"blog/2025/04/01/embed-anything/#federated-learning-training-agents-in-a-privacy-conscious-world","title":"Federated Learning: Training Agents in a Privacy-Conscious World","text":"<p>Another compelling session, \"The future of AI training is federated,\" addressed the growing need for privacy-preserving AI development. With increasing data privacy regulations (like GDPR) and logistical challenges in centralizing data, federated learning is emerging as a crucial paradigm. This talk provided a practical introduction to building FL systems using the open-source Flower framework.</p> <p>The core concept is simple: instead of bringing the data to the model, we bring the model to the data. This allows us to train AI models on decentralized datasets without compromising sensitive information. The presenter walked us through converting a centralized ML workflow into a federated one, highlighting the specific steps involved in configuring clients, persisting state, and evaluating models. The biggest insight? Federated learning is no longer a niche research area; it's a practical solution for building AI agents that respect data privacy and comply with evolving regulations.</p>"},{"location":"blog/2025/04/01/embed-anything/#mcp-bridging-the-gap-between-agents-and-complex-systems","title":"MCP: Bridging the Gap Between Agents and Complex Systems","text":"<p>\"From Idea to Integration: An Intro to the Model Context Protocol (MCP)\" introduced a powerful standard for connecting Large Language Models with diverse data sources. The Model Context Protocol (MCP) acts as a bridge, enabling agents to interact with complex systems and access real-time data. The talk showcased how to build an MCP server and demonstrated its potential for empowering both developers and non-technical users. Imagine an agent that can access and interpret data from your smart home, your CRM, or any other complex system \u2013 that's the power of MCP.</p> <p>The key takeaway here is that building intelligent agents requires seamless integration with the real world. MCP provides a standardized way to achieve this, opening up a world of possibilities for contextual AI applications.</p>"},{"location":"blog/2025/04/01/embed-anything/#beyond-the-hype-strategic-ai-in-2025-and-beyond","title":"Beyond the Hype: Strategic AI in 2025 and Beyond","text":"<p>The talk \"Beyond Agents: What AI Strategy Really Needs in 2025\" offered a high-level perspective on the evolving AI landscape. Drawing insights from NVIDIA's GTC 2025, the speaker emphasized the convergence of AI with simulation, synthetic data, and robotics. He urged technical leaders to think beyond individual tools and embrace a more holistic approach to AI development.</p> <p>The session highlighted the importance of interdisciplinary collaboration and the rise of powerful, local AI systems. The message was clear: building the future of AI requires strategic thinking, a focus on convergence, and a willingness to collaborate across domains.</p>"},{"location":"blog/2025/04/01/embed-anything/#the-open-source-imperative-building-trust-and-control-in-ai","title":"The Open Source Imperative: Building Trust and Control in AI","text":"<p>Finally, \"The Future of AI: Building the Most Impactful Technology Together\" emphasized the crucial role of open-source principles in AI development. The presenter argued that openness in language models is essential for building trust, mitigating biases, and achieving true alignment. He highlighted the growing momentum of open models and called on the community to build the next generation of AI tools collaboratively.</p> <p>The core message resonated deeply: the future of AI is open. By embracing open-source principles, we can foster innovation, ensure transparency, and build AI systems that are truly beneficial to society.</p>"},{"location":"blog/2025/04/01/embed-anything/#conclusion-a-call-to-action","title":"Conclusion: A Call to Action","text":"<p>PyConDE 2025 left me feeling inspired and energized. The talks I attended highlighted the diverse challenges and exciting opportunities in the field of AI agents. From improving data ingestion to embracing federated learning and championing open-source models, the path forward is clear: building the future of AI requires a collaborative, strategic, and ethical approach.</p>"},{"location":"blog/2025/05/01/observability/","title":"Easy Observability to our agentic framework; LUMO","text":"<p>In the rapidly evolving landscape of AI agents, particularly those employing Large Language Models (LLMs), observability and tracing have emerged as fundamental requirements rather than optional features. As agents become more complex and handle increasingly critical tasks, understanding their inner workings, debugging issues, and establishing accountability becomes paramount.</p>"},{"location":"blog/2025/05/01/observability/#understanding-observability-in-ai-agents","title":"Understanding Observability in AI Agents","text":"<p>Observability refers to the ability to understand the internal state of a system through its external outputs. In AI agents, comprehensive observability encompasses:</p> <ol> <li>Decision Visibility: Transparency into how and why an agent made specific decisions</li> <li>State Tracking: Monitoring the agent's internal state as it evolves throughout task execution</li> <li>Resource Utilization: Measuring computational resources, API calls, and external interactions</li> <li>Performance Metrics: Capturing response times, completion rates, and quality indicators</li> </ol>"},{"location":"blog/2025/05/01/observability/#the-multi-faceted-value-of-tracing-and-observability","title":"The Multi-Faceted Value of Tracing and Observability","text":""},{"location":"blog/2025/05/01/observability/#1-debugging-and-troubleshooting","title":"1. Debugging and Troubleshooting","text":"<p>AI agents, especially those leveraging LLMs, operate with inherent complexity and sometimes unpredictability. Without proper observability:</p> <ul> <li>Silent Failures become common, where agents fail without clear indications of what went wrong</li> <li>Root Cause Analysis becomes nearly impossible as there's no trace of the execution path</li> </ul>"},{"location":"blog/2025/05/01/observability/#2-performance-optimization","title":"2. Performance Optimization","text":"<p>Observability provides crucial insights for optimizing agent performance:</p> <ul> <li>Caching Opportunities: Recognize repeated patterns that could benefit from caching</li> </ul>"},{"location":"blog/2025/05/01/observability/#3-security-and-compliance","title":"3. Security and Compliance","text":"<p>As agents gain more capabilities and autonomy, security becomes increasingly critical:</p> <ul> <li>Audit Trails: Maintain comprehensive logs of all agent actions for compliance and security reviews</li> <li>Prompt Injection Detection: Identify potential attempts to manipulate the agent's behavior</li> </ul>"},{"location":"blog/2025/05/01/observability/#4-user-trust-and-transparency","title":"4. User Trust and Transparency","text":"<p>For end-users working with AI agents, transparency builds trust:</p> <ul> <li>Action Justification: Provide clear explanations for why the agent took specific actions</li> <li>Confidence Indicators: Show reliability metrics for different types of responses</li> </ul>"},{"location":"blog/2025/05/01/observability/#5-continuous-improvement","title":"5. Continuous Improvement","text":"<p>Observability creates a foundation for systematic improvement:</p> <ul> <li>Pattern Recognition: Identify standard failure modes or suboptimal behaviors</li> <li>A/B Testing: Compare different agent configurations with detailed performance metrics</li> </ul>"},{"location":"blog/2025/05/01/observability/#implementing-effective-observability-in-lumo","title":"Implementing Effective Observability in Lumo","text":"<p>For Tracing and Observability</p> <p><pre><code>vim ~/.bashrc\n</code></pre> Add the three keys from Langfuse:</p> <pre><code>LANGFUSE_PUBLIC_KEY_DEV=your-dev-public-key\nLANGFUSE_SECRET_KEY_DEV=your-dev-secret-key\nLANGFUSE_HOST_DEV=http://localhost:3000  # Or your dev Langfuse instance URL\n</code></pre> <p>Start lumo-cli or lumo server then press:</p> <p><pre><code>CTRL + C\n</code></pre> And it\u2019s added to the dashboard</p> <p></p>"},{"location":"blog/2025/05/01/observability/#conclusion","title":"Conclusion","text":"<p>Observability and tracing are no longer optional components for serious AI agent implementations. They form the foundation for reliable, secure, and continuously improving systems. As agents take on more responsibility and autonomy, the ability to observe, understand, and explain their behavior becomes not just a technical requirement but an ethical imperative.</p> <p>Organizations building or deploying AI agents should invest early in robust observability infrastructure, treating it as a core capability rather than an afterthought. The insights gained will improve current systems and also inform the development of better, more trustworthy agents in the future.</p>"},{"location":"blog/2025/01/01/modernBERT/","title":"version 0.5","text":"<p>We are thrilled to share that EmbedAnything version 0.5 is out now and comprise of insane development like support for ModernBert and ReRanker models. Along with Ingestion pipeline support for DocX, and HTML let\u2019s get in details.</p> <p>The best of all have been support for late-interaction model, both ColPali and ColBERT on onnx.</p> <ol> <li>ModernBert Support: Well it made quite a splash, and we were obliged to add it, in the fastest inference engine, embedanything. In addition to being faster and more accurate, ModernBERT also increases context length to 8k tokens (compared to just 512 for most encoders), and is the first encoder-only model that includes a large amount of code in its training data.</li> <li>ColPali- Onnx : \u00a0Running the ColPali model directly on a local machine might not always be feasible. To address this, we developed a\u00a0quantized version of ColPali. Find it on our hugging face, link here. You could also run it both on Candle and on ONNX.</li> <li>ColBERT: ColBERT is a\u00a0fast\u00a0and\u00a0accurate\u00a0retrieval model, enabling scalable BERT-based search over large text collections in tens of milliseconds.</li> <li>ReRankers: EmbedAnything recently contributed for the support of reranking models to Candle so as to add it in our own library. It can support any kind of reranking models. Precision meets performance! Use reranking models to refine your retrieval results for even greater accuracy.</li> <li>Jina V3: Also contributed to V3 models, for Jina can seamlessly integrate any V3 model.</li> <li> <p>\ud835\uddd7\ud835\udde2\ud835\uddd6\ud835\uddeb \ud835\udde3\ud835\uddff\ud835\uddfc\ud835\uddf0\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\uddf6\ud835\uddfb\ud835\uddf4</p> <p>Effortlessly extract text from .docx files and convert it into embeddings. Simplify your document workflows like never before!</p> </li> <li> <p>\ud835\udddb\ud835\udde7\ud835\udde0\ud835\udddf \ud835\udde3\ud835\uddff\ud835\uddfc\ud835\uddf0\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\uddf6\ud835\uddfb\ud835\uddf4:</p> </li> </ol> <p>Parsing and embedding HTML documents just got easier!</p> <pre><code>\u2705 Extract rich metadata with embeddings\n\u2705 Handle code blocks separately for better context\n</code></pre> <p>Supercharge your documentation retrieval with these advanced capabilities.</p>"},{"location":"blog/2024/03/31/vector-streaming/","title":"Vector Streaming","text":"<p>Introducing vector streaming in EmbedAnything, a feature designed to optimize large-scale document embedding. By enabling asynchronous chunking and embedding using Rust\u2019s concurrency, it reduces memory usage and speeds up the process. We also show how to integrate it with the Weaviate Vector Database for seamless image embedding and search.</p> <p>In my previous article Supercharge Your Embeddings Pipeline with EmbedAnything, I discussed the idea behind EmbedAnything and how it makes creating embeddings from multiple modalities easy. In this article, I want to introduce a new feature of EmbedAnything called vector streaming and see how it works with Weaviate Vector Database. </p>"},{"location":"blog/2024/03/31/vector-streaming/#what-is-the-problem","title":"What is the problem?","text":"<p>First, let's examine the current problem with creating embeddings, especially in large-scale documents. The current embedding frameworks operate on a two-step process: chunking and embedding. First, the text is extracted from all the files, and chunks/nodes are created. Then, these chunks are fed to an embedding model with a specific batch size to process the embeddings. While this is done, the chunks and the embeddings stay on the system memory. This is not a problem when the files are small, and the embedding dimensions are small. But this becomes a problem when there are many files and you are working with large models and, even worse, multi-vector embeddings. Thus, to work with this, a high RAM is required to process the embeddings. Also, if this is done synchronously, a lot of time is wasted while the chunks are being created, as chunking is not a compute-heavy operation. As the chunks are being made, passing them to the embedding model would be efficient. </p>"},{"location":"blog/2024/03/31/vector-streaming/#our-solution","title":"Our Solution","text":"<p>The solution is to create an asynchronous chunking and embedding task. We can effectively spawn threads to handle this task using Rust's concurrency patterns and thread safety. This is done using Rust's MPSC (Multi-producer Single Consumer) module, which passes messages between threads. Thus, this creates a stream of chunks passed into the embedding thread with a buffer. Once the buffer is complete, it embeds the chunks and sends the embeddings back to the main thread, where they are sent to the vector database. This ensures no time is wasted on a single operation and no bottlenecks. Moreover, only the chunks and embeddings in the buffer are stored in the system memory. They are erased from the memory once moved to the vector database. </p> <p></p>"},{"location":"blog/2024/03/31/vector-streaming/#example-use-case","title":"Example Use Case","text":"<p>Now, let's see this feature in action. </p> <p>With EmbedAnything, streaming the vectors from a directory of files to the vector database is a simple three-step process. </p> <ol> <li> <p>Create an adapter for your vector database: This is a wrapper around the database's functions that allows you to create an index, convert metadata from EmbedAnything's format to the format required by the database, and the function to insert the embeddings in the index. Adapters for the prominent databases are already created and present here: </p> </li> <li> <p>Initiate an embedding model of your choice: You can choose from different local models or even cloud models. The configuration can also be determined to set the chunk size and buffer size for how many embeddings need to be streamed at once. Ideally, this should be as high as possible, but the system RAM limits this. </p> </li> <li> <p>Call the embedding function from EmbedAnything: Just pass the directory path to be embedded, the embedding model, the adapter, and the configuration. </p> </li> </ol> <p>In this example, we will embed a directory of images and send it to the vector databases. </p>"},{"location":"blog/2024/03/31/vector-streaming/#step-1-create-the-adapter","title":"Step 1: Create the Adapter","text":"<p>In EmbedAnything, the adapters are created outside so as to not make the library heavy and you get to choose which database you want to work with. Here is a simple adapter for Weaviate.</p> <pre><code>from embed_anything import EmbedData\nfrom embed_anything.vectordb import Adapter\n\nclass WeaviateAdapter(Adapter):\n    def __init__(self, api_key, url):\n        super().__init__(api_key)\n        self.client = weaviate.connect_to_weaviate_cloud(\n            cluster_url=url, auth_credentials=wvc.init.Auth.api_key(api_key)\n        )\n        if self.client.is_ready():\n            print(\"Weaviate is ready\")\n\n    def create_index(self, index_name: str):\n        self.index_name = index_name\n        self.collection = self.client.collections.create(\n            index_name, vectorizer_config=wvc.config.Configure.Vectorizer.none()\n        )\n        return self.collection\n\n    def convert(self, embeddings: List[EmbedData]):\n        data = []\n        for embedding in embeddings:\n            property = embedding.metadata\n            property[\"text\"] = embedding.text\n            data.append(\n                wvc.data.DataObject(properties=property, vector=embedding.embedding)\n            )\n        return data\n\n    def upsert(self, embeddings):\n        data = self.convert(embeddings)\n        self.client.collections.get(self.index_name).data.insert_many(data)\n\n    def delete_index(self, index_name: str):\n        self.client.collections.delete(index_name)\n\n### Start the client and index\n\nURL = \"your-weaviate-url\"\nAPI_KEY = \"your-weaviate-api-key\"\nweaviate_adapter = WeaviateAdapter(API_KEY, URL)\n\nindex_name = \"Test_index\"\nif index_name in weaviate_adapter.client.collections.list_all():\n    weaviate_adapter.delete_index(index_name)\nweaviate_adapter.create_index(\"Test_index\")\n</code></pre>"},{"location":"blog/2024/03/31/vector-streaming/#step-2-create-the-embedding-model","title":"Step 2: Create the Embedding Model","text":"<p>Here, since we are embedding images, we can use the clip model </p> <pre><code>import embed_anything import WhichModel\n\nmodel = embed_anything.EmbeddingModel.from_pretrained_hf(\n    embed_anything.WhichModel.Clip,\n    model_id=\"openai/clip-vit-base-patch16\",\n    # revision=\"refs/pr/15\",\n)\n</code></pre>"},{"location":"blog/2024/03/31/vector-streaming/#step-3-embed-the-directory","title":"Step 3: Embed the Directory","text":"<pre><code>data= embed_anything.embed_image_directory(\n    \"/content/EmbedAnything/test_files/clip\", embedder=model, adapter=weaviate_adapter\n)\n</code></pre>"},{"location":"blog/2024/03/31/vector-streaming/#step-4-query-the-vector-database","title":"Step 4: Query the Vector Database","text":"<pre><code>query_vector = embed_anything.embed_query([\"image of a cat\"], embedder=model)[0].embedding\n</code></pre>"},{"location":"blog/2024/03/31/vector-streaming/#step-5-query-the-vector-database","title":"Step 5: Query the Vector Database","text":"<pre><code>response = weaviate_adapter.collection.query.near_vector(\n    near_vector=query_vector,\n    limit=2,\n    return_metadata=wvc.query.MetadataQuery(certainty=True),\n)\n</code></pre> <p>Check the response;</p> <p></p> <p>Check out the notebook for the code here on colab</p> <p></p>"},{"location":"blog/2024/03/31/vector-streaming/#conclusion","title":"Conclusion","text":"<p>We think vector streaming is one of the features that will empower many engineers to opt for a more optimized and no-tech debt solution. Instead of using bulky frameworks on the cloud, you can use a lightweight streaming option. Please don't forget to give us a \u2b50 on our GitHub repo over here: EmbedAnything Repo</p>"},{"location":"guides/Qwen/","title":"Using Qwen3-Embedding","text":"<p>Qwen3-Embedding is a powerful multilingual embedding model designed for text embedding, retrieval, and reranking tasks. It supports over 100 languages, including various programming languages, making it ideal for international applications and code search.</p>"},{"location":"guides/Qwen/#key-features","title":"Key Features","text":"<ul> <li>Multilingual support: Over 100 languages including programming languages</li> <li>High quality: State-of-the-art performance on retrieval benchmarks</li> <li>Efficient: 0.6B parameter model balances quality and speed</li> <li>Versatile: Works for both embedding and reranking tasks</li> </ul>"},{"location":"guides/Qwen/#basic-usage","title":"Basic Usage","text":"<pre><code>from embed_anything import EmbeddingModel, WhichModel, TextEmbedConfig, Dtype\nimport numpy as np\n\n# Initialize Qwen3 embedding model\nmodel = EmbeddingModel.from_pretrained_hf(\n    model_id=\"Qwen/Qwen3-Embedding-0.6B\",\n    dtype=Dtype.F32  # Use float32 for best quality, F16 for faster inference\n)\n\n# Configure embedding parameters\nconfig = TextEmbedConfig(\n    chunk_size=1000,\n    batch_size=2,\n    splitting_strategy=\"sentence\"\n)\n\n# Embed a document\ndata = model.embed_file(\"test_files/document.pdf\", config=config)\n\n# Access embeddings\nfor item in data:\n    print(f\"Text: {item.text[:200]}...\")\n    print(f\"Embedding dimension: {len(item.embedding)}\")\n</code></pre>"},{"location":"guides/Qwen/#query-and-retrieval","title":"Query and Retrieval","text":"<pre><code>from embed_anything import EmbeddingModel, WhichModel, TextEmbedConfig, Dtype\nimport numpy as np\n\n# Load model\nmodel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Qwen3, \n    model_id=\"Qwen/Qwen3-Embedding-0.6B\",\n    dtype=Dtype.F32\n)\n\n# Embed documents\nconfig = TextEmbedConfig(chunk_size=1000, batch_size=2, splitting_strategy=\"sentence\")\ndata = model.embed_file(\"test_files/document.pdf\", config=config)\n\n# Embed a query\nquery = \"Which GPU is used for training\"\nquery_embedding = np.array(model.embed_query([query])[0].embedding)\n\n# Calculate similarities\nembedding_array = np.array([e.embedding for e in data])\nsimilarities = np.matmul(query_embedding, embedding_array.T)\n\n# Get top 5 most relevant chunks\ntop_5_indices = np.argsort(similarities)[-5:][::-1]\n\nprint(\"Top 5 most relevant results:\")\nfor i, idx in enumerate(top_5_indices, 1):\n    print(f\"\\n{i}. Score: {similarities[idx]:.4f}\")\n    print(f\"   Text: {data[idx].text[:200]}...\")\n</code></pre>"},{"location":"guides/Qwen/#multilingual-support","title":"Multilingual Support","text":"<p>Qwen3 excels at multilingual tasks:</p> <pre><code>from embed_anything import EmbeddingModel, WhichModel, Dtype\nimport numpy as np\n\n# Load model\nmodel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Qwen3,\n    model_id=\"Qwen/Qwen3-Embedding-0.6B\",\n    dtype=Dtype.F32\n)\n\n# Embed text in different languages\ntexts = [\n    \"Hello, how are you?\",           # English\n    \"Bonjour, comment allez-vous?\",  # French\n    \"Hola, \u00bfc\u00f3mo est\u00e1s?\",            # Spanish\n    \"\u4f60\u597d\uff0c\u4f60\u597d\u5417\uff1f\",                  # Chinese\n    \"\u3053\u3093\u306b\u3061\u306f\u3001\u5143\u6c17\u3067\u3059\u304b\uff1f\"          # Japanese\n]\n\nembeddings = model.embed_query(texts)\nembeddings_array = np.array([e.embedding for e in embeddings])\n\n# Calculate cross-lingual similarities\nsimilarities = np.matmul(embeddings_array, embeddings_array.T)\n\n# Find similar meanings across languages\nfor i, text1 in enumerate(texts):\n    for j, text2 in enumerate(texts[i+1:], i+1):\n        print(f\"Similarity: {similarities[i][j]:.4f}\")\n        print(f\"  {text1}\")\n        print(f\"  {text2}\\n\")\n</code></pre>"},{"location":"guides/Qwen/#code-search","title":"Code Search","text":"<p>Qwen3 is excellent for code search and retrieval:</p> <pre><code>from embed_anything import EmbeddingModel, WhichModel, TextEmbedConfig, Dtype\nimport numpy as np\n\n# Load model\nmodel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Qwen3,\n    model_id=\"Qwen/Qwen3-Embedding-0.6B\",\n    dtype=Dtype.F32\n)\n\n# Embed code files\nconfig = TextEmbedConfig(chunk_size=1000, batch_size=4)\ncode_embeddings = model.embed_file(\"src/main.py\", config=config)\n\n# Search for code functionality\nquery = \"function that processes user input\"\nquery_emb = np.array(model.embed_query([query])[0].embedding)\n\n# Find relevant code snippets\ncode_array = np.array([e.embedding for e in code_embeddings])\nsimilarities = np.matmul(query_emb, code_array.T)\n\ntop_results = np.argsort(similarities)[-3:][::-1]\nfor idx in top_results:\n    print(f\"Score: {similarities[idx]:.4f}\")\n    print(f\"Code: {code_embeddings[idx].text}\\n\")\n</code></pre>"},{"location":"guides/Qwen/#performance-optimization","title":"Performance Optimization","text":"<pre><code>from embed_anything import EmbeddingModel, WhichModel, Dtype\n\n# Use F16 for faster inference with minimal quality loss\nmodel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Qwen3,\n    model_id=\"Qwen/Qwen3-Embedding-0.6B\",\n    dtype=Dtype.F16  # Half precision for 2x speedup\n)\n\n# Increase batch size for better throughput\nconfig = TextEmbedConfig(\n    chunk_size=1000,\n    batch_size=8,  # Larger batch for better GPU utilization\n    splitting_strategy=\"sentence\"\n)\n</code></pre>"},{"location":"guides/Qwen/#complete-example","title":"Complete Example","text":"<pre><code>import heapq\nfrom embed_anything import EmbedData, EmbeddingModel, TextEmbedConfig, Dtype\n\nfrom embed_anything import Dtype, ONNXModel\nimport numpy as np\n\nmodel:EmbeddingModel = EmbeddingModel.from_pretrained_hf(\n    model_id=\"Qwen/Qwen3-Embedding-0.6B\", dtype=Dtype.F32\n)\n\nconfig = TextEmbedConfig(\n    chunk_size=1000,\n    batch_size=2,\n    splitting_strategy=\"sentence\",\n    late_chunking=False,\n)\n\n# Embed a single file\ndata: list[EmbedData] = model.embed_file(\"test_files/attention.pdf\", config=config)\n\n\nquery = \"Which GPU is used for training\"\n\nquery_embedding = np.array(model.embed_query([query])[0].embedding)\n\nembedding_array = np.array([e.embedding for e in data])\n\nsimilarities = np.matmul(query_embedding, embedding_array.T)\n\n# get top 5 similarities and its index\ntop_5_similarities = np.argsort(similarities)[-10:][::-1]\n\n# Print the top 5 similarities with sentences\nfor i in top_5_similarities:\n    print(f\"Score: {similarities[i]:.2} | {data[i].text}\")\n    print(\"---\" * 20)\n\ncontext = \"\\n\".join([data[i].text for i in top_5_similarities])\n</code></pre>"},{"location":"guides/Qwen/#when-to-use-qwen3","title":"When to Use Qwen3","text":"<ul> <li>Multilingual applications: When you need to support multiple languages</li> <li>Code search: Searching through codebases and documentation</li> <li>International RAG: Building RAG systems for global users</li> <li>Mixed content: Documents with text and code</li> <li>Quality over speed: When embedding quality is more important than speed</li> </ul>"},{"location":"guides/Qwen/#comparison-with-other-models","title":"Comparison with Other Models","text":"Model Languages Code Support Speed Quality Qwen3 100+ Excellent Medium High Jina English-focused Good Fast High BERT English Limited Fast Medium"},{"location":"guides/Qwen/#best-practices","title":"Best Practices","text":"<ol> <li>Use F32 for production: Better quality, acceptable speed</li> <li>Use F16 for development: Faster iteration, minimal quality loss</li> <li>Batch processing: Process multiple documents together</li> <li>Appropriate chunk size: 500-1500 characters work well</li> <li>Sentence splitting: Use sentence splitting for better chunk quality</li> </ol>"},{"location":"guides/Qwen3_Reranker/","title":"Qwen3 Reranker in EmbedAnything","text":"<p>The Qwen3 Reranker is a powerful document relevance scoring and ranking model that has been integrated into EmbedAnything. It provides high-quality relevance scores for documents based on user queries, making it ideal for search and retrieval applications.</p>"},{"location":"guides/Qwen3_Reranker/#overview","title":"Overview","text":"<p>The Qwen3 Reranker is based on the Qwen3 architecture and has been optimized with ONNX for efficient inference. It's specifically designed for:</p> <ul> <li>Document Relevance Scoring: Assigning relevance scores to documents based on queries</li> <li>Search Result Reranking: Improving search result quality by reranking candidates</li> <li>Information Retrieval: Enhancing retrieval systems with semantic understanding</li> <li>Question-Answering: Ranking candidate answers by relevance to questions</li> </ul>"},{"location":"guides/Qwen3_Reranker/#key-features","title":"Key Features","text":"<ul> <li>High Quality: State-of-the-art relevance scoring capabilities</li> <li>ONNX Optimized: Fast inference using ONNX Runtime</li> <li>Batch Processing: Efficient handling of multiple queries and documents</li> <li>Flexible Scoring: Both ranked results and raw scores available</li> <li>Easy Integration: Simple Python API for quick implementation</li> </ul>"},{"location":"guides/Qwen3_Reranker/#installation","title":"Installation","text":"<p>The Qwen3 Reranker is included with EmbedAnything. Install the package:</p> <pre><code>pip install embed-anything\n</code></pre> <p>Additional dependencies: <pre><code>pip install onnxruntime  # For ONNX inference\n</code></pre></p>"},{"location":"guides/Qwen3_Reranker/#quick-start","title":"Quick Start","text":""},{"location":"guides/Qwen3_Reranker/#basic-usage","title":"Basic Usage","text":"<pre><code>from embed_anything import Reranker, Dtype\n\n# Qwen3 Reranker requires additional formatting\ndef format_query(query: str, instruction=None):\n    \"\"\"You may add instruction to get better results in specific fields.\"\"\"\n    if instruction is None:\n        instruction = \"Given a web search query, retrieve relevant passages that answer the query\"\n    return f\"&lt;Instruct&gt;: {instruction}\\n&lt;Query&gt;: {query}\\n\"\n\ndef format_document(doc: str):\n    return f\"&lt;Document&gt;: {doc}\"\n\n# Initialize the Qwen3 reranker\nreranker = Reranker.from_pretrained(\n    \"zhiqing/Qwen3-Reranker-0.6B-ONNX\", \n    dtype=Dtype.F32\n)\n\n# Rerank documents for a query\nquery = [\"What is machine learning?\"]\ndocuments = [\n    \"Machine learning is a subset of AI.\",\n    \"The weather is sunny today.\",\n    \"ML algorithms can learn from data.\",\n    \"Pizza is a popular food.\"\n]\n\n# Format query and documents\nquery = [format_query(x) for x in query]\ndocuments = [format_document(x) for x in documents]\n\nresults = reranker.rerank(query, documents, top_k=2)\n\n# Display results\nfor result in results:\n    print(f\"Query: {result.query}\")\n    for doc in result.documents:\n        print(f\"  Rank {doc.rank}: {doc.document}\")\n        print(f\"    Score: {doc.relevance_score:.4f}\")\n</code></pre>"},{"location":"guides/Qwen3_Reranker/#multiple-queries","title":"Multiple Queries","text":"<pre><code># Rerank for multiple queries simultaneously\nqueries = [\n    \"How to make coffee?\",\n    \"What is Python programming?\"\n]\n\ndocuments = [\n    \"Coffee is brewed from beans.\",\n    \"Python is a programming language.\",\n    \"The weather is nice.\",\n    \"Coffee can be made with a French press.\",\n    \"Python is great for beginners.\"\n]\n\n# Format queries and documents\nqueries = [format_query(x) for x in queries]\ndocuments = [format_document(x) for x in documents]\n\nresults = reranker.rerank(queries, documents, top_k=3)\n\nfor result in results:\n    print(f\"Query: {result.query}\")\n    for doc in result.documents:\n        print(f\"  {doc.document} (Score: {doc.relevance_score:.4f})\")\n</code></pre>"},{"location":"guides/Qwen3_Reranker/#custom-scoring","title":"Custom Scoring","text":"<pre><code># Get raw scores for custom processing\nscores = reranker.compute_scores(query, documents, batch_size=4)\n\n# Custom ranking logic\ndoc_scores = list(zip(documents, scores[0]))\ndoc_scores.sort(key=lambda x: x[1], reverse=True)\n\nfor doc, score in doc_scores:\n    print(f\"Score: {score:.4f} | {doc}\")\n</code></pre>"},{"location":"guides/Qwen3_Reranker/#api-reference","title":"API Reference","text":""},{"location":"guides/Qwen3_Reranker/#reranker-class","title":"Reranker Class","text":""},{"location":"guides/Qwen3_Reranker/#rerankerfrom_pretrained","title":"<code>Reranker.from_pretrained()</code>","text":"<p>Loads a pre-trained reranker model.</p> <p>Parameters: - <code>model_id</code> (str): Hugging Face model ID (e.g., \"zhiqing/Qwen3-Reranker-0.6B-ONNX\") - <code>revision</code> (str, optional): Model revision/branch - <code>dtype</code> (Dtype, optional): Model data type (F32, F16, etc.) - <code>path_in_repo</code> (str, optional): Path to model files in repository</p> <p>Returns: - <code>Reranker</code>: Initialized reranker instance</p>"},{"location":"guides/Qwen3_Reranker/#rerank","title":"<code>rerank()</code>","text":"<p>Reranks documents for given queries.</p> <p>Parameters: - <code>query</code> (List[str]): List of query strings - <code>documents</code> (List[str]): List of document strings to rank - <code>top_k</code> (int): Number of top documents to return</p> <p>Returns: - <code>List[RerankerResult]</code>: List of reranking results</p>"},{"location":"guides/Qwen3_Reranker/#compute_scores","title":"<code>compute_scores()</code>","text":"<p>Computes raw relevance scores.</p> <p>Parameters: - <code>query</code> (List[str]): List of query strings - <code>documents</code> (List[str]): List of document strings - <code>batch_size</code> (int): Batch size for processing</p> <p>Returns: - <code>List[List[float]]</code>: Raw relevance scores for each query-document pair</p>"},{"location":"guides/Qwen3_Reranker/#data-structures","title":"Data Structures","text":""},{"location":"guides/Qwen3_Reranker/#rerankerresult","title":"<code>RerankerResult</code>","text":"<pre><code>class RerankerResult:\n    query: str                    # The query string\n    documents: List[DocumentRank] # Ranked documents\n</code></pre>"},{"location":"guides/Qwen3_Reranker/#documentrank","title":"<code>DocumentRank</code>","text":"<pre><code>class DocumentRank:\n    document: str        # Document text\n    relevance_score: float # Relevance score (0.0 to 1.0)\n    rank: int           # Rank position (1-based)\n</code></pre>"},{"location":"guides/Qwen3_Reranker/#model-variants","title":"Model Variants","text":""},{"location":"guides/Qwen3_Reranker/#available-models","title":"Available Models","text":"<ul> <li>zhiqing/Qwen3-Reranker-0.6B-ONNX: 0.6B parameter model, ONNX optimized</li> <li>zhiqing/Qwen3-Reranker-0.6B: Original PyTorch model</li> </ul>"},{"location":"guides/Qwen3_Reranker/#data-types","title":"Data Types","text":"<ul> <li>F32: Full precision (default, best quality)</li> <li>F16: Half precision (faster, slightly lower quality)</li> <li>INT8: 8-bit quantization (fastest, lower quality)</li> </ul>"},{"location":"guides/Qwen3_Reranker/#performance-considerations","title":"Performance Considerations","text":""},{"location":"guides/Qwen3_Reranker/#batch-processing","title":"Batch Processing","text":"<p>For optimal performance when processing multiple documents:</p> <pre><code># Use appropriate batch sizes\nscores = reranker.compute_scores(queries, documents, batch_size=8)\n</code></pre>"},{"location":"guides/Qwen3_Reranker/#memory-usage","title":"Memory Usage","text":"<ul> <li>F32: ~2.4GB memory usage</li> <li>F16: ~1.2GB memory usage</li> <li>INT8: ~600MB memory usage</li> </ul>"},{"location":"guides/Qwen3_Reranker/#speed-vs-quality-trade-offs","title":"Speed vs Quality Trade-offs","text":"<ul> <li>F32: Best quality, slower inference</li> <li>F16: Good quality, balanced performance</li> <li>INT8: Lower quality, fastest inference</li> </ul>"},{"location":"guides/Qwen3_Reranker/#use-cases","title":"Use Cases","text":""},{"location":"guides/Qwen3_Reranker/#1-search-engine-reranking","title":"1. Search Engine Reranking","text":"<pre><code># After vector search, rerank results\nvector_results = vector_search(query, top_k=100)\nreranked_results = reranker.rerank([query], vector_results, top_k=10)\n</code></pre>"},{"location":"guides/Qwen3_Reranker/#2-question-answering","title":"2. Question Answering","text":"<pre><code># Rank candidate answers\nquestion = \"What is the capital of France?\"\ncandidate_answers = [\n    \"Paris is the capital of France.\",\n    \"France is a country in Europe.\",\n    \"The Eiffel Tower is in Paris.\"\n]\n\nranked_answers = reranker.rerank([question], candidate_answers, top_k=1)\nbest_answer = ranked_answers[0].documents[0].document\n</code></pre>"},{"location":"guides/Qwen3_Reranker/#3-document-filtering","title":"3. Document Filtering","text":"<pre><code># Filter documents by relevance threshold\nscores = reranker.compute_scores([query], documents, batch_size=4)\nrelevant_docs = [\n    doc for doc, score in zip(documents, scores[0]) \n    if score &gt; 0.5  # Threshold\n]\n</code></pre>"},{"location":"guides/Qwen3_Reranker/#4-content-recommendation","title":"4. Content Recommendation","text":"<pre><code># Rank content by user query relevance\nuser_interest = \"machine learning\"\ncontent_items = [\n    \"Introduction to ML\",\n    \"Python programming basics\",\n    \"Advanced ML algorithms\",\n    \"Web development tutorial\"\n]\n\nrecommendations = reranker.rerank([user_interest], content_items, top_k=3)\n</code></pre>"},{"location":"guides/Qwen3_Reranker/#best-practices","title":"Best Practices","text":""},{"location":"guides/Qwen3_Reranker/#1-model-selection","title":"1. Model Selection","text":"<ul> <li>Use ONNX models for production (faster inference)</li> <li>Choose data type based on quality vs. speed requirements</li> <li>Consider model size for memory constraints</li> </ul>"},{"location":"guides/Qwen3_Reranker/#2-query-formulation","title":"2. Query Formulation","text":"<ul> <li>Keep queries clear and specific</li> <li>Use natural language (the model understands context)</li> <li>Avoid overly long or complex queries</li> </ul>"},{"location":"guides/Qwen3_Reranker/#3-document-preparation","title":"3. Document Preparation","text":"<ul> <li>Ensure documents are well-formatted</li> <li>Remove irrelevant content before reranking</li> <li>Consider document length (very long documents may affect performance)</li> </ul>"},{"location":"guides/Qwen3_Reranker/#4-performance-optimization","title":"4. Performance Optimization","text":"<ul> <li>Use batch processing for multiple queries</li> <li>Implement caching for repeated queries</li> <li>Consider async processing for web applications</li> </ul>"},{"location":"guides/Qwen3_Reranker/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/Qwen3_Reranker/#common-issues","title":"Common Issues","text":"<ol> <li>Model Loading Errors</li> <li>Ensure ONNX Runtime is installed</li> <li>Check internet connection for model download</li> <li> <p>Verify model ID is correct</p> </li> <li> <p>Memory Issues</p> </li> <li>Use smaller data types (F16, INT8)</li> <li>Reduce batch size</li> <li> <p>Process documents in smaller chunks</p> </li> <li> <p>Performance Issues</p> </li> <li>Use ONNX models</li> <li>Optimize batch sizes</li> <li>Consider hardware acceleration (GPU)</li> </ol>"},{"location":"guides/Qwen3_Reranker/#error-messages","title":"Error Messages","text":"<ul> <li>\"Model not found\": Check model ID and internet connection</li> <li>\"ONNX Runtime error\": Ensure onnxruntime is properly installed</li> <li>\"Memory allocation failed\": Reduce batch size or use smaller data type</li> </ul>"},{"location":"guides/Qwen3_Reranker/#examples","title":"Examples","text":"<p>See the following example files for complete working examples:</p> <ul> <li><code>examples/qwen3_reranker.py</code> - Comprehensive examples</li> <li><code>examples/reranker.py</code> - Basic usage examples</li> <li><code>rust/examples/reranker.rs</code> - Rust implementation examples</li> </ul>"},{"location":"guides/Qwen3_Reranker/#contributing","title":"Contributing","text":"<p>The Qwen3 reranker implementation is part of the EmbedAnything project. Contributions are welcome! See the main project documentation for contribution guidelines.</p>"},{"location":"guides/Qwen3_Reranker/#license","title":"License","text":"<p>The Qwen3 reranker is subject to the same license as the EmbedAnything project. Please refer to the project LICENSE file for details.</p>"},{"location":"guides/actix_server/","title":"OpenAI-Compatible Server","text":"<p>This server provides an OpenAI-compatible API for generating embeddings using the EmbedAnything library. We choose Actix Server for:</p> <ol> <li>Blazing fast: Consistently ranks among the fastest web frameworks in benchmarks like TechEmpower.</li> <li>Asynchronous by default: Built on Rust\u2019s async/await, enabling efficient I/O-bound workloads.</li> <li>Lightweight &amp; modular: Minimal core with extensible middleware, plugins, and integrations.</li> <li>Type-safe: Strong type guarantees ensure fewer runtime surprises.</li> <li>Production-ready: Stable, mature, and already used in industries like fintech, IoT, and SaaS platforms.</li> </ol> <p>For benchmarks between python and rust servers, you check out this blog: https://www.jonvet.com/blog/benchmarking-python-rust-web-servers</p>"},{"location":"guides/actix_server/#features","title":"Features","text":"<ul> <li>OpenAI-compatible <code>/v1/embeddings</code> endpoint</li> <li>PDF embeddings via <code>/v1/pdf_embeddings/upload</code> (file upload)</li> <li>Support for multiple embedding models (Jina, BERT, etc.)</li> <li>Health check endpoint</li> </ul>"},{"location":"guides/actix_server/#running-the-server","title":"Running the Server","text":"<pre><code>cargo run -p server --release\n</code></pre> <p>The server will start on <code>http://0.0.0.0:8080</code>.</p>"},{"location":"guides/actix_server/#api-usage","title":"API Usage","text":""},{"location":"guides/actix_server/#create-embeddings","title":"Create Embeddings","text":"<p>Endpoint: <code>POST /v1/embeddings</code></p> <p>Request: <pre><code>{\n  \"model\": \"sentence-transformers/all-MiniLM-L12-v2\",\n  \"input\": [\"The quick brown fox jumps over the lazy dog\"]\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [0.0023064255, -0.009327292, ...]\n    }\n  ],\n  \"model\": \"sentence-transformers/all-MiniLM-L12-v2\",\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"total_tokens\": 9\n  }\n}\n</code></pre></p>"},{"location":"guides/actix_server/#pdf-embeddings-upload-files","title":"PDF Embeddings (Upload Files)","text":"<p>Endpoint: <code>POST /v1/pdf_embeddings/upload</code></p> <p>Upload PDF files as multipart form data to generate embeddings. Accepts one or more PDF files.</p> <p>Request: <code>multipart/form-data</code> - <code>model</code> (required): The embedding model to use (e.g., <code>sentence-transformers/all-MiniLM-L12-v2</code>) - <code>files</code> (required): One or more PDF file uploads</p> <p>Response: <pre><code>{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [0.0023064255, -0.009327292, ...],\n      \"metadata\": null,\n      \"text\": \"Extracted text from PDF page...\"\n    }\n  ],\n  \"model\": \"sentence-transformers/all-MiniLM-L12-v2\"\n}\n</code></pre></p>"},{"location":"guides/actix_server/#health-check","title":"Health Check","text":"<p>Endpoint: <code>GET /health_check</code></p> <p>Returns a 200 OK status if the server is running.</p>"},{"location":"guides/actix_server/#error-handling","title":"Error Handling","text":"<p>The API returns OpenAI-compatible error responses:</p> <pre><code>{\n  \"error\": {\n    \"message\": \"Error description\",\n    \"type\": \"error_type\",\n    \"code\": \"error_code\"\n  }\n}\n</code></pre>"},{"location":"guides/actix_server/#example-usage-with-curl","title":"Example Usage with curl","text":"<pre><code># Create embeddings\ncurl -X POST http://localhost:8080/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"sentence-transformers/all-MiniLM-L12-v2\",\n    \"input\": [\"Hello world\", \"How are you?\"]\n  }'\n\n# Upload PDF files for embeddings\ncurl -X POST http://localhost:8080/v1/pdf_embeddings/upload \\\n  -F \"model=sentence-transformers/all-MiniLM-L12-v2\" \\\n  -F \"files=@document1.pdf\" \\\n  -F \"files=@document2.pdf\"\n\n# Health check\ncurl http://localhost:8080/health_check\n</code></pre>"},{"location":"guides/actix_server/#example-usage-with-python","title":"Example Usage with Python","text":"<pre><code>import requests\n\n# Create embeddings\nresponse = requests.post(\n    \"http://localhost:8080/v1/embeddings\",\n    json={\n        \"model\": \"sentence-transformers/all-MiniLM-L12-v2\",\n        \"input\": [\"The quick brown fox jumps over the lazy dog\"]\n    }\n)\n\nif response.status_code == 200:\n    data = response.json()\n    print(f\"Generated {len(data['data'])} embeddings\")\n    print(f\"First embedding dimension: {len(data['data'][0]['embedding'])}\")\nelse:\n    print(f\"Error: {response.json()}\")\n\n# Upload PDF files for embeddings\nwith open(\"document.pdf\", \"rb\") as f:\n    upload_response = requests.post(\n        \"http://localhost:8080/v1/pdf_embeddings/upload\",\n        data={\"model\": \"sentence-transformers/all-MiniLM-L12-v2\"},\n        files={\"files\": (\"document.pdf\", f, \"application/pdf\")}\n    )\nif upload_response.status_code == 200:\n    data = upload_response.json()\n    print(f\"Generated {len(data['data'])} PDF embeddings\")\n</code></pre>"},{"location":"guides/adapters/","title":"Using Vector Database Adapters","text":"<p>Vector database adapters allow you to stream embeddings directly to your vector database without keeping them in memory. This enables efficient, memory-safe indexing of large document collections.</p>"},{"location":"guides/adapters/#how-adapters-work","title":"How Adapters Work","text":"<p>Adapters implement a common interface that: 1. Create indexes: Set up vector indexes with the correct dimensions 2. Stream embeddings: Receive embeddings as they're generated 3. Store metadata: Preserve text, file paths, and other metadata 4. Enable search: Make embeddings searchable via the vector database</p>"},{"location":"guides/adapters/#using-elasticsearch","title":"Using Elasticsearch","text":"<p>Elasticsearch is a distributed search and analytics engine. Perfect for production deployments requiring scalability.</p> <p>Installation: <pre><code>pip install elasticsearch\n</code></pre></p> <p>Basic Usage: <pre><code>import embed_anything\nimport os\nfrom embed_anything import EmbeddingModel, WhichModel, TextEmbedConfig\nfrom embed_anything.vectordb import ElasticAdapter\n\n# Initialize Elasticsearch adapter\nes_adapter = ElasticAdapter(\n    url=\"http://localhost:9200\",  # Elasticsearch URL\n    api_key=None  # Optional: API key for cloud Elasticsearch\n)\n\n# Create an index\nes_adapter.create_index(\n    dimension=384,           # Embedding dimension\n    metric=\"cosine\",         # Similarity metric\n    index_name=\"documents\"\n)\n\n# Load embedding model\nmodel = EmbeddingModel.from_pretrained_hf(\n    model_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n)\n\n# Embed files and stream to Elasticsearch\nconfig = TextEmbedConfig(chunk_size=1000, batch_size=32)\ndata = embed_anything.embed_file(\n    \"test_files/document.pdf\",\n    embedder=model,\n    config=config,\n    adapter=es_adapter  # Streams directly to Elasticsearch\n)\n</code></pre></p> <p>Complete Example: <pre><code>import embed_anything\nimport os\n\nfrom typing import Dict, List\nfrom embed_anything import EmbedData\nfrom embed_anything.vectordb import Adapter\nfrom embed_anything import EmbedData, EmbeddingModel, TextEmbedConfig, WhichModel\n\nfrom elasticsearch import Elasticsearch\nfrom elasticsearch.helpers import bulk\n\n\nclass ElasticsearchAdapter(Adapter):\n\n    def __init__(self, api_key: str, cloud_id: str, index_name: str = \"anything\"):\n        self.es = Elasticsearch(cloud_id=cloud_id, api_key=api_key)\n        self.index_name = index_name\n\n    def create_index(\n        self, dimension: int, metric: str, mappings={}, settings={}, **kwargs\n    ):\n\n        if \"index_name\" in kwargs:\n            self.index_name = kwargs[\"index_name\"]\n\n        self.es.indices.create(\n            index=self.index_name, mappings=mappings, settings=settings\n        )\n\n    def convert(self, embeddings: List[List[EmbedData]]) -&gt; List[Dict]:\n        data = []\n        for embedding in embeddings:\n            data.append(\n                {\n                    \"text\": embedding.text,\n                    \"embeddings\": embedding.embedding,\n                    \"metadata\": {\n                        \"file_name\": embedding.metadata[\"file_name\"],\n                        \"modified\": embedding.metadata[\"modified\"],\n                        \"created\": embedding.metadata[\"created\"],\n                    },\n                }\n            )\n        return data\n\n    def delete_index(self, index_name: str):\n        self.es.indices.delete(index=index_name)\n\n    def gendata(self, data):\n        for doc in data:\n            yield doc\n\n    def upsert(self, data: List[Dict]):\n        data = self.convert(data)\n        bulk(client=self.es, index=\"anything\", actions=self.gendata(data))\n\n\nindex_name = \"anything\"\nelastic_api_key = os.environ.get(\"ELASTIC_API_KEY\")\nelastic_cloud_id = os.environ.get(\"ELASTIC_CLOUD_ID\")\n\n# Initialize the ElasticsearchAdapter Class\nelasticsearch_adapter = ElasticsearchAdapter(\n    api_key=elastic_api_key,\n    cloud_id=elastic_cloud_id,\n    index_name=index_name,\n)\n\n# Prase PDF and insert documents into Elasticsearch.\nmodel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Bert, model_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n)\n\n\ndata = embed_anything.embed_file(\n    \"/home/sonamAI/projects/EmbedAnything/test_files/attention.pdf\",\n    embedder=model,\n    adapter=elasticsearch_adapter\n)\n\n# Create an Index with explicit mappings.\nmappings = {\n    \"properties\": {\n        \"embeddings\": {\"type\": \"dense_vector\", \"dims\": 384},\n        \"text\": {\"type\": \"text\"},\n    }\n}\nsettings = {}\n\nelasticsearch_adapter.create_index(\n    dimension=384,\n    metric=\"cosine\",\n    mappings=mappings,\n    settings=settings,\n)\n\n# Delete an Index\nelasticsearch_adapter.delete_index(index_name=index_name)\n</code></pre></p>"},{"location":"guides/adapters/#using-weaviate","title":"Using Weaviate","text":"<p>Weaviate is an open-source vector database with built-in ML capabilities.</p> <p>Installation: <pre><code>pip install weaviate-client\n</code></pre></p> <p>Basic Usage: <pre><code>import embed_anything\nfrom embed_anything import EmbeddingModel, WhichModel\nfrom embed_anything.vectordb import WeaviateAdapter\n\n# Initialize Weaviate adapter\nweaviate_adapter = WeaviateAdapter(\n    url=\"http://localhost:8080\",  # Weaviate instance URL\n    api_key=None  # Optional: API key for cloud Weaviate\n)\n\n# Create a collection (index)\nweaviate_adapter.create_index(\n    dimension=384,\n    metric=\"cosine\",\n    index_name=\"Documents\"\n)\n\n# Load model and embed\nmodel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Bert,\n    model_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n)\n\n# Stream embeddings to Weaviate\ndata = embed_anything.embed_directory(\n    \"test_files/\",\n    embedder=model,\n    adapter=weaviate_adapter\n)\n</code></pre></p> <p>Complete Example: <pre><code>import weaviate, os\nimport weaviate.classes as wvc\nfrom tqdm.auto import tqdm\nimport embed_anything\nfrom embed_anything import EmbedData, EmbeddingModel, TextEmbedConfig, WhichModel\nfrom embed_anything.vectordb import Adapter\nimport textwrap\n\n## Weaviate Adapter\n\nfrom typing import List\n\n\nclass WeaviateAdapter(Adapter):\n    def __init__(self, api_key, url):\n        super().__init__(api_key)\n        self.client = weaviate.connect_to_weaviate_cloud(\n            cluster_url=url, auth_credentials=wvc.init.Auth.api_key(api_key)\n        )\n        if self.client.is_ready():\n            print(\"Weaviate is ready\")\n\n    def create_index(self, index_name: str):\n        self.index_name = index_name\n        self.collection = self.client.collections.create(\n            index_name, vectorizer_config=wvc.config.Configure.Vectorizer.none()\n        )\n        return self.collection\n\n    def convert(self, embeddings: List[EmbedData]):\n        data = []\n        for embedding in embeddings:\n            property = embedding.metadata\n            property[\"text\"] = embedding.text\n            data.append(\n                wvc.data.DataObject(properties=property, vector=embedding.embedding)\n            )\n        return data\n\n    def upsert(self, data_):\n        data_ = self.convert(data_)\n        self.client.collections.get(self.index_name).data.insert_many(data_)\n\n    def delete_index(self, index_name: str):\n        self.client.collections.delete(index_name)\n\n\nURL = \"URL\"\nAPI_KEY = \"API_KEY\"\nweaviate_adapter = WeaviateAdapter(API_KEY, URL)\n\n\n# create index\nindex_name = \"Test_index\"\nif index_name in weaviate_adapter.client.collections.list_all():\n    weaviate_adapter.delete_index(index_name)\nweaviate_adapter.create_index(\"Test_index\")\n\n\nmodel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Bert, model_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n)\n\n\ndata = embed_anything.embed_file(\n    \"/home/sonamAI/projects/EmbedAnything/test_files/attention.pdf\",\n    embedder=model,\n    adapter=weaviate_adapter,\n)\n\nquery_vector = embed_anything.embed_query([\"What is attention\"], embedder=model)[\n    0\n].embedding\n\n\nresponse = weaviate_adapter.collection.query.near_vector(\n    near_vector=query_vector,\n    limit=2,\n    return_metadata=wvc.query.MetadataQuery(certainty=True),\n)\n\nfor i in range(len(response.objects)):\n    print(response.objects[i].properties[\"text\"])\n\n\nfor res in response.objects:\n    print(textwrap.fill(res.properties[\"text\"], width=120), end=\"\\n\\n\")\n</code></pre></p>"},{"location":"guides/adapters/#using-pinecone","title":"Using Pinecone","text":"<p>Pinecone is a managed vector database service, ideal for production applications.</p> <p>Installation: <pre><code>pip install pinecone\n</code></pre></p> <p>Basic Usage: <pre><code>import embed_anything\nimport os\nfrom embed_anything import EmbeddingModel, WhichModel\nfrom embed_anything.vectordb import PineconeAdapter\n\n# Initialize Pinecone adapter\napi_key = os.environ.get(\"PINECONE_API_KEY\")\npinecone_adapter = PineconeAdapter(api_key)\n\n# Create or use existing index\ntry:\n    pinecone_adapter.delete_index(\"documents\")\nexcept:\n    pass\n\npinecone_adapter.create_index(\n    dimension=512,      # Must match your model's embedding dimension\n    metric=\"cosine\",\n    index_name=\"documents\"\n)\n\n# Load model\nmodel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Clip,\n    model_id=\"openai/clip-vit-base-patch16\"\n)\n\n# Embed images and stream to Pinecone\ndata = embed_anything.embed_image_directory(\n    \"test_files\",\n    embedder=model,\n    adapter=pinecone_adapter\n)\n</code></pre></p> <p>Complete Example: <pre><code>import re\nfrom typing import Dict, List\nimport uuid\nimport embed_anything\nimport os\n\nfrom embed_anything.vectordb import Adapter\nfrom pinecone import Pinecone, ServerlessSpec\n\nfrom embed_anything import EmbedData, EmbeddingModel, WhichModel, TextEmbedConfig\n\n\nclass PineconeAdapter(Adapter):\n    \"\"\"\n    Adapter class for interacting with Pinecone, a vector database service.\n    \"\"\"\n\n    def __init__(self, api_key: str):\n        \"\"\"\n        Initializes a new instance of the PineconeAdapter class.\n\n        Args:\n            api_key (str): The API key for accessing the Pinecone service.\n        \"\"\"\n        super().__init__(api_key)\n        self.pc = Pinecone(api_key=self.api_key)\n        self.index_name = None\n\n    def create_index(\n        self,\n        dimension: int,\n        metric: str = \"cosine\",\n        index_name: str = \"anything\",\n        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n    ):\n        \"\"\"\n        Creates a new index in Pinecone.\n\n        Args:\n            dimension (int): The dimensionality of the embeddings.\n            metric (str, optional): The distance metric to use for similarity search. Defaults to \"cosine\".\n            index_name (str, optional): The name of the index. Defaults to \"anything\".\n            spec (ServerlessSpec, optional): The serverless specification for the index. Defaults to AWS in us-east-1 region.\n        \"\"\"\n        self.index_name = index_name\n        self.pc.create_index(\n            name=index_name, dimension=dimension, metric=metric, spec=spec\n        )\n\n    def delete_index(self, index_name: str):\n        \"\"\"\n        Deletes an existing index from Pinecone.\n\n        Args:\n            index_name (str): The name of the index to delete.\n        \"\"\"\n        self.pc.delete_index(name=index_name)\n\n    def convert(self, embeddings: List[EmbedData]) -&gt; List[Dict]:\n        \"\"\"\n        Converts a list of embeddings into the required format for upserting into Pinecone.\n\n        Args:\n            embeddings (List[EmbedData]): The list of embeddings to convert.\n\n        Returns:\n            List[Dict]: The converted data in the required format for upserting into Pinecone.\n        \"\"\"\n        data_emb = []\n\n        for embedding in embeddings:\n            data_emb.append(\n                {\n                    \"id\": str(uuid.uuid4()),\n                    \"values\": embedding.embedding,\n                    \"metadata\": {\n                        \"text\": embedding.text,\n                        \"file\": re.split(\n                            r\"/|\\\\\", embedding.metadata.get(\"file_name\", \"\")\n                        )[-1],\n                    },\n                }\n            )\n        return data_emb\n\n    def upsert(self, data: List[Dict]):\n        \"\"\"\n        Upserts data into the specified index in Pinecone.\n\n        Args:\n            data (List[Dict]): The data to upsert into Pinecone.\n\n        Raises:\n            ValueError: If the index has not been created before upserting data.\n        \"\"\"\n        data = self.convert(data)\n        if not self.index_name:\n            raise ValueError(\"Index must be created before upserting data\")\n        self.pc.Index(name=self.index_name).upsert(data)\n\n\n# Initialize the PineconeEmbedder class\napi_key = os.environ.get(\"PINECONE_API_KEY\")\nindex_name = \"anything\"\npinecone_adapter = PineconeAdapter(api_key)\n\ntry:\n    pinecone_adapter.delete_index(\"anything\")\nexcept:\n    pass\n\n# Initialize the PineconeEmbedder class\n\npinecone_adapter.create_index(dimension=512, metric=\"cosine\")\n\n\nmodel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Bert, model_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n)\n\n\ndata = embed_anything.embed_file(\n    \"/home/sonamAI/projects/EmbedAnything/test_files/attention.pdf\",\n    embedder=model,\n    adapter=pinecone_adapter,\n)\n\n\n\ndata = embed_anything.embed_image_directory(\n    \"test_files\",\n    embedder=model,\n    adapter=pinecone_adapter\n)\nprint(data)\n</code></pre></p>"},{"location":"guides/adapters/#using-qdrant","title":"Using Qdrant","text":"<p>Qdrant is a high-performance vector database written in Rust.</p> <p>Installation: <pre><code>pip install qdrant-client\n</code></pre></p> <p>Basic Usage: <pre><code>import embed_anything\nfrom embed_anything import EmbeddingModel, WhichModel\nfrom embed_anything.vectordb import QdrantAdapter\n\n# Initialize Qdrant adapter\nqdrant_adapter = QdrantAdapter(\n    url=\"http://localhost:6333\",  # Qdrant server URL\n    api_key=None  # Optional: API key for cloud Qdrant\n)\n\n# Create a collection\nqdrant_adapter.create_index(\n    dimension=384,\n    metric=\"cosine\",\n    index_name=\"documents\"\n)\n\n# Load model and embed\nmodel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Bert,\n    model_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n)\n\n# Stream to Qdrant\ndata = embed_anything.embed_file(\n    \"test_files/document.pdf\",\n    embedder=model,\n    adapter=qdrant_adapter\n)\n</code></pre></p> <p>Complete Example: <pre><code>import uuid\nfrom typing import List, Dict\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import (\n    Distance,\n    VectorParams,\n    PointStruct,\n)\nimport embed_anything\nfrom embed_anything import EmbedData, EmbeddingModel, WhichModel\nfrom embed_anything.vectordb import Adapter\n\n\nclass QdrantAdapter(Adapter):\n    \"\"\"\n    Adapter class for interacting with [Qdrant](https://qdrant.tech/).\n    \"\"\"\n\n    def __init__(self, client: QdrantClient):\n        \"\"\"\n        Initializes a new instance of the QdrantAdapter class.\n\n        Args:\n            client : An instance of qdrant_client.QdrantClient\n        \"\"\"\n        self.client = client\n\n    def create_index(\n        self,\n        dimension: int,\n        metric: Distance = Distance.COSINE,\n        index_name: str = \"embed-anything\",\n        **kwargs,\n    ):\n        self.collection_name = index_name\n\n        if not self.client.collection_exists(index_name):\n            self.client.create_collection(\n                collection_name=index_name,\n                vectors_config=VectorParams(size=dimension, distance=metric),\n            )\n\n    def delete_index(self, index_name: str):\n        self.client.delete_collection(collection_name=index_name)\n\n    def convert(self, embeddings: List[EmbedData]) -&gt; List[PointStruct]:\n        points = []\n        for embedding in embeddings:\n            points.append(\n                PointStruct(\n                    id=str(uuid.uuid4()),\n                    vector=embedding.embedding,\n                    payload={\n                        \"text\": embedding.text,\n                        \"file_name\": embedding.metadata[\"file_name\"],\n                        \"modified\": embedding.metadata[\"modified\"],\n                        \"created\": embedding.metadata[\"created\"],\n                    },\n                )\n            )\n        return points\n\n    def upsert(self, data: List[Dict]):\n        points = self.convert(data)\n        self.client.upsert(\n            collection_name=self.collection_name,\n            points=points,\n        )\n\n\ndef main():\n    adapter = QdrantAdapter(QdrantClient(location=\":memory:\"))\n    adapter.create_index(dimension=384)\n\n    model = EmbeddingModel.from_pretrained_hf(\n        WhichModel.Bert, model_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n    )\n\n    embed_anything.embed_file(\n        \"test_files/attention.pdf\",\n        embedder=model,\n        adapter=adapter,\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre></p>"},{"location":"guides/adapters/#using-milvus","title":"Using Milvus","text":"<p>Milvus is an open-source vector database built for scalable similarity search.</p> <p>Installation: <pre><code>pip install pymilvus\n</code></pre></p> <p>Basic Usage: <pre><code>import embed_anything\nfrom embed_anything import EmbeddingModel, WhichModel\nfrom embed_anything.vectordb import MilvusAdapter\n\n# Initialize Milvus adapter\nmilvus_adapter = MilvusAdapter(\n    host=\"localhost\",  # Milvus host\n    port=19530,        # Milvus port\n    user=None,         # Optional: username\n    password=None      # Optional: password\n)\n\n# Create a collection\nmilvus_adapter.create_index(\n    dimension=384,\n    metric=\"cosine\",\n    index_name=\"documents\"\n)\n\n# Load model and embed\nmodel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Bert,\n    model_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n)\n\n# Stream to Milvus\ndata = embed_anything.embed_directory(\n    \"test_files/\",\n    embedder=model,\n    adapter=milvus_adapter\n)\n</code></pre></p> <p>Complete Example: <pre><code>from pymilvus import MilvusClient, DataType\nimport os\nfrom typing import Dict, List\n\nimport embed_anything\nfrom embed_anything.vectordb import Adapter\nfrom embed_anything import EmbedData, EmbeddingModel, WhichModel\n\nprint(\"Milvus Vector DB - Adapter\")\n\n# Default embedding dimension\nEMBEDDINGS_DIM = 384\n# Maximum VARCHAR field length for text content\nTEXT_CONTENT_VARCHARS = 4098\n\n# Type annotation for embeddings\nVectorEmbeddings = List[List[EmbedData]]\n\nclass MilvusVectorAdapter(Adapter):\n    def __init__(self, uri: str = './milvus.db', token: str = '', collection_name: str = \"embed_anything_collection\"):\n        \"\"\"\n        Initialize the MilvusVectorAdapter.\n\n        Args:\n            uri (str): The URI to connect to, comes in the form of\n                \"https://address:port\" for Milvus or Zilliz Cloud service,\n                or \"path/to/local/milvus.db\" for the lite local Milvus. Defaults to\n                \"./milvus.db\".\n            token (str): The token for log in. Defaults to \"\".\n            collection_name (str): Name of the collection to use. Defaults to\n                \"embed_anything_collection\".\n        \"\"\"\n        self.collection_name = collection_name\n        self.client = MilvusClient(uri=uri, token=token)\n        print(\"Ok - Milvus DB connection established.\")\n\n    def create_index(self, dimension: int = EMBEDDINGS_DIM):\n        \"\"\"\n        Create a collection and index for embeddings.\n\n        Args:\n            dimension: Dimension of the embedding vectors.\n            **kwargs: Additional parameters for index creation.\n        \"\"\"\n        # Delete collection if it exists\n        if self.client.has_collection(self.collection_name):\n            self.delete_index()\n\n        # Create collection schema\n        schema = self.client.create_schema(auto_id=True)\n        schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n        schema.add_field(\n            field_name=\"embeddings\",\n            datatype=DataType.FLOAT_VECTOR,\n            dim=dimension\n        )\n        schema.add_field(\n            field_name=\"text\",\n            datatype=DataType.VARCHAR,\n            max_length=TEXT_CONTENT_VARCHARS\n        )\n        schema.add_field(\n            field_name=\"file_name\",\n            datatype=DataType.VARCHAR,\n            max_length=255\n        )\n        schema.add_field(\n            field_name=\"modified\",\n            datatype=DataType.VARCHAR,\n            max_length=50\n        )\n        schema.add_field(\n            field_name=\"created\",\n            datatype=DataType.VARCHAR,\n            max_length=50\n        )\n\n        # Create the collection\n        self.client.create_collection(\n            collection_name=self.collection_name,\n            schema=schema\n        )\n\n        # Create the index\n        index_params = self.client.prepare_index_params()\n        index_params.add_index(\n            field_name=\"embeddings\",\n            index_type=\"IVF_FLAT\",\n            metric_type=\"L2\",\n            params={\"nlist\": 1024}\n        )\n\n        # Apply the index\n        self.client.create_index(\n            collection_name=self.collection_name,\n            index_params=index_params\n        )\n\n        # Load the collection\n        self.client.load_collection(\n            collection_name=self.collection_name\n        )\n\n        print(f\"Collection '{self.collection_name}' created with index.\")\n\n    def convert(self, embeddings: List[EmbedData]) -&gt; List[Dict]:\n        \"\"\"\n        Convert EmbedData objects to a format compatible with Milvus.\n\n        Args:\n            embeddings: List of EmbedData objects.\n\n        Returns:\n            List of dictionaries with data formatted for Milvus.\n        \"\"\"\n        ret_data = []\n        for i, embedding in enumerate(embeddings):\n            data_dict = {\n                \"embeddings\": embedding.embedding,\n                \"text\": embedding.text,\n                \"file_name\": embedding.metadata[\"file_name\"],\n                \"modified\": embedding.metadata[\"modified\"],\n                \"created\": embedding.metadata[\"created\"],\n            }\n            ret_data.append(data_dict)\n\n        print(f\"Converted {len(ret_data)} embeddings for insertion.\")\n        return ret_data\n\n    def delete_index(self):\n        \"\"\"\n        Delete the collection and its index.\n        \"\"\"\n        try:\n            self.client.drop_collection(self.collection_name)\n            print(f\"Collection '{self.collection_name}' dropped.\")\n        except Exception as e:\n            print(f\"Failed to drop collection: {e}\")\n\n\n    def upsert(self, data: List[EmbedData]):\n        \"\"\"\n        Insert or update embeddings in the collection.\n\n        Args:\n            data: List of EmbedData objects to insert.\n        \"\"\"\n        # Convert data to Milvus format\n        formatted_data = self.convert(data)\n\n        # Insert data\n        self.client.insert(\n            collection_name=self.collection_name,\n            data=formatted_data\n        )\n\n        print(f\"Successfully inserted {len(formatted_data)} embeddings.\")\n\n\n\n\n\nif __name__ == \"__main__\":\n    # Initialize the MilvusVectorAdapter class\n    index_name = \"embed_anything_milvus_collection\"\n    milvus_adapter = MilvusVectorAdapter(uri='./milvus.db', collection_name=index_name)\n\n    # Delete existing index if it exists\n    try:\n        milvus_adapter.delete_index(index_name)\n    except:\n        pass\n\n    # Create a new index\n    milvus_adapter.create_index()\n\n    # Initialize the embedding model\n    model = EmbeddingModel.from_pretrained_hf(\n        WhichModel.Bert, \n        model_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n    )\n\n    # Embed a PDF file\n    data = embed_anything.embed_file(\n        \"path/to/your/file.pdf\",\n        embedder=model,\n        adapter=milvus_adapter,\n    )\n</code></pre></p>"},{"location":"guides/adapters/#benefits-of-using-adapters","title":"Benefits of Using Adapters","text":"<ol> <li>Memory efficiency: Embeddings are streamed directly to the database</li> <li>Scalability: Handle large document collections without memory issues</li> <li>Persistence: Embeddings are stored permanently in the database</li> <li>Search ready: Immediately searchable after embedding</li> <li>Production ready: Integrates with your existing vector database infrastructure</li> </ol>"},{"location":"guides/adapters/#choosing-the-right-adapter","title":"Choosing the Right Adapter","text":"<ul> <li>Elasticsearch: Best for existing Elasticsearch deployments, full-text + vector search</li> <li>Weaviate: Great for ML-powered applications, built-in classification</li> <li>Pinecone: Managed service, zero infrastructure management</li> <li>Qdrant: High performance, Rust-based, self-hosted</li> <li>Milvus: Scalable, cloud-native, production-grade</li> </ul>"},{"location":"guides/adapters/#best-practices","title":"Best Practices","text":"<ol> <li>Match dimensions: Ensure the adapter's dimension matches your model's output</li> <li>Choose metric: Use \"cosine\" for most cases, \"euclidean\" for distance-based</li> <li>Batch processing: Process multiple files for better throughput</li> <li>Error handling: Wrap adapter calls in try-except blocks</li> <li>Index management: Delete and recreate indexes when changing models</li> </ol>"},{"location":"guides/colpali/","title":"Using ColPali","text":"<p>ColPali is a specialized model designed for high-performance document embedding and semantic search. It processes PDFs by treating each page as an image, making it ideal for documents with complex layouts, tables, and figures. ColPali supports both native Candle and ONNX formats for flexible deployment.</p>"},{"location":"guides/colpali/#key-features","title":"Key Features","text":"<ul> <li>Document-focused: Optimized for PDF documents with visual elements</li> <li>Page-level embeddings: Each page is embedded as a whole, preserving layout context</li> <li>Fast inference: ONNX support for optimized performance</li> <li>Query matching: Efficient similarity search across document pages</li> </ul>"},{"location":"guides/colpali/#basic-usage","title":"Basic Usage","text":"<pre><code>from embed_anything import ColpaliModel\nimport numpy as np\nfrom pathlib import Path\n\n# Load ColPali model (Candle backend)\nmodel = ColpaliModel.from_pretrained(\"vidore/colpali-v1.2-merged\", None)\n\n# Or load ONNX model for faster inference\n# model = ColpaliModel.from_pretrained_onnx(\n#     \"starlight-ai/colpali-v1.2-merged-onnx\", \n#     None\n# )\n\n# Get all PDF files in a directory\ndirectory = Path(\"test_files\")\nfiles = list(directory.glob(\"*.pdf\"))\n\n# Embed all PDF files\nfile_embed_data = []\nfor file in files:\n    try:\n        # Embed each file (returns page-level embeddings)\n        embedding = model.embed_file(str(file), batch_size=1)\n        file_embed_data.extend(embedding)\n    except Exception as e:\n        print(f\"Error embedding file {file}: {e}\")\n\nprint(f\"Total pages embedded: {len(file_embed_data)}\")\n</code></pre>"},{"location":"guides/colpali/#querying-documents","title":"Querying Documents","text":"<pre><code>from embed_anything import ColpaliModel\nimport numpy as np\nfrom tabulate import tabulate\n\n# Load model\nmodel = ColpaliModel.from_pretrained_onnx(\n    \"starlight-ai/colpali-v1.2-merged-onnx\", \n    None\n)\n\n# Embed documents (assuming file_embed_data from previous example)\nfile_embeddings = np.array([e.embedding for e in file_embed_data])\n\n# Define your query\nquery = \"What are Positional Encodings\"\n\n# Embed the query\nquery_embedding = model.embed_query(query)\nquery_embeddings = np.array([e.embedding for e in query_embedding])\n\n# Calculate similarity scores using einsum for efficient computation\n# This computes the maximum similarity across tokens and sums them\nscores = (\n    np.einsum(\"bnd,csd-&gt;bcns\", query_embeddings, file_embeddings)\n    .max(axis=3)      # Max across token dimension\n    .sum(axis=2)      # Sum across query tokens\n    .squeeze()        # Remove singleton dimensions\n)\n\n# Get top 5 most relevant pages\ntop_pages = np.argsort(scores)[::-1][:5]\n\n# Display results\ntable = [\n    [\n        file_embed_data[page].metadata[\"file_path\"],\n        file_embed_data[page].metadata[\"page_number\"],\n        f\"{scores[page]:.4f}\"\n    ]\n    for page in top_pages\n]\n\nprint(tabulate(table, headers=[\"File Name\", \"Page Number\", \"Score\"], tablefmt=\"grid\"))\n\n# Access page images if needed\nimages = [file_embed_data[page].metadata.get(\"image\") for page in top_pages]\n</code></pre>"},{"location":"guides/colpali/#complete-example","title":"Complete Example","text":"<pre><code>from embed_anything import EmbedData, ColpaliModel\nimport numpy as np\nfrom tabulate import tabulate\nfrom pathlib import Path\n\n\n# Load the model\nmodel: ColpaliModel = ColpaliModel.from_pretrained(\"vidore/colpali-v1.2-merged\", None)\n\n# Load ONNX Model\n# model: ColpaliModel = ColpaliModel.from_pretrained_onnx(\n#     \"starlight-ai/colpali-v1.2-merged-onnx\", None\n# )\n\n# Get all PDF files in the directory\ndirectory = Path(\"test_files\")\nfiles = list(directory.glob(\"*.pdf\"))\n# files = [Path(\"test_files/attention.pdf\")]\n\nfile_embed_data: list[EmbedData] = []\nfor file in files:\n    try:\n        embedding: list[EmbedData] = model.embed_file(str(file), batch_size=1)\n        file_embed_data.extend(embedding)\n    except Exception as e:\n        print(f\"Error embedding file {file}: {e}\")\n\n# Define the query\nquery = \"What are Positional Encodings\"\n\n# Scoring\nfile_embeddings = np.array([e.embedding for e in file_embed_data])\nquery_embedding = model.embed_query(query)\nquery_embeddings = np.array([e.embedding for e in query_embedding])\nprint(file_embeddings.shape)\nprint(query_embeddings.shape)\n\nscores = (\n    np.einsum(\"bnd,csd-&gt;bcns\", query_embeddings, file_embeddings)\n    .max(axis=3)\n    .sum(axis=2)\n    .squeeze()\n)\n\n# Get top pages\ntop_pages = np.argsort(scores)[::-1][:5]\n\n# Extract file names and page numbers\ntable = [\n    [\n        file_embed_data[page].metadata[\"file_path\"],\n        file_embed_data[page].metadata[\"page_number\"],\n    ]\n    for page in top_pages\n]\n\n# Print the results in a table\nprint(tabulate(table, headers=[\"File Name\", \"Page Number\"], tablefmt=\"grid\"))\n\nimages = [file_embed_data[page].metadata[\"image\"] for page in top_pages]\n</code></pre>"},{"location":"guides/colpali/#when-to-use-colpali","title":"When to Use ColPali","text":"<ul> <li>Document search: When you need to search through PDFs with complex layouts</li> <li>Visual content: Documents with tables, figures, and diagrams</li> <li>Page-level retrieval: When you want to retrieve entire pages rather than text chunks</li> <li>Multimodal documents: Documents where visual layout is important</li> </ul>"},{"location":"guides/colpali/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use ONNX for production: ONNX models are faster and use less memory</li> <li>Batch processing: Process multiple files in parallel when possible</li> <li>Batch size: Use <code>batch_size=1</code> for ColPali as it processes pages individually</li> <li>GPU acceleration: Install <code>embed-anything-gpu</code> for GPU support</li> </ol>"},{"location":"guides/images/","title":"Searching Images","text":"<p>EmbedAnything enables semantic image search using vision-language models like CLIP and SigLip. These models can understand both images and text, allowing you to search images using natural language queries.</p>"},{"location":"guides/images/#overview","title":"Overview","text":"<p>Image search with EmbedAnything works by: 1. Embedding images: Converting images into high-dimensional vectors 2. Embedding queries: Converting text queries into the same vector space 3. Similarity search: Finding images with similar embeddings to your query</p>"},{"location":"guides/images/#basic-image-search","title":"Basic Image Search","text":"<pre><code>import embed_anything\nimport numpy as np\nfrom embed_anything import EmbeddingModel, WhichModel\n\n# Load CLIP model for image-text embeddings\nmodel = EmbeddingModel.from_pretrained_hf(\n    model_id=\"openai/clip-vit-base-patch16\"\n)\n\n# Embed all images in a directory\ndata = embed_anything.embed_image_directory(\"test_files\", embedder=model)\n\n# Convert embeddings to numpy array\nembeddings = np.array([item.embedding for item in data])\n\n# Embed a text query\nquery = [\"Photo of a monkey\"]\nquery_embedding = np.array(\n    embed_anything.embed_query(query, embedder=model)[0].embedding\n)\n\n# Calculate cosine similarities\nsimilarities = np.dot(embeddings, query_embedding)\n\n# Find most similar images\nmost_similar_idx = np.argmax(similarities)\nprint(f\"Most similar image: {data[most_similar_idx].text}\")\nprint(f\"Similarity score: {similarities[most_similar_idx]:.4f}\")\n</code></pre>"},{"location":"guides/images/#ranking-multiple-results","title":"Ranking Multiple Results","text":"<pre><code>import embed_anything\nimport numpy as np\nfrom embed_anything import EmbeddingModel, WhichModel\n\n# Load model\nmodel = EmbeddingModel.from_pretrained_hf(\n    model_id=\"openai/clip-vit-base-patch16\"\n)\n\n# Embed images\ndata = embed_anything.embed_image_directory(\"test_files\", embedder=model)\nembeddings = np.array([item.embedding for item in data])\n\n# Query\nquery = [\"Photo of a monkey\"]\nquery_embedding = np.array(\n    embed_anything.embed_query(query, embedder=model)[0].embedding\n)\n\n# Calculate similarities\nsimilarities = np.dot(embeddings, query_embedding)\n\n# Get top 5 results\ntop_5_indices = np.argsort(similarities)[::-1][:5]\n\nprint(\"Top 5 most similar images:\")\nfor i, idx in enumerate(top_5_indices, 1):\n    print(f\"{i}. {data[idx].text} (score: {similarities[idx]:.4f})\")\n</code></pre>"},{"location":"guides/images/#using-siglip-alternative-to-clip","title":"Using SigLip (Alternative to CLIP)","text":"<p>SigLip is a newer model that often performs better than CLIP:</p> <pre><code>from embed_anything import EmbeddingModel\n\n# Load SigLip model\nmodel = EmbeddingModel.from_pretrained_hf(\n    model_id=\"google/siglip-base-patch16-224\"\n)\n\n# Use it the same way as CLIP\ndata = embed_anything.embed_image_directory(\"test_files\", embedder=model)\n</code></pre>"},{"location":"guides/images/#image-to-image-search","title":"Image-to-Image Search","text":"<p>You can also search for images similar to a reference image:</p> <pre><code>import embed_anything\nimport numpy as np\nfrom embed_anything import EmbeddingModel, WhichModel\n\n# Load model\nmodel = EmbeddingModel.from_pretrained_hf(\n    model_id=\"openai/clip-vit-base-patch16\"\n)\n\n# Embed all images\ndata = embed_anything.embed_image_directory(\"test_files\", embedder=model)\nembeddings = np.array([item.embedding for item in data])\n\n# Embed a reference image (treat it as a query)\nreference_image_path = \"test_files/reference.jpg\"\nreference_data = embed_anything.embed_file(reference_image_path, embedder=model)\nreference_embedding = np.array(reference_data[0].embedding)\n\n# Find similar images\nsimilarities = np.dot(embeddings, reference_embedding)\nmost_similar_idx = np.argmax(similarities)\n\nprint(f\"Most similar to reference: {data[most_similar_idx].text}\")\n</code></pre>"},{"location":"guides/images/#complete-example","title":"Complete Example","text":"<pre><code>import numpy as np\nimport embed_anything\nfrom embed_anything import EmbedData\nimport time\n\nstart = time.time()\n\n# Load the model.\nmodel = embed_anything.EmbeddingModel.from_pretrained_hf(\n    model_id=\"google/siglip-base-patch16-224\",\n)\ndata: list[EmbedData] = embed_anything.embed_image_directory(\n    \"test_files\", embedder=model\n)\n\n# Convert the embeddings to a numpy array\nembeddings = np.array([data.embedding for data in data])\n\n# Embed a query\nquery = [\"Photo of a monkey\"]\nquery_embedding = np.array(\n    embed_anything.embed_query(query, embedder=model)[0].embedding\n)\n\n# Calculate the similarities between the query embedding and all the embeddings\nsimilarities = np.dot(embeddings, query_embedding)\n\n# Find the index of the most similar embedding\nmax_index = np.argmax(similarities)\n\nprint(\"Descending order of similarity: \")\nindices = np.argsort(similarities)[::-1]\nfor idx in indices:\n    print(data[idx].text)\n\nprint(\"----------- \")\n\n# Print the most similar image\nprint(\"Most similar image: \", data[max_index].text)\nend = time.time()\nprint(\"Time taken: \", end - start)\n</code></pre>"},{"location":"guides/images/#supported-models","title":"Supported Models","text":"<p>EmbedAnything supports the following models for image search:</p>"},{"location":"guides/images/#clip-models","title":"CLIP Models","text":"<ul> <li><code>openai/clip-vit-base-patch32</code> - Fastest, smaller embeddings</li> <li><code>openai/clip-vit-base-patch16</code> - Balanced performance (recommended)</li> <li><code>openai/clip-vit-large-patch14-336</code> - Best quality, slower</li> <li><code>openai/clip-vit-large-patch14</code> - High quality</li> </ul>"},{"location":"guides/images/#siglip-models","title":"SigLip Models","text":"<ul> <li><code>google/siglip-base-patch16-224</code> - Modern alternative to CLIP</li> <li><code>google/siglip-large-patch16-384</code> - Higher quality</li> </ul>"},{"location":"guides/images/#performance-tips","title":"Performance Tips","text":"<ol> <li>Model selection: Use <code>clip-vit-base-patch16</code> for a good balance of speed and quality</li> <li>Batch processing: Process multiple images in parallel</li> <li>GPU acceleration: Install <code>embed-anything-gpu</code> for faster inference</li> <li>Normalize embeddings: For cosine similarity, embeddings are automatically normalized</li> </ol>"},{"location":"guides/images/#use-cases","title":"Use Cases","text":"<ul> <li>Content moderation: Find inappropriate images</li> <li>Product search: Search e-commerce catalogs</li> <li>Photo organization: Organize personal photo libraries</li> <li>Visual search: Find similar products or designs</li> <li>Accessibility: Describe images for visually impaired users</li> </ul>"},{"location":"guides/ocr/","title":"Use PDFs that need OCR","text":"<p>Embed Anything can be used to embed scanned documents using OCR. This is useful for tasks such as document search and retrieval. You can set <code>use_ocr=True</code> in the <code>TextEmbedConfig</code> to enable OCR. But this requires <code>tesseract</code> and <code>poppler</code> to be installed.</p> <p>You can install <code>tesseract</code> and <code>poppler</code> using the following commands:</p>"},{"location":"guides/ocr/#install-tesseract-and-poppler","title":"Install Tesseract and Poppler","text":""},{"location":"guides/ocr/#windows","title":"Windows","text":"<p>For Tesseract, download the installer from here and install it.</p> <p>For Poppler, download the installer from here and install it.</p>"},{"location":"guides/ocr/#macos","title":"MacOS","text":"<p>For Tesseract, you can install it using Homebrew.</p> <pre><code>brew install tesseract\n</code></pre> <p>For Poppler, you can install it using Homebrew.</p> <pre><code>brew install poppler\n</code></pre>"},{"location":"guides/ocr/#linux","title":"Linux","text":"<p>For Tesseract, you can install it using the package manager for your Linux distribution. For example, on Ubuntu, you can install it using:</p> <pre><code>sudo apt install tesseract-ocr\nsudo apt install libtesseract-dev\n</code></pre> <p>For Poppler, you can install it using the package manager for your Linux distribution. For example, on Ubuntu, you can install it using:</p> <pre><code>sudo apt install poppler-utils\n</code></pre> <p>For more information, refer to the Tesseract installation guide.</p>"},{"location":"guides/ocr/#example-usage","title":"Example Usage","text":"<pre><code># OCR Requires `tesseract` and `poppler` to be installed.\n\nimport time\nimport embed_anything\nfrom embed_anything import EmbedData, EmbeddingModel, TextEmbedConfig, WhichModel\nfrom time import time\n\n\nmodel = EmbeddingModel.from_pretrained_hf(\n    model_id=\"jinaai/jina-embeddings-v2-small-en\"\n)\n\nconfig = TextEmbedConfig(\n    chunk_size=1000,\n    batch_size=32,\n    buffer_size=64,\n    splitting_strategy=\"sentence\",\n    use_ocr=True,\n)\n\nstart = time()\n\ndata: list[EmbedData] = embed_anything.embed_file(\n    \"/home/akshay/projects/starlaw/src-server/test_files/court.pdf\",  # Replace with your file path\n    embedder=model,\n    config=config,\n)\nend = time()\n\nfor d in data:\n    print(d.text)\n    print(\"---\" * 20)\n\nprint(f\"Time taken: {end - start} seconds\")\n</code></pre>"},{"location":"guides/onnx_models/","title":"Using ONNX Models","text":""},{"location":"guides/onnx_models/#supported-models","title":"Supported Models","text":"Enum Variant Description <code>AllMiniLML6V2</code> sentence-transformers/all-MiniLM-L6-v2 <code>AllMiniLML6V2Q</code> Quantized sentence-transformers/all-MiniLM-L6-v2 <code>AllMiniLML12V2</code> sentence-transformers/all-MiniLM-L12-v2 <code>AllMiniLML12V2Q</code> Quantized sentence-transformers/all-MiniLM-L12-v2 <code>ModernBERTBase</code> nomic-ai/modernbert-embed-base <code>ModernBERTLarge</code> nomic-ai/modernbert-embed-large <code>BGEBaseENV15</code> BAAI/bge-base-en-v1.5 <code>BGEBaseENV15Q</code> Quantized BAAI/bge-base-en-v1.5 <code>BGELargeENV15</code> BAAI/bge-large-en-v1.5 <code>BGELargeENV15Q</code> Quantized BAAI/bge-large-en-v1.5 <code>BGESmallENV15</code> BAAI/bge-small-en-v1.5 - Default <code>BGESmallENV15Q</code> Quantized BAAI/bge-small-en-v1.5 <code>NomicEmbedTextV1</code> nomic-ai/nomic-embed-text-v1 <code>NomicEmbedTextV15</code> nomic-ai/nomic-embed-text-v1.5 <code>NomicEmbedTextV15Q</code> Quantized nomic-ai/nomic-embed-text-v1.5 <code>ParaphraseMLMiniLML12V2</code> sentence-transformers/paraphrase-MiniLM-L6-v2 <code>ParaphraseMLMiniLML12V2Q</code> Quantized sentence-transformers/paraphrase-MiniLM-L6-v2 <code>ParaphraseMLMpnetBaseV2</code> sentence-transformers/paraphrase-mpnet-base-v2 <code>BGESmallZHV15</code> BAAI/bge-small-zh-v1.5 <code>MultilingualE5Small</code> intfloat/multilingual-e5-small <code>MultilingualE5Base</code> intfloat/multilingual-e5-base <code>MultilingualE5Large</code> intfloat/multilingual-e5-large <code>MxbaiEmbedLargeV1</code> mixedbread-ai/mxbai-embed-large-v1 <code>MxbaiEmbedLargeV1Q</code> Quantized mixedbread-ai/mxbai-embed-large-v1 <code>GTEBaseENV15</code> Alibaba-NLP/gte-base-en-v1.5 <code>GTEBaseENV15Q</code> Quantized Alibaba-NLP/gte-base-en-v1.5 <code>GTELargeENV15</code> Alibaba-NLP/gte-large-en-v1.5 <code>GTELargeENV15Q</code> Quantized Alibaba-NLP/gte-large-en-v1.5 <code>JINAV2SMALLEN</code> jinaai/jina-embeddings-v2-small-en <code>JINAV2BASEEN</code> jinaai/jina-embeddings-v2-base-en <code>JINAV3</code> jinaai/jina-embeddings-v3"},{"location":"guides/onnx_models/#example-usage","title":"Example Usage","text":"<pre><code>import heapq\nfrom embed_anything import (\n    EmbeddingModel,\n    TextEmbedConfig,\n    WhichModel,\n    embed_query,\n    ONNXModel,\n    Dtype,\n)\nimport os\nfrom time import time\nimport numpy as np\n\nmodel = EmbeddingModel.from_pretrained_onnx(\n    model = WhichModel.Bert, model_name=ONNXModel.ModernBERTBase, dtype = Dtype.Q4F16\n)\n\n# model = EmbeddingModel.from_pretrained_hf(\n#     model = WhichModel.Bert, model_name=\"BAAI/bge-small-en-v1.5\"\n# )\n\nsentences = [\n    \"The quick brown fox jumps over the lazy dog\",\n    \"The cat is sleeping on the mat\",\n    \"The dog is barking at the moon\",\n    \"I love pizza\",\n    \"I like to have pasta\",\n    \"The dog is sitting in the park\",\n]\n\nembedddings = embed_query(sentences, embedder=model)\n\nembed_vector = np.array([e.embedding for e in embedddings])\n\nprint(\"shape of embed_vector\", embed_vector.shape)\nsimilarities = np.matmul(embed_vector, embed_vector.T)\n\n# get top 5 similarities and show the two sentences and their similarity scores\n# Flatten the upper triangle of the similarity matrix, excluding the diagonal\nsimilarity_scores = [\n    (similarities[i, j], i, j)\n    for i in range(len(sentences))\n    for j in range(i + 1, len(sentences))\n]\n\n# Get the top 5 similarity scores\ntop_5_similarities = heapq.nlargest(5, similarity_scores, key=lambda x: x[0])\n\n# Print the top 5 similarities with sentences\nfor score, i, j in top_5_similarities:\n    print(f\"Score: {score:.2} | {sentences[i]} | {sentences[j]}\")\n\n\nfrom embed_anything import EmbeddingModel, WhichModel, embed_query, TextEmbedConfig\nimport os\nimport pymupdf\nfrom semantic_text_splitter import TextSplitter\nimport os\n\nmodel = EmbeddingModel.from_pretrained_onnx(WhichModel.Bert, ONNXModel.BGESmallENV15Q)\nsplitter = TextSplitter(1000)\nconfig = TextEmbedConfig(batch_size=128)\n\n\ndef embed_anything():\n    # get all pdfs from test_files\n\n    for file in os.listdir(\"bench\"):\n        text = []\n        doc = pymupdf.open(\"bench/\" + file)\n\n        for page in doc:\n            text.append(page.get_text())\n\n        text = \" \".join(text)\n        chunks = splitter.chunks(text)\n        embeddings = embed_query(chunks, model, config)\n\n\nstart = time()\nembed_anything()\n\nprint(time() - start)\n</code></pre>"},{"location":"guides/semantic/","title":"Using Semantic Chunking","text":"<p>Semantic chunking is a technique that splits text at semantically meaningful boundaries rather than fixed character or token limits. This approach preserves the logical flow and meaning of text, making it essential for applications like document retrieval, question answering, and summarization.</p>"},{"location":"guides/semantic/#why-semantic-chunking","title":"Why Semantic Chunking?","text":"<p>Traditional chunking methods split text at fixed sizes (e.g., every 1000 characters), which can: - Break sentences mid-thought - Split related concepts - Lose context between chunks</p> <p>Semantic chunking addresses these issues by: - Preserving complete thoughts and sentences - Maintaining context within chunks - Improving retrieval quality - Better alignment with embedding model understanding</p>"},{"location":"guides/semantic/#basic-usage","title":"Basic Usage","text":"<pre><code>from embed_anything import EmbeddingModel, WhichModel, TextEmbedConfig\nimport embed_anything\n\n# Main embedding model - generates final embeddings\nmodel = EmbeddingModel.from_pretrained_hf(\n    model_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n)\n\n# Semantic encoder - determines where to split text\n# This model analyzes text to find natural semantic breaks\nsemantic_encoder = EmbeddingModel.from_pretrained_hf(\n    model_id=\"jinaai/jina-embeddings-v2-small-en\"\n)\n\n# Configure semantic chunking\nconfig = TextEmbedConfig(\n    chunk_size=1000,                    # Target chunk size (approximate)\n    batch_size=32,                      # Process 32 chunks at once\n    splitting_strategy=\"semantic\",      # Enable semantic splitting\n    semantic_encoder=semantic_encoder    # Model for semantic analysis\n)\n\n# Embed a file with semantic chunking\ndata = embed_anything.embed_file(\"test_files/document.pdf\", embedder=model, config=config)\n\n# Chunks will be split at semantically meaningful boundaries\nfor i, item in enumerate(data):\n    print(f\"Chunk {i+1}:\")\n    print(f\"Text: {item.text[:200]}...\")\n    print(f\"Length: {len(item.text)} characters\")\n    print(\"---\" * 20)\n</code></pre>"},{"location":"guides/semantic/#how-it-works","title":"How It Works","text":"<ol> <li>Text Analysis: The semantic encoder analyzes the text to identify semantic boundaries</li> <li>Boundary Detection: Finds natural break points (sentence endings, paragraph breaks, topic shifts)</li> <li>Chunk Creation: Creates chunks that respect these boundaries while staying close to the target size</li> <li>Embedding: The main model embeds each semantically coherent chunk</li> </ol>"},{"location":"guides/semantic/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>from embed_anything import EmbeddingModel, WhichModel, TextEmbedConfig\nimport embed_anything\n\n# Use a more powerful semantic encoder for better boundary detection\nsemantic_encoder = EmbeddingModel.from_pretrained_hf(\n    model_id=\"jinaai/jina-embeddings-v2-base-en\"  # Larger model for better analysis\n)\n\n# Fine-tune chunking parameters\nconfig = TextEmbedConfig(\n    chunk_size=1500,                    # Larger chunks for more context\n    batch_size=16,                      # Smaller batches for memory efficiency\n    splitting_strategy=\"semantic\",\n    semantic_encoder=semantic_encoder,\n    buffer_size=128                     # Buffer for streaming\n)\n\n# Process multiple files\ndata = embed_anything.embed_directory(\n    \"test_files/\",\n    embedder=model,\n    config=config\n)\n</code></pre>"},{"location":"guides/semantic/#comparison-semantic-vs-regular-chunking","title":"Comparison: Semantic vs. Regular Chunking","text":""},{"location":"guides/semantic/#regular-chunking-sentence-based","title":"Regular Chunking (Sentence-based)","text":"<p><pre><code>config = TextEmbedConfig(\n    chunk_size=1000,\n    splitting_strategy=\"sentence\"  # Simple sentence splitting\n)\n</code></pre> Result: Chunks may break at awkward points, losing context.</p>"},{"location":"guides/semantic/#semantic-chunking","title":"Semantic Chunking","text":"<p><pre><code>config = TextEmbedConfig(\n    chunk_size=1000,\n    splitting_strategy=\"semantic\",\n    semantic_encoder=semantic_encoder\n)\n</code></pre> Result: Chunks preserve meaning and context, improving retrieval quality.</p>"},{"location":"guides/semantic/#best-practices","title":"Best Practices","text":"<ol> <li>Choose appropriate semantic encoder: Use a model that matches your domain</li> <li>Balance chunk size: Too small loses context, too large dilutes focus</li> <li>Match encoder and embedder: Use similar model families for consistency</li> <li>Test chunk quality: Inspect chunks to ensure they're semantically coherent</li> </ol>"},{"location":"guides/semantic/#complete-example","title":"Complete Example","text":"<pre><code>import embed_anything\nfrom embed_anything import EmbeddingModel, TextEmbedConfig, WhichModel\n\nmodel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Jina, model_id=\"jinaai/jina-embeddings-v2-small-en\"\n)\n\n# with semantic encoder\nsemantic_encoder = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Jina, model_id=\"jinaai/jina-embeddings-v2-small-en\"\n)\nconfig = TextEmbedConfig(\n    chunk_size=1000,\n    batch_size=32,\n    splitting_strategy=\"semantic\",\n    semantic_encoder=semantic_encoder,\n)\n\ndata = embed_anything.embed_file(\"test_files/bank.txt\", embedder=model, config=config)\n\nfor d in data:\n    print(d.text)\n    print(\"---\" * 20)\n</code></pre>"},{"location":"guides/semantic/#use-cases","title":"Use Cases","text":"<ul> <li>RAG Systems: Better context preservation for retrieval-augmented generation</li> <li>Document Q&amp;A: More accurate answers by maintaining context</li> <li>Long-form content: Books, research papers, technical documentation</li> <li>Conversational AI: Maintaining dialogue context</li> </ul>"},{"location":"roadmap/contribution/","title":"Contribution Guidelines","text":""},{"location":"roadmap/contribution/#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>To get started, check the [Issues Section] for tasks labeled \"Good First Issue\" or \"Help Needed\". These issues are perfect for new contributors or those looking to make a valuable impact quickly.</p> <p>If you find an issue you want to tackle:</p> <p>Comment on the issue to let us know you\u2019d like to work on it. Wait for confirmation\u2014an admin will assign the issue to you. \ud83d\udcbb Setting Up Your Development Environment To start working on the project, follow these steps:</p> <ol> <li>Fork the Repository: Begin by forking the repository from the dev branch. We do not allow direct contributions to the main branch.</li> <li>Clone Your Fork: After forking, clone the repository to your local machine.</li> <li>Create a New Branch: For each contribution, create a new branch following the naming convention: feature/your-feature-name or bugfix/your-bug-name.</li> </ol>"},{"location":"roadmap/contribution/#contributing-guidelines","title":"\ud83d\udee0\ufe0f Contributing Guidelines","text":"<p>\ud83d\udd0d Reporting Bugs If you find a bug, here\u2019s how to report it effectively:</p> <p>Title: Use a clear and descriptive title, with appropriate labels. Description: Provide a detailed description of the issue, including: Steps to reproduce the problem. Expected and actual behavior.</p> <p>Any relevant logs, screenshots, or additional context. Submit the Bug Report: Open a new issue in the [Issues Section] and include all the details. This helps us understand and resolve the problem faster.</p>"},{"location":"roadmap/contribution/#contributing-to-python-code","title":"\ud83d\udc0d Contributing to Python Code","text":"<p>If you're contributing to the Python codebase, follow these steps:</p> <ol> <li>Create an Independent File: Write your code in a new file within the python folder. </li> <li>Build with Maturin: After writing your code, use maturin build to build the package. </li> <li>Import and Call the Function: </li> <li>Use the following import syntax: from embed_anything. import *  <li>Then, call the function using: from embed_anything import   Feel free to open an issue if you encounter any problems during the process. <p>\ud83e\udde9 Contributing to Adapters To contribute to adapters, follow these guidelines:</p> <ol> <li>Implement Adapter Class: Create an Adapter class that supports the create, add, and delete operations for your specific use case. </li> <li>Check Existing Adapters: Use the existing Pinecone and Weaviate adapters as references to maintain consistency in structure and functionality. </li> <li>Testing: Ensure your adapter is tested thoroughly before submitting a pull request.</li> </ol>"},{"location":"roadmap/contribution/#submitting-a-pull-request","title":"\ud83d\udd04 Submitting a Pull Request","text":"<p>Once your contribution is ready: </p> <p>Push Your Branch: Push your branch to your forked repository.</p> <p>Submit a Pull Request (PR): Open a PR from your branch to the dev branch of the main repository. Ensure your PR includes:</p> <ol> <li>A clear description of the changes.</li> <li>Any relevant issue numbers (e.g., \"Closes #123\").</li> <li>Wait for Review: A maintainer will review your PR. Please be responsive to any feedback or requested changes.</li> </ol>"},{"location":"roadmap/roadmap/","title":"\ud83c\udfce\ufe0f RoadMap","text":""},{"location":"roadmap/roadmap/#accomplishments","title":"Accomplishments","text":"<p>One of the aims of EmbedAnything is to allow AI engineers to easily use state of the art embedding models on typical files and documents. A lot has already been accomplished here and these are the formats that we support right now and a few more have to be done. </p>"},{"location":"roadmap/roadmap/#modalities-and-source","title":"\ud83d\uddbc\ufe0f Modalities and Source","text":"<p>We\u2019re excited to share that we've expanded our platform to support multiple modalities, including:</p> <ul> <li> <p> Audio files</p> </li> <li> <p> Markdowns</p> </li> <li> <p> Websites</p> </li> <li> <p> Images</p> </li> <li> <p> Videos</p> </li> <li> <p> Graph</p> </li> </ul> <p>This gives you the flexibility to work with various data types all in one place! \ud83c\udf10 </p>"},{"location":"roadmap/roadmap/#product","title":"\ud83d\udc9c Product","text":"<p>We\u2019ve rolled out some major updates in version 0.3 to improve both functionality and performance. Here\u2019s what\u2019s new:</p> <ul> <li> <p>Semantic Chunking: Optimized chunking strategy for better Retrieval-Augmented Generation (RAG) workflows.</p> </li> <li> <p>Streaming for Efficient Indexing: We\u2019ve introduced streaming for memory-efficient indexing in vector databases. Want to know more? Check out our article on this feature here: https://www.analyticsvidhya.com/blog/2024/09/vector-streaming/</p> </li> <li> <p>Zero-Shot Applications: Explore our zero-shot application demos to see the power of these updates in action.</p> </li> <li> <p>Intuitive Functions: Version 0.3 includes a complete refactor for more intuitive functions, making the platform easier to use.</p> </li> <li> <p>Chunkwise Streaming: Instead of file-by-file streaming, we now support chunkwise streaming, allowing for more flexible and efficient data processing.</p> </li> </ul> <p>Check out the latest release :  and see how these features can supercharge your GenerativeAI pipeline! \u2728</p>"},{"location":"roadmap/roadmap/#coming-soon","title":"\ud83d\ude80Coming Soon","text":""},{"location":"roadmap/roadmap/#performance","title":"\u2699\ufe0f Performance","text":"<p>We've received quite a few questions about why we're using Candle, so here's a quick explanation:</p> <p>One of the main reasons is that Candle doesn't require any specific ONNX format models, which means it can work seamlessly with any Hugging Face model. This flexibility has been a key factor for us. However, we also recognize that we\u2019ve been compromising a bit on speed in favor of that flexibility.</p> <p>What\u2019s Next? To address this, we\u2019re excited to announce that we\u2019re introducing Candle-ONNX along with our previous framework on hugging-face ,</p> <p>\u27a1\ufe0f Support for GGUF models  - Significantly faster performance - Stay tuned for these exciting updates! \ud83d\ude80</p>"},{"location":"roadmap/roadmap/#embeddings","title":"\ud83e\uded0Embeddings:","text":"<p>We had multimodality from day one for our infrastructure. We have already included it for websites, images and audios but we want to expand it further to.</p> <p>\u2611\ufe0fGraph embedding -- build deepwalks embeddings depth first and word to vec  \u2611\ufe0fVideo Embedding  \u2611\ufe0f Yolo Clip </p>"},{"location":"roadmap/roadmap/#expansion-to-other-vector-adapters","title":"\ud83c\udf0aExpansion to other Vector Adapters","text":"<p>We currently support a wide range of vector databases for streaming embeddings, including:</p> <ul> <li>Elastic: thanks to amazing and active Elastic team for the contribution </li> <li>Weaviate</li> <li>Pinecone</li> <li>Qdrant</li> <li>Milvus</li> </ul> <p>But we're not stopping there! We're actively working to expand this list.</p> <p>Want to Contribute? If you\u2019d like to add support for your favorite vector database, we\u2019d love to have your help! Check out our contribution.md for guidelines, or feel free to reach out directly starlight-search@proton.me. Let's build something amazing together! \ud83d\udca1</p>"},{"location":"blog/page/2/","title":"\ud83d\udcf0 All Posts","text":""}]}