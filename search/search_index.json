{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"[![Downloads](https://static.pepy.tech/badge/embed-anything)](https://pepy.tech/project/embed-anything) [![gpu](https://static.pepy.tech/badge/embed-anything-gpu)](https://www.pepy.tech/projects/embed-anything-gpu) [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1CowJrqZxDDYJzkclI-rbHaZHgL9C6K3p?usp=sharing) [![roadmap](https://img.shields.io/badge/Discord-%235865F2.svg?style=flat&amp;logo=discord&amp;logoColor=white)](https://discord.gg/juETVTMdZu) [![MkDocs](https://img.shields.io/badge/Blogs-F38020?.svg?logoColor=fff)](https://embed-anything.com/blog/)   <p>  Highly Performant, Modular and Memory Safe  Ingestion, Inference and Indexing in Rust \ud83e\udd80 Python docs \u00bb Rust docs \u00bb Benchmarks     \u00b7     FAQ     \u00b7     Adapters     .     Collaborations     .      Notebooks </p> <p>EmbedAnything is a minimalist, yet highly performant, modular, lightning-fast, lightweight, multisource, multimodal, and local embedding pipeline built in Rust. Whether you're working with text, images, audio, PDFs, websites, or other media, EmbedAnything streamlines the process of generating embeddings from various sources and seamlessly streaming (memory-efficient-indexing) them to a vector database. It supports dense, sparse, ONNX, model2vec and late-interaction embeddings, offering flexibility for a wide range of use cases.</p> Table of Contents <ol> <li> About The Project <ul> <li>Built With Rust</li> <li>Why Candle?</li> </ul> </li> <li> Getting Started <ul> <li>Installation</li> </ul> </li> <li>Usage</li> <li>Roadmap</li> <li>Contributing</li> <li>How to add custom model and chunk size</li> </ol>"},{"location":"#key-features","title":"\ud83d\ude80 Key Features","text":"<ul> <li>No Dependency on Pytorch: Easy to deploy on cloud, comes with low memory footprint.</li> <li>Highly Modular : Choose any vectorDB adapter for RAG, with ~~1 line~~ 1 word of code</li> <li>Candle Backend : Supports BERT, Jina, ColPali, Splade, ModernBERT, Reranker, Qwen</li> <li>ONNX Backend: Supports BERT, Jina, ColPali, ColBERT Splade, Reranker, ModernBERT, Qwen</li> <li>Cloud Embedding Models:: Supports OpenAI, Cohere, and Gemini.</li> <li>MultiModality : Works with text sources like PDFs, txt, md, Images JPG and Audio, .WAV</li> <li>GPU support : Hardware acceleration on GPU as well.</li> <li>Chunking : In-built chunking methods like semantic, late-chunking</li> <li>Vector Streaming: Separate file processing, Indexing and Inferencing on different threads, reduces latency.</li> </ul>"},{"location":"#what-is-vector-streaming","title":"\ud83d\udca1What is Vector Streaming","text":"<p>Embedding models are computationally expensive and time-consuming. By separating document preprocessing from model inference, you can significantly reduce pipeline latency and improve throughput.</p> <p>Vector streaming transforms a sequential bottleneck into an efficient, concurrent workflow.</p> <p>The embedding process happens separetly from the main process, so as to maintain high performance enabled by rust MPSC, and no memory leak as embeddings are directly saved to vector database. Find our blog.</p> <p></p>"},{"location":"#why-embed-anything","title":"\ud83e\udd80 Why Embed Anything","text":"<p>\u27a1\ufe0fFaster execution.  \u27a1\ufe0fNo Pytorch Dependency, thus low-memory footprint and easy to deploy on cloud.  \u27a1\ufe0fTrue multithreading  \u27a1\ufe0fRunning embedding models locally and efficiently  \u27a1\ufe0fIn-built chunking methods like semantic, late-chunking  \u27a1\ufe0fSupports range of models, Dense, Sparse, Late-interaction, ReRanker, ModernBert. \u27a1\ufe0fMemory Management: Rust enforces memory management simultaneously, preventing memory leaks and crashes that can plague other languages </p>"},{"location":"#our-past-collaborations","title":"\ud83c\udf53 Our Past Collaborations:","text":"<p>We have collaborated with reputed enterprise like Elastic, Weaviate, SingleStore, Milvus  and Analytics Vidya Datahours</p> <p>You can get in touch with us for further collaborations.</p>"},{"location":"#benchmarks","title":"Benchmarks","text":""},{"location":"#inference-speed-benchmarks","title":"Inference Speed benchmarks.","text":"<p>Only measures embedding model inference speed, on onnx-runtime. Code</p> <p></p> <p>Benchmarks with other fromeworks coming soon!! \ud83d\ude80</p>"},{"location":"#supported-models","title":"\u2b50 Supported Models","text":"<p>We support any hugging-face models on Candle. And We also support ONNX runtime for BERT and ColPali.</p>"},{"location":"#how-to-add-custom-model-on-candle-from_pretrained_hf","title":"How to add custom model on candle: from_pretrained_hf","text":"<pre><code>model = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Bert, model_id=\"model link from huggingface\"\n)\nconfig = TextEmbedConfig(chunk_size=1000, batch_size=32)\ndata = embed_anything.embed_file(\"file_address\", embedder=model, config=config)\n</code></pre> Model HF link Jina Jina Models Bert All Bert based models CLIP openai/clip-* Whisper OpenAI Whisper models ColPali starlight-ai/colpali-v1.2-merged-onnx Colbert answerdotai/answerai-colbert-small-v1, jinaai/jina-colbert-v2 and more Splade Splade Models and other Splade like models Model2Vec model2vec, minishlab/potion-base-8M Qwen3-Embedding Qwen/Qwen3-Embedding-0.6B Reranker Jina Reranker Models, Xenova/bge-reranker, Qwen/Qwen3-Reranker-4B"},{"location":"#splade-models","title":"Splade Models:","text":"<pre><code>model = EmbeddingModel.from_pretrained_hf(\n    WhichModel.SparseBert, \"prithivida/Splade_PP_en_v1\"\n)\n</code></pre>"},{"location":"#onnx-runtime-from_pretrained_onnx","title":"ONNX-Runtime: from_pretrained_onnx","text":""},{"location":"#bert","title":"BERT","text":"<pre><code>model = EmbeddingModel.from_pretrained_onnx(\n  WhichModel.Bert, model_id=\"onnx_model_link\"\n)\n</code></pre>"},{"location":"#colpali","title":"ColPali","text":"<pre><code>model: ColpaliModel = ColpaliModel.from_pretrained_onnx(\"starlight-ai/colpali-v1.2-merged-onnx\", None)\n</code></pre>"},{"location":"#colbert","title":"Colbert","text":"<pre><code>sentences = [\n\"The quick brown fox jumps over the lazy dog\", \n\"The cat is sleeping on the mat\", \"The dog is barking at the moon\", \n\"I love pizza\", \n\"The dog is sitting in the park\"]\n\nmodel = ColbertModel.from_pretrained_onnx(\"jinaai/jina-colbert-v2\", path_in_repo=\"onnx/model.onnx\")\nembeddings = model.embed(sentences, batch_size=2)\n</code></pre>"},{"location":"#modernbert","title":"ModernBERT","text":"<pre><code>model = EmbeddingModel.from_pretrained_onnx(\n    WhichModel.Bert, ONNXModel.ModernBERTBase, dtype = Dtype.Q4F16\n)\n</code></pre>"},{"location":"#rerankers","title":"ReRankers","text":"<pre><code>reranker = Reranker.from_pretrained(\"jinaai/jina-reranker-v1-turbo-en\", dtype=Dtype.F16)\n\nresults: list[RerankerResult] = reranker.rerank([\"What is the capital of France?\"], [\"France is a country in Europe.\", \"Paris is the capital of France.\"], 2)\n</code></pre>"},{"location":"#embed-4","title":"Embed 4","text":"<pre><code># Initialize the model once\nmodel: EmbeddingModel = EmbeddingModel.from_pretrained_cloud(\n    WhichModel.CohereVision, model_id=\"embed-v4.0\"\n)\n</code></pre>"},{"location":"#qwen-3-embedding","title":"Qwen 3 - Embedding","text":"<pre><code># Initialize the model once\nmodel:EmbeddingModel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Qwen3, model_id=\"Qwen/Qwen3-Embedding-0.6B\"\n)\n</code></pre>"},{"location":"#for-semantic-chunking","title":"For Semantic Chunking","text":"<pre><code>model = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Bert, model_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n)\n\n# with semantic encoder\nsemantic_encoder = EmbeddingModel.from_pretrained_hf(WhichModel.Jina, model_id = \"jinaai/jina-embeddings-v2-small-en\")\nconfig = TextEmbedConfig(chunk_size=1000, batch_size=32, splitting_strategy = \"semantic\", semantic_encoder=semantic_encoder)\n</code></pre>"},{"location":"#for-late-chunking","title":"For late-chunking","text":"<pre><code>config = TextEmbedConfig(\n    chunk_size=1000,\n    batch_size=8,\n    splitting_strategy=\"sentence\",\n    late_chunking=True,\n)\n\n# Embed a single file\ndata: list[EmbedData] = model.embed_file(\"test_files/attention.pdf\", config=config)\n</code></pre>"},{"location":"#getting-started","title":"\ud83e\uddd1\u200d\ud83d\ude80 Getting Started","text":""},{"location":"#installation","title":"\ud83d\udc9a Installation","text":"<p><code>pip install embed-anything</code></p> <p>For GPUs and using special models like ColPali </p> <p><code>pip install embed-anything-gpu</code></p> <p>\ud83d\udea7\u274c If it shows cuda error while running on windowns, run the following command:</p> <pre><code>os.add_dll_directory(\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v12.6/bin\")\n</code></pre>"},{"location":"#notebooks","title":"\ud83d\udcd2 Notebooks","text":"End-to-End Retrieval and Reranking using VectorDB Adapters ColPali-Onnx Adapters Qwen3- Embedings Benchmarks"},{"location":"#usage","title":"Usage","text":""},{"location":"#usage-for-03-and-later-version","title":"\u27a1\ufe0f Usage For 0.3 and later version","text":"<pre><code>model = EmbeddingModel.from_pretrained_local(\n    WhichModel.Bert, model_id=\"Hugging_face_link\"\n)\ndata = embed_anything.embed_file(\"test_files/test.pdf\", embedder=model)\n</code></pre>"},{"location":"#using-onnx-models","title":"Using ONNX Models","text":"<p>To use ONNX models, you can either use the <code>ONNXModel</code> enum or the <code>model_id</code> from the Hugging Face model.</p> <p>Using the above method is best to ensure that the model works correctly as these models are tested. But if you want to use other models, like finetuned models, you can use the <code>hf_model_id</code> and <code>path_in_repo</code> to load the model like below.</p> <p><pre><code>model = EmbeddingModel.from_pretrained_onnx(\n  WhichModel.Jina, hf_model_id = \"jinaai/jina-embeddings-v2-small-en\", path_in_repo=\"model.onnx\"\n)\n</code></pre> To see all the ONNX models supported with model_name, see here</p>"},{"location":"#faq","title":"\u2049\ufe0fFAQ","text":""},{"location":"#do-i-need-to-know-rust-to-use-or-contribute-to-embedanything","title":"Do I need to know rust to use or contribute to embedanything?","text":"<p>The answer is No. EmbedAnything provides you pyo3 bindings, so you can run any function in python without any issues. To contibute you should check out our guidelines and python folder example of adapters.</p>"},{"location":"#how-is-it-different-from-fastembed","title":"How is it different from fastembed?","text":"<p>We provide both backends, candle and onnx. On top of it we also give an end-to-end pipeline, that is you can ingest different data-types and index to any vector database, and inference any model. Fastembed is just an onnx-wrapper.</p>"},{"location":"#weve-received-quite-a-few-questions-about-why-were-using-candle","title":"We've received quite a few questions about why we're using Candle.","text":"<p>One of the main reasons is that Candle doesn't require any specific ONNX format models, which means it can work seamlessly with any Hugging Face model. This flexibility has been a key factor for us. However, we also recognize that we\u2019ve been compromising a bit on speed in favor of that flexibility.</p>"},{"location":"#contributing-to-embedanything","title":"\ud83d\udea7 Contributing to EmbedAnything","text":"<p>First of all, thank you for taking the time to contribute to this project. We truly appreciate your contributions, whether it's bug reports, feature suggestions, or pull requests. Your time and effort are highly valued in this project. \ud83d\ude80</p> <p>This document provides guidelines and best practices to help you to contribute effectively. These are meant to serve as guidelines, not strict rules. We encourage you to use your best judgment and feel comfortable proposing changes to this document through a pull request.</p> <li>Roadmap</li> <li>Quick Start</li> <li>Guidelines</li>"},{"location":"#roadmap","title":"\ud83c\udfce\ufe0f RoadMap","text":""},{"location":"#accomplishments","title":"Accomplishments","text":"<p>One of the aims of EmbedAnything is to allow AI engineers to easily use state of the art embedding models on typical files and documents. A lot has already been accomplished here and these are the formats that we support right now and a few more have to be done. </p>"},{"location":"#modalities-and-source","title":"\ud83d\uddbc\ufe0f Modalities and Source","text":"<p>We\u2019re excited to share that we've expanded our platform to support multiple modalities, including:</p> <ul> <li> <p> Audio files</p> </li> <li> <p> Markdowns</p> </li> <li> <p> Websites</p> </li> <li> <p> Images</p> </li> <li> <p> Videos</p> </li> <li> <p> Graph</p> </li> </ul> <p>This gives you the flexibility to work with various data types all in one place! \ud83c\udf10 </p>"},{"location":"#performance","title":"\u2699\ufe0f Performance","text":"<p>We now support both candle and Onnx backend \u27a1\ufe0f Support for GGUF models </p>"},{"location":"#embeddings","title":"\ud83e\uded0Embeddings:","text":"<p>We had multimodality from day one for our infrastructure. We have already included it for websites, images and audios but we want to expand it further to.</p> <p>\u27a1\ufe0f Graph embedding -- build deepwalks embeddings depth first and word to vec  \u27a1\ufe0f Video Embedding  \u27a1\ufe0f Yolo Clip </p>"},{"location":"#expansion-to-other-vector-adapters","title":"\ud83c\udf0aExpansion to other Vector Adapters","text":"<p>We currently support a wide range of vector databases for streaming embeddings, including:</p> <ul> <li>Elastic: thanks to amazing and active Elastic team for the contribution </li> <li>Weaviate </li> <li>Pinecone </li> <li>Qdrant </li> <li>Milvus</li> <li>Chroma </li> </ul> <p>How to add an adpters: https://starlight-search.com/blog/2024/02/25/adapter-development-guide.md</p>"},{"location":"#create-wasm-demos-to-integrate-embedanything-directly-to-the-browser","title":"\ud83d\udca5 Create WASM demos to integrate embedanything directly to the browser.","text":""},{"location":"#add-support-for-ingestion-from-remote-sources","title":"\ud83d\udc9c Add support for ingestion from remote sources","text":"<p>\u27a1\ufe0f Support for S3 bucket  \u27a1\ufe0f Support for azure storage  \u27a1\ufe0f Support for google drive/dropbox</p> <p>But we're not stopping there! We're actively working to expand this list.</p> <p>Want to Contribute? If you\u2019d like to add support for your favorite vector database, we\u2019d love to have your help! Check out our contribution.md for guidelines, or feel free to reach out directly turingatverge@gmail.com . Let's build something amazing together! \ud83d\udca1</p>"},{"location":"#a-big-thank-you-to-all-our-stargazers","title":"A big Thank you to all our StarGazers","text":""},{"location":"#star-history","title":"Star History","text":""},{"location":"references/","title":"\ud83d\udcda References","text":"<p>This module provides functions and classes for embedding queries, files, and directories using different embedding models.</p> <p>The module includes the following functions:</p> <ul> <li><code>embed_query</code>: Embeds the given query and returns an EmbedData object.</li> <li><code>embed_file</code>: Embeds the file at the given path and returns a list of EmbedData objects.</li> <li><code>embed_directory</code>: Embeds all the files in the given directory and returns a list of EmbedData objects.</li> </ul> <p>The module also includes the <code>EmbedData</code> class, which represents the data of an embedded file.</p>"},{"location":"references/#python.python.embed_anything--usage","title":"Usage:","text":"<pre><code>import embed_anything\nfrom embed_anything import EmbedData\n\n#For text files\n\nmodel = EmbeddingModel.from_pretrained_local(\n    WhichModel.Bert, model_id=\"Hugging_face_link\"\n)\ndata = embed_anything.embed_file(\"test_files/test.pdf\", embedder=model)\n\n\n#For images\nmodel = embed_anything.EmbeddingModel.from_pretrained_local(\n    embed_anything.WhichModel.Clip,\n    model_id=\"openai/clip-vit-base-patch16\",\n    # revision=\"refs/pr/15\",\n)\ndata: list[EmbedData] = embed_anything.embed_directory(\"test_files\", embedder=model)\nembeddings = np.array([data.embedding for data in data])\nquery = [\"Photo of a monkey?\"]\nquery_embedding = np.array(\n    embed_anything.embed_query(query, embedder=model)[0].embedding\n)\n# For audio files\nfrom embed_anything import (\n    AudioDecoderModel,\n    EmbeddingModel,\n    embed_audio_file,\n    TextEmbedConfig,\n)\n# choose any whisper or distilwhisper model from https://huggingface.co/distil-whisper or https://huggingface.co/collections/openai/whisper-release-6501bba2cf999715fd953013\naudio_decoder = AudioDecoderModel.from_pretrained_hf(\n    \"openai/whisper-tiny.en\", revision=\"main\", model_type=\"tiny-en\", quantized=False\n)\nembedder = EmbeddingModel.from_pretrained_hf(\n    embed_anything.WhichModel.Bert,\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    revision=\"main\",\n)\nconfig = TextEmbedConfig(chunk_size=200, batch_size=32)\ndata = embed_anything.embed_audio_file(\n    \"test_files/audio/samples_hp0.wav\",\n    audio_decoder=audio_decoder,\n    embedder=embedder,\n    text_embed_config=config,\n)\n</code></pre> <p>You can also store the embeddings to a vector database and not keep them on memory. Here is an example of how to use the <code>PineconeAdapter</code> class:</p> <pre><code>import embed_anything\nimport os\n\nfrom embed_anything.vectordb import PineconeAdapter\n\n\n# Initialize the PineconeEmbedder class\napi_key = os.environ.get(\"PINECONE_API_KEY\")\nindex_name = \"anything\"\npinecone_adapter = PineconeAdapter(api_key)\n\ntry:\n    pinecone_adapter.delete_index(\"anything\")\nexcept:\n    pass\n\n# Initialize the PineconeEmbedder class\n\npinecone_adapter.create_index(dimension=512, metric=\"cosine\")\n\n# bert_model = EmbeddingModel.from_pretrained_hf(\n#     WhichModel.Bert, \"sentence-transformers/all-MiniLM-L12-v2\", revision=\"main\"\n# )\n\nclip_model = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Clip, \"openai/clip-vit-base-patch16\", revision=\"main\"\n)\n\nembed_config = TextEmbedConfig(chunk_size=512, batch_size=32)\n\n\ndata = embed_anything.embed_image_directory(\n    \"test_files\",\n    embedder=clip_model,\n    adapter=pinecone_adapter,\n    # config=embed_config,\n</code></pre>"},{"location":"references/#python.python.embed_anything--supported-embedding-models","title":"Supported Embedding Models:","text":"<ul> <li> <p>Text Embedding Models:</p> <ul> <li>\"OpenAI\"</li> <li>\"Bert\"</li> <li>\"Jina\"</li> </ul> </li> <li> <p>Image Embedding Models:</p> <ul> <li>\"Clip\"</li> <li>\"SigLip\" (Coming Soon)</li> </ul> </li> <li> <p>Audio Embedding Models:</p> <ul> <li>\"Whisper\"</li> </ul> </li> </ul>"},{"location":"references/#python.python.embed_anything.Adapter","title":"<code>Adapter</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class Adapter(ABC):\n    def __init__(self, api_key: str):\n        \"\"\"\n        Initializes the Adapter object.\n\n        Args:\n            api_key: The API key for accessing the adapter.\n        \"\"\"\n\n    @abstractmethod\n    def create_index(self, dimension: int, metric: str, index_name: str, **kwargs): ...\n    \"\"\"\n    Creates an index for storing the embeddings.\n\n    Args:\n        dimension: The dimension of the embeddings.\n        metric: The metric for measuring the distance between embeddings.\n        index_name: The name of the index.\n        kwargs: Additional keyword arguments.\n    \"\"\"\n    @abstractmethod\n    def delete_index(self, index_name: str):\n        \"\"\"\n        Deletes an index.\n\n        Args:\n            index_name: The name of the index to delete.\n        \"\"\"\n\n    @abstractmethod\n    def convert(self, embeddings: List[List[EmbedData]]) -&gt; List[Dict]:\n        \"\"\"\n        Converts the embeddings to a list of dictionaries.\n\n        Args:\n            embeddings: The list of embeddings.\n\n        Returns:\n            A list of dictionaries.\n        \"\"\"\n\n    @abstractmethod\n    def upsert(self, data: List[Dict]):\n        \"\"\"\n        Upserts the data into the index.\n\n        Args:\n            data: The list of data to upsert.\n        \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.Adapter.__init__","title":"<code>__init__(api_key)</code>","text":"<p>Initializes the Adapter object.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>The API key for accessing the adapter.</p> required Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def __init__(self, api_key: str):\n    \"\"\"\n    Initializes the Adapter object.\n\n    Args:\n        api_key: The API key for accessing the adapter.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.Adapter.convert","title":"<code>convert(embeddings)</code>  <code>abstractmethod</code>","text":"<p>Converts the embeddings to a list of dictionaries.</p> <p>Parameters:</p> Name Type Description Default <code>embeddings</code> <code>List[List[EmbedData]]</code> <p>The list of embeddings.</p> required <p>Returns:</p> Type Description <code>List[Dict]</code> <p>A list of dictionaries.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>@abstractmethod\ndef convert(self, embeddings: List[List[EmbedData]]) -&gt; List[Dict]:\n    \"\"\"\n    Converts the embeddings to a list of dictionaries.\n\n    Args:\n        embeddings: The list of embeddings.\n\n    Returns:\n        A list of dictionaries.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.Adapter.delete_index","title":"<code>delete_index(index_name)</code>  <code>abstractmethod</code>","text":"<p>Deletes an index.</p> <p>Parameters:</p> Name Type Description Default <code>index_name</code> <code>str</code> <p>The name of the index to delete.</p> required Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>@abstractmethod\ndef delete_index(self, index_name: str):\n    \"\"\"\n    Deletes an index.\n\n    Args:\n        index_name: The name of the index to delete.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.Adapter.upsert","title":"<code>upsert(data)</code>  <code>abstractmethod</code>","text":"<p>Upserts the data into the index.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[Dict]</code> <p>The list of data to upsert.</p> required Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>@abstractmethod\ndef upsert(self, data: List[Dict]):\n    \"\"\"\n    Upserts the data into the index.\n\n    Args:\n        data: The list of data to upsert.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.AudioDecoderModel","title":"<code>AudioDecoderModel</code>","text":"<p>Represents an audio decoder model.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>str</code> <p>The ID of the audio decoder model.</p> <code>revision</code> <code>str</code> <p>The revision of the audio decoder model.</p> <code>model_type</code> <code>str</code> <p>The type of the audio decoder model.</p> <code>quantized</code> <code>bool</code> <p>A flag indicating whether the audio decoder model is quantized or not.</p> <p>Example: <pre><code>model = embed_anything.AudioDecoderModel.from_pretrained_hf(\n    model_id=\"openai/whisper-tiny.en\",\n    revision=\"main\",\n    model_type=\"tiny-en\",\n    quantized=False\n)\n</code></pre></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class AudioDecoderModel:\n    \"\"\"\n    Represents an audio decoder model.\n\n    Attributes:\n        model_id: The ID of the audio decoder model.\n        revision: The revision of the audio decoder model.\n        model_type: The type of the audio decoder model.\n        quantized: A flag indicating whether the audio decoder model is quantized or not.\n\n    Example:\n    ```python\n\n    model = embed_anything.AudioDecoderModel.from_pretrained_hf(\n        model_id=\"openai/whisper-tiny.en\",\n        revision=\"main\",\n        model_type=\"tiny-en\",\n        quantized=False\n    )\n    ```\n    \"\"\"\n\n    model_id: str\n    revision: str\n    model_type: str\n    quantized: bool\n\n    def from_pretrained_hf(\n        model_id: str | None = None,\n        revision: str | None = None,\n        model_type: str | None = None,\n        quantized: bool | None = None,\n    ): ...\n</code></pre>"},{"location":"references/#python.python.embed_anything.ColbertModel","title":"<code>ColbertModel</code>","text":"<p>Represents the Colbert model.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class ColbertModel:\n    \"\"\"\n    Represents the Colbert model.\n    \"\"\"\n\n    def __init__(\n        self,\n        hf_model_id: str | None = None,\n        revision: str | None = None,\n        path_in_repo: str | None = None,\n    ):\n        \"\"\"\n        Initializes the ColbertModel object.\n        \"\"\"\n\n    def from_pretrained_onnx(\n        self,\n        hf_model_id: str | None = None,\n        revision: str | None = None,\n        path_in_repo: str | None = None,\n    ) -&gt; ColbertModel:\n        \"\"\"\n        Loads a pre-trained Colbert model from the Hugging Face model hub.\n\n        Attributes:\n            hf_model_id: The ID of the model from Hugging Face.\n            revision: The revision of the model.\n            path_in_repo: The path to the model in the repository.\n\n        Returns:\n            A ColbertModel object.\n        \"\"\"\n\n    def embed(\n        self, text_batch: list[str], batch_size: int | None = None, is_doc: bool = True\n    ) -&gt; list[EmbedData]:\n        \"\"\"\n        Embeds the given text and returns a list of EmbedData objects.\n        \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.ColbertModel.__init__","title":"<code>__init__(hf_model_id=None, revision=None, path_in_repo=None)</code>","text":"<p>Initializes the ColbertModel object.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def __init__(\n    self,\n    hf_model_id: str | None = None,\n    revision: str | None = None,\n    path_in_repo: str | None = None,\n):\n    \"\"\"\n    Initializes the ColbertModel object.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.ColbertModel.embed","title":"<code>embed(text_batch, batch_size=None, is_doc=True)</code>","text":"<p>Embeds the given text and returns a list of EmbedData objects.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed(\n    self, text_batch: list[str], batch_size: int | None = None, is_doc: bool = True\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given text and returns a list of EmbedData objects.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.ColbertModel.from_pretrained_onnx","title":"<code>from_pretrained_onnx(hf_model_id=None, revision=None, path_in_repo=None)</code>","text":"<p>Loads a pre-trained Colbert model from the Hugging Face model hub.</p> <p>Attributes:</p> Name Type Description <code>hf_model_id</code> <p>The ID of the model from Hugging Face.</p> <code>revision</code> <p>The revision of the model.</p> <code>path_in_repo</code> <p>The path to the model in the repository.</p> <p>Returns:</p> Type Description <code>ColbertModel</code> <p>A ColbertModel object.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def from_pretrained_onnx(\n    self,\n    hf_model_id: str | None = None,\n    revision: str | None = None,\n    path_in_repo: str | None = None,\n) -&gt; ColbertModel:\n    \"\"\"\n    Loads a pre-trained Colbert model from the Hugging Face model hub.\n\n    Attributes:\n        hf_model_id: The ID of the model from Hugging Face.\n        revision: The revision of the model.\n        path_in_repo: The path to the model in the repository.\n\n    Returns:\n        A ColbertModel object.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.ColpaliModel","title":"<code>ColpaliModel</code>","text":"<p>Represents the Colpali model.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class ColpaliModel:\n    \"\"\"\n    Represents the Colpali model.\n    \"\"\"\n\n    def __init__(self, model_id: str, revision: str | None = None):\n        \"\"\"\n        Initializes the ColpaliModel object.\n\n        Args:\n            model_id: The ID of the model from Hugging Face.\n            revision: The revision of the model.\n        \"\"\"\n\n    def from_pretrained(model_id: str, revision: str | None = None) -&gt; ColpaliModel:\n        \"\"\"\n        Loads a pre-trained Colpali model from the Hugging Face model hub.\n\n        Args:\n            model_id: The ID of the model from Hugging Face.\n            revision: The revision of the model.\n\n        Returns:\n            A ColpaliModel object.\n        \"\"\"\n\n    def from_pretrained_onnx(\n        model_id: str, revision: str | None = None\n    ) -&gt; ColpaliModel:\n        \"\"\"\n        Loads a pre-trained Colpali model from the Hugging Face model hub.\n\n        Args:\n            model_id: The ID of the model from Hugging Face.\n            revision: The revision of the model.\n\n        Returns:\n            A ColpaliModel object.\n        \"\"\"\n\n    def embed_file(self, file_path: str, batch_size: int | None = 1) -&gt; list[EmbedData]:\n        \"\"\"\n        Embeds the given pdf file and returns a list of EmbedData objects for each page in the file This first convert the pdf file into images and then embed each image.\n\n        Args:\n            file_path: The path to the pdf file to embed.\n            batch_size: The batch size for processing the embeddings. Default is 1.\n\n        Returns:\n            A list of EmbedData objects for each page in the file.\n        \"\"\"\n\n    def embed_query(self, query: str) -&gt; list[EmbedData]:\n        \"\"\"\n        Embeds the given query and returns a list of EmbedData objects.\n\n        Args:\n            query: The query to embed.\n\n        Returns:\n            A list of EmbedData objects.\n\n        \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.ColpaliModel.__init__","title":"<code>__init__(model_id, revision=None)</code>","text":"<p>Initializes the ColpaliModel object.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The ID of the model from Hugging Face.</p> required <code>revision</code> <code>str | None</code> <p>The revision of the model.</p> <code>None</code> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def __init__(self, model_id: str, revision: str | None = None):\n    \"\"\"\n    Initializes the ColpaliModel object.\n\n    Args:\n        model_id: The ID of the model from Hugging Face.\n        revision: The revision of the model.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.ColpaliModel.embed_file","title":"<code>embed_file(file_path, batch_size=1)</code>","text":"<p>Embeds the given pdf file and returns a list of EmbedData objects for each page in the file This first convert the pdf file into images and then embed each image.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the pdf file to embed.</p> required <code>batch_size</code> <code>int | None</code> <p>The batch size for processing the embeddings. Default is 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects for each page in the file.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_file(self, file_path: str, batch_size: int | None = 1) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given pdf file and returns a list of EmbedData objects for each page in the file This first convert the pdf file into images and then embed each image.\n\n    Args:\n        file_path: The path to the pdf file to embed.\n        batch_size: The batch size for processing the embeddings. Default is 1.\n\n    Returns:\n        A list of EmbedData objects for each page in the file.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.ColpaliModel.embed_query","title":"<code>embed_query(query)</code>","text":"<p>Embeds the given query and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to embed.</p> required <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_query(self, query: str) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given query and returns a list of EmbedData objects.\n\n    Args:\n        query: The query to embed.\n\n    Returns:\n        A list of EmbedData objects.\n\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.ColpaliModel.from_pretrained","title":"<code>from_pretrained(model_id, revision=None)</code>","text":"<p>Loads a pre-trained Colpali model from the Hugging Face model hub.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The ID of the model from Hugging Face.</p> required <code>revision</code> <code>str | None</code> <p>The revision of the model.</p> <code>None</code> <p>Returns:</p> Type Description <code>ColpaliModel</code> <p>A ColpaliModel object.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def from_pretrained(model_id: str, revision: str | None = None) -&gt; ColpaliModel:\n    \"\"\"\n    Loads a pre-trained Colpali model from the Hugging Face model hub.\n\n    Args:\n        model_id: The ID of the model from Hugging Face.\n        revision: The revision of the model.\n\n    Returns:\n        A ColpaliModel object.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.ColpaliModel.from_pretrained_onnx","title":"<code>from_pretrained_onnx(model_id, revision=None)</code>","text":"<p>Loads a pre-trained Colpali model from the Hugging Face model hub.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The ID of the model from Hugging Face.</p> required <code>revision</code> <code>str | None</code> <p>The revision of the model.</p> <code>None</code> <p>Returns:</p> Type Description <code>ColpaliModel</code> <p>A ColpaliModel object.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def from_pretrained_onnx(\n    model_id: str, revision: str | None = None\n) -&gt; ColpaliModel:\n    \"\"\"\n    Loads a pre-trained Colpali model from the Hugging Face model hub.\n\n    Args:\n        model_id: The ID of the model from Hugging Face.\n        revision: The revision of the model.\n\n    Returns:\n        A ColpaliModel object.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.DocumentRank","title":"<code>DocumentRank</code>","text":"<p>Represents the rank of a document.</p> <p>Attributes:</p> Name Type Description <code>document</code> <code>str</code> <p>The document to rank.</p> <code>relevance_score</code> <code>float</code> <p>The relevance score of the document.</p> <code>rank</code> <code>int</code> <p>The rank of the document.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class DocumentRank:\n    \"\"\"\n    Represents the rank of a document.\n\n    Attributes:\n        document: The document to rank.\n        relevance_score: The relevance score of the document.\n        rank: The rank of the document.\n    \"\"\"\n\n    document: str\n    relevance_score: float\n    rank: int\n</code></pre>"},{"location":"references/#python.python.embed_anything.Dtype","title":"<code>Dtype</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Represents the data type of the model.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class Dtype(Enum):\n    \"\"\"\n    Represents the data type of the model.\n    \"\"\"\n\n    F16 = \"F16\"\n    INT8 = \"INT8\"\n    Q4 = \"Q4\"\n    UINT8 = \"UINT8\"\n    BNB4 = \"BNB4\"\n    Q4F16 = \"Q4F16\"\n    BF16 = \"BF16\"\n    F32 = \"F32\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbedData","title":"<code>EmbedData</code>","text":"<p>Represents the data of an embedded file.</p> <p>Attributes:</p> Name Type Description <code>embedding</code> <code>list[float]</code> <p>The embedding of the file.</p> <code>text</code> <code>str</code> <p>The text for which the embedding is generated for.</p> <code>metadata</code> <code>dict[str, str]</code> <p>Additional metadata associated with the embedding.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class EmbedData:\n    \"\"\"Represents the data of an embedded file.\n\n    Attributes:\n        embedding: The embedding of the file.\n        text: The text for which the embedding is generated for.\n        metadata: Additional metadata associated with the embedding.\n    \"\"\"\n\n    def __init__(self, embedding: list[float], text: str, metadata: dict[str, str]):\n        self.embedding = embedding\n        self.text = text\n        self.metadata = metadata\n    embedding: list[float]\n    text: str\n    metadata: dict[str, str]\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbeddingModel","title":"<code>EmbeddingModel</code>","text":"<p>Represents an embedding model.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class EmbeddingModel:\n    \"\"\"\n    Represents an embedding model.\n    \"\"\"\n\n    def from_pretrained_hf(\n        model: WhichModel,\n        model_id: str,\n        revision: str | None = None,\n        token: str | None = None,\n        dtype: Dtype | None = None,\n    ) -&gt; EmbeddingModel:\n        \"\"\"\n        Loads an embedding model from the Hugging Face model hub.\n\n        Attributes:\n            model_id: The ID of the model.\n            revision: The revision of the model.\n            token: The Hugging Face token.\n            dtype: The dtype of the model.\n        Returns:\n            An EmbeddingModel object.\n\n        Example:\n        ```python\n        model = EmbeddingModel.from_pretrained_hf(\n            model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n            revision=\"main\"\n        )\n        ```\n\n        \"\"\"\n\n    def from_pretrained_cloud(\n        model: WhichModel, model_id: str, api_key: str | None = None\n    ) -&gt; EmbeddingModel:\n        \"\"\"\n        Loads an embedding model from a cloud-based service.\n\n        Attributes:\n            model (WhichModel): The cloud service to use. Currently supports WhichModel.OpenAI and WhichModel.Cohere.\n            model_id (str): The ID of the model to use.\n\n                - For OpenAI, see available models at https://platform.openai.com/docs/guides/embeddings/embedding-models\n                - For Cohere, see available models at https://docs.cohere.com/docs/cohere-embed\n                - For CohereVision, see available models at https://docs.cohere.com/docs/cohere-embed\n            api_key (str | None, optional): The API key for accessing the model. If not provided, it is taken from the environment variable:\n\n                - For OpenAI: OPENAI_API_KEY\n                - For Cohere: CO_API_KEY\n\n        Returns:\n            EmbeddingModel: An initialized EmbeddingModel object.\n\n        Raises:\n            ValueError: If an unsupported model is specified.\n\n        Example:\n        ```python\n        # Using Cohere\n        model = EmbeddingModel.from_pretrained_cloud(\n            model=WhichModel.Cohere,\n            model_id=\"embed-english-v3.0\"\n        )\n\n        # Using OpenAI\n        model = EmbeddingModel.from_pretrained_cloud(\n            model=WhichModel.OpenAI,\n            model_id=\"text-embedding-3-small\"\n        )\n        ```\n        \"\"\"\n\n    def from_pretrained_onnx(\n        model: WhichModel,\n        model_name: Optional[ONNXModel] | None = None,\n        hf_model_id: Optional[str] | None = None,\n        revision: Optional[str] | None = None,\n        dtype: Optional[Dtype] | None = None,\n        path_in_repo: Optional[str] | None = None,\n    ) -&gt; EmbeddingModel:\n        \"\"\"\n        Loads an ONNX embedding model.\n\n        Args:\n            model (WhichModel): The architecture of the embedding model to use.\n            model_name (ONNXModel | None, optional): The name of the model. Defaults to None.\n            hf_model_id (str | None, optional): The ID of the model from Hugging Face. Defaults to None.\n            revision (str | None, optional): The revision of the model. Defaults to None.\n            dtype (Dtype | None, optional): The dtype of the model. Defaults to None.\n            path_in_repo (str | None, optional): The path to the model in the repository. Defaults to None.\n        Returns:\n            EmbeddingModel: An initialized EmbeddingModel object.\n\n        Atleast one of the following arguments must be provided:\n            - model_name\n            - hf_model_id\n\n        If hf_model_id is provided, dtype is ignored and the path_in_repo has to be provided pointing to the model file in the repository.\n        If model_name is provided, dtype is used to determine the model file to load.\n\n        Example:\n        ```python\n        model = EmbeddingModel.from_pretrained_onnx(\n            model=WhichModel.Bert,\n            model_name=ONNXModel.BGESmallENV15Q,\n            dtype=Dtype.Q4F16\n        )\n\n        model = EmbeddingModel.from_pretrained_onnx(\n            model=WhichModel.Bert,\n            hf_model_id=\"jinaai/jina-embeddings-v3\",\n            path_in_repo=\"onnx/model_fp16.onnx\"\n        )\n        ```\n\n        Note:\n        This method loads a pre-trained model in ONNX format, which can offer improved inference speed\n        compared to standard PyTorch models. ONNX models are particularly useful for deployment\n        scenarios where performance is critical.\n        \"\"\"\n\n    def embed_file(\n        self,\n        file_path: str,\n        config: TextEmbedConfig | None = None,\n        adapter: Adapter | None = None,\n    ) -&gt; list[EmbedData]:\n        \"\"\"\n        Embeds the given file and returns a list of EmbedData objects.\n\n        Args:\n            file_path: The path to the file to embed.\n            config: The configuration for the embedding.\n            adapter: The adapter for the embedding.\n\n        Returns:\n            A list of EmbedData objects.\n        \"\"\"\n\n    def embed_files_batch(\n        self,\n        files: list[str],\n        config: TextEmbedConfig | None = None,\n        adapter: Adapter | None = None,\n    ) -&gt; list[EmbedData]:\n        \"\"\"\n        Embeds the given files and returns a list of EmbedData objects.\n\n        Args:\n            files: The list of files to embed.\n            config: The configuration for the embedding.\n            adapter: The adapter for the embedding.\n\n        Returns:\n            A list of EmbedData objects.\n        \"\"\"\n\n    def embed_audio_file(\n        self,\n        audio_file: str,\n        audio_decoder: AudioDecoderModel,\n        config: TextEmbedConfig | None = None,\n    ) -&gt; list[EmbedData]:\n        \"\"\"\n        Embeds the given audio file and returns a list of EmbedData objects.\n\n        Args:\n            audio_file: The path to the audio file to embed.\n            audio_decoder: The audio decoder for the audio file.\n            config: The configuration for the embedding.\n\n        Returns:\n            A list of EmbedData objects.\n        \"\"\"\n\n    def embed_query(\n        self,\n        query: list[str],\n        config: TextEmbedConfig | None = None,\n    ) -&gt; list[EmbedData]:\n        \"\"\"\n        Embeds the given list of queries and returns a list of EmbedData objects.\n\n        Args:\n            query: The list of queries to embed.\n            config: The configuration for the embedding.\n\n        Returns:\n            A list of EmbedData objects.\n        \"\"\"\n\n    def embed_webpage(\n        self,\n        url: str,\n        config: TextEmbedConfig | None = None,\n        adapter: Adapter | None = None,\n    ) -&gt; list[EmbedData]:\n        \"\"\"\n        Embeds the given webpage and returns a list of EmbedData objects.\n\n        Args:\n            url: The URL of the webpage to embed.\n            config: The configuration for the embedding.\n            adapter: The adapter for the embedding.\n\n        Returns:\n            A list of EmbedData objects.\n        \"\"\"\n\n    def embed_directory(\n        self,\n        directory: str,\n        config: TextEmbedConfig | None = None,\n        adapter: Adapter | None = None,\n    ) -&gt; list[EmbedData]:\n        \"\"\"\n        Embeds the given directory and returns a list of EmbedData objects.\n\n        Args:\n            directory: The path to the directory to embed.\n            config: The configuration for the embedding.\n            adapter: The adapter for the embedding.\n\n        Returns:\n            A list of EmbedData objects.\n        \"\"\"\n\n    def embed_directory_stream(\n        self,\n        directory: str,\n        config: TextEmbedConfig | None = None,\n        adapter: Adapter | None = None,\n    ) -&gt; list[EmbedData]:\n        \"\"\"\n        Embeds the given directory and returns a list of EmbedData objects.\n\n        Args:\n            directory: The path to the directory to embed.\n            config: The configuration for the embedding.\n            adapter: The adapter for the embedding.\n\n        Returns:\n            A list of EmbedData objects.\n        \"\"\"\n\n    def embed_webpage(\n        self,\n        url: str,\n        config: TextEmbedConfig | None = None,\n        adapter: Adapter | None = None,\n    ) -&gt; list[EmbedData]:\n        \"\"\"\n        Embeds the given webpage and returns a list of EmbedData objects.\n\n        Args:\n            url: The URL of the webpage to embed.\n            config: The configuration for the embedding.\n            adapter: The adapter for the embedding.\n\n        Returns:\n            A list of EmbedData objects.\n        \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbeddingModel.embed_audio_file","title":"<code>embed_audio_file(audio_file, audio_decoder, config=None)</code>","text":"<p>Embeds the given audio file and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>audio_file</code> <code>str</code> <p>The path to the audio file to embed.</p> required <code>audio_decoder</code> <code>AudioDecoderModel</code> <p>The audio decoder for the audio file.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_audio_file(\n    self,\n    audio_file: str,\n    audio_decoder: AudioDecoderModel,\n    config: TextEmbedConfig | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given audio file and returns a list of EmbedData objects.\n\n    Args:\n        audio_file: The path to the audio file to embed.\n        audio_decoder: The audio decoder for the audio file.\n        config: The configuration for the embedding.\n\n    Returns:\n        A list of EmbedData objects.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbeddingModel.embed_directory","title":"<code>embed_directory(directory, config=None, adapter=None)</code>","text":"<p>Embeds the given directory and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>The path to the directory to embed.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding.</p> <code>None</code> <code>adapter</code> <code>Adapter | None</code> <p>The adapter for the embedding.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_directory(\n    self,\n    directory: str,\n    config: TextEmbedConfig | None = None,\n    adapter: Adapter | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given directory and returns a list of EmbedData objects.\n\n    Args:\n        directory: The path to the directory to embed.\n        config: The configuration for the embedding.\n        adapter: The adapter for the embedding.\n\n    Returns:\n        A list of EmbedData objects.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbeddingModel.embed_directory_stream","title":"<code>embed_directory_stream(directory, config=None, adapter=None)</code>","text":"<p>Embeds the given directory and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>The path to the directory to embed.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding.</p> <code>None</code> <code>adapter</code> <code>Adapter | None</code> <p>The adapter for the embedding.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_directory_stream(\n    self,\n    directory: str,\n    config: TextEmbedConfig | None = None,\n    adapter: Adapter | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given directory and returns a list of EmbedData objects.\n\n    Args:\n        directory: The path to the directory to embed.\n        config: The configuration for the embedding.\n        adapter: The adapter for the embedding.\n\n    Returns:\n        A list of EmbedData objects.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbeddingModel.embed_file","title":"<code>embed_file(file_path, config=None, adapter=None)</code>","text":"<p>Embeds the given file and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the file to embed.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding.</p> <code>None</code> <code>adapter</code> <code>Adapter | None</code> <p>The adapter for the embedding.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_file(\n    self,\n    file_path: str,\n    config: TextEmbedConfig | None = None,\n    adapter: Adapter | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given file and returns a list of EmbedData objects.\n\n    Args:\n        file_path: The path to the file to embed.\n        config: The configuration for the embedding.\n        adapter: The adapter for the embedding.\n\n    Returns:\n        A list of EmbedData objects.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbeddingModel.embed_files_batch","title":"<code>embed_files_batch(files, config=None, adapter=None)</code>","text":"<p>Embeds the given files and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list[str]</code> <p>The list of files to embed.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding.</p> <code>None</code> <code>adapter</code> <code>Adapter | None</code> <p>The adapter for the embedding.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_files_batch(\n    self,\n    files: list[str],\n    config: TextEmbedConfig | None = None,\n    adapter: Adapter | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given files and returns a list of EmbedData objects.\n\n    Args:\n        files: The list of files to embed.\n        config: The configuration for the embedding.\n        adapter: The adapter for the embedding.\n\n    Returns:\n        A list of EmbedData objects.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbeddingModel.embed_query","title":"<code>embed_query(query, config=None)</code>","text":"<p>Embeds the given list of queries and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>list[str]</code> <p>The list of queries to embed.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_query(\n    self,\n    query: list[str],\n    config: TextEmbedConfig | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given list of queries and returns a list of EmbedData objects.\n\n    Args:\n        query: The list of queries to embed.\n        config: The configuration for the embedding.\n\n    Returns:\n        A list of EmbedData objects.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbeddingModel.embed_webpage","title":"<code>embed_webpage(url, config=None, adapter=None)</code>","text":"<p>Embeds the given webpage and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the webpage to embed.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding.</p> <code>None</code> <code>adapter</code> <code>Adapter | None</code> <p>The adapter for the embedding.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_webpage(\n    self,\n    url: str,\n    config: TextEmbedConfig | None = None,\n    adapter: Adapter | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given webpage and returns a list of EmbedData objects.\n\n    Args:\n        url: The URL of the webpage to embed.\n        config: The configuration for the embedding.\n        adapter: The adapter for the embedding.\n\n    Returns:\n        A list of EmbedData objects.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbeddingModel.from_pretrained_cloud","title":"<code>from_pretrained_cloud(model, model_id, api_key=None)</code>","text":"<p>Loads an embedding model from a cloud-based service.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>WhichModel</code> <p>The cloud service to use. Currently supports WhichModel.OpenAI and WhichModel.Cohere.</p> <code>model_id</code> <code>str</code> <p>The ID of the model to use.</p> <ul> <li>For OpenAI, see available models at https://platform.openai.com/docs/guides/embeddings/embedding-models</li> <li>For Cohere, see available models at https://docs.cohere.com/docs/cohere-embed</li> <li>For CohereVision, see available models at https://docs.cohere.com/docs/cohere-embed</li> </ul> <code>api_key</code> <code>str | None</code> <p>The API key for accessing the model. If not provided, it is taken from the environment variable:</p> <ul> <li>For OpenAI: OPENAI_API_KEY</li> <li>For Cohere: CO_API_KEY</li> </ul> <p>Returns:</p> Name Type Description <code>EmbeddingModel</code> <code>EmbeddingModel</code> <p>An initialized EmbeddingModel object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported model is specified.</p> <p>Example: <pre><code># Using Cohere\nmodel = EmbeddingModel.from_pretrained_cloud(\n    model=WhichModel.Cohere,\n    model_id=\"embed-english-v3.0\"\n)\n\n# Using OpenAI\nmodel = EmbeddingModel.from_pretrained_cloud(\n    model=WhichModel.OpenAI,\n    model_id=\"text-embedding-3-small\"\n)\n</code></pre></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def from_pretrained_cloud(\n    model: WhichModel, model_id: str, api_key: str | None = None\n) -&gt; EmbeddingModel:\n    \"\"\"\n    Loads an embedding model from a cloud-based service.\n\n    Attributes:\n        model (WhichModel): The cloud service to use. Currently supports WhichModel.OpenAI and WhichModel.Cohere.\n        model_id (str): The ID of the model to use.\n\n            - For OpenAI, see available models at https://platform.openai.com/docs/guides/embeddings/embedding-models\n            - For Cohere, see available models at https://docs.cohere.com/docs/cohere-embed\n            - For CohereVision, see available models at https://docs.cohere.com/docs/cohere-embed\n        api_key (str | None, optional): The API key for accessing the model. If not provided, it is taken from the environment variable:\n\n            - For OpenAI: OPENAI_API_KEY\n            - For Cohere: CO_API_KEY\n\n    Returns:\n        EmbeddingModel: An initialized EmbeddingModel object.\n\n    Raises:\n        ValueError: If an unsupported model is specified.\n\n    Example:\n    ```python\n    # Using Cohere\n    model = EmbeddingModel.from_pretrained_cloud(\n        model=WhichModel.Cohere,\n        model_id=\"embed-english-v3.0\"\n    )\n\n    # Using OpenAI\n    model = EmbeddingModel.from_pretrained_cloud(\n        model=WhichModel.OpenAI,\n        model_id=\"text-embedding-3-small\"\n    )\n    ```\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbeddingModel.from_pretrained_hf","title":"<code>from_pretrained_hf(model, model_id, revision=None, token=None, dtype=None)</code>","text":"<p>Loads an embedding model from the Hugging Face model hub.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <p>The ID of the model.</p> <code>revision</code> <p>The revision of the model.</p> <code>token</code> <p>The Hugging Face token.</p> <code>dtype</code> <p>The dtype of the model.</p> <p>Returns:     An EmbeddingModel object.</p> <p>Example: <pre><code>model = EmbeddingModel.from_pretrained_hf(\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    revision=\"main\"\n)\n</code></pre></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def from_pretrained_hf(\n    model: WhichModel,\n    model_id: str,\n    revision: str | None = None,\n    token: str | None = None,\n    dtype: Dtype | None = None,\n) -&gt; EmbeddingModel:\n    \"\"\"\n    Loads an embedding model from the Hugging Face model hub.\n\n    Attributes:\n        model_id: The ID of the model.\n        revision: The revision of the model.\n        token: The Hugging Face token.\n        dtype: The dtype of the model.\n    Returns:\n        An EmbeddingModel object.\n\n    Example:\n    ```python\n    model = EmbeddingModel.from_pretrained_hf(\n        model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n        revision=\"main\"\n    )\n    ```\n\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbeddingModel.from_pretrained_onnx","title":"<code>from_pretrained_onnx(model, model_name=None, hf_model_id=None, revision=None, dtype=None, path_in_repo=None)</code>","text":"<p>Loads an ONNX embedding model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>WhichModel</code> <p>The architecture of the embedding model to use.</p> required <code>model_name</code> <code>ONNXModel | None</code> <p>The name of the model. Defaults to None.</p> <code>None</code> <code>hf_model_id</code> <code>str | None</code> <p>The ID of the model from Hugging Face. Defaults to None.</p> <code>None</code> <code>revision</code> <code>str | None</code> <p>The revision of the model. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>Dtype | None</code> <p>The dtype of the model. Defaults to None.</p> <code>None</code> <code>path_in_repo</code> <code>str | None</code> <p>The path to the model in the repository. Defaults to None.</p> <code>None</code> <p>Returns:     EmbeddingModel: An initialized EmbeddingModel object.</p> Atleast one of the following arguments must be provided <ul> <li>model_name</li> <li>hf_model_id</li> </ul> <p>If hf_model_id is provided, dtype is ignored and the path_in_repo has to be provided pointing to the model file in the repository. If model_name is provided, dtype is used to determine the model file to load.</p> <p>Example: <pre><code>model = EmbeddingModel.from_pretrained_onnx(\n    model=WhichModel.Bert,\n    model_name=ONNXModel.BGESmallENV15Q,\n    dtype=Dtype.Q4F16\n)\n\nmodel = EmbeddingModel.from_pretrained_onnx(\n    model=WhichModel.Bert,\n    hf_model_id=\"jinaai/jina-embeddings-v3\",\n    path_in_repo=\"onnx/model_fp16.onnx\"\n)\n</code></pre></p> <p>Note: This method loads a pre-trained model in ONNX format, which can offer improved inference speed compared to standard PyTorch models. ONNX models are particularly useful for deployment scenarios where performance is critical.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def from_pretrained_onnx(\n    model: WhichModel,\n    model_name: Optional[ONNXModel] | None = None,\n    hf_model_id: Optional[str] | None = None,\n    revision: Optional[str] | None = None,\n    dtype: Optional[Dtype] | None = None,\n    path_in_repo: Optional[str] | None = None,\n) -&gt; EmbeddingModel:\n    \"\"\"\n    Loads an ONNX embedding model.\n\n    Args:\n        model (WhichModel): The architecture of the embedding model to use.\n        model_name (ONNXModel | None, optional): The name of the model. Defaults to None.\n        hf_model_id (str | None, optional): The ID of the model from Hugging Face. Defaults to None.\n        revision (str | None, optional): The revision of the model. Defaults to None.\n        dtype (Dtype | None, optional): The dtype of the model. Defaults to None.\n        path_in_repo (str | None, optional): The path to the model in the repository. Defaults to None.\n    Returns:\n        EmbeddingModel: An initialized EmbeddingModel object.\n\n    Atleast one of the following arguments must be provided:\n        - model_name\n        - hf_model_id\n\n    If hf_model_id is provided, dtype is ignored and the path_in_repo has to be provided pointing to the model file in the repository.\n    If model_name is provided, dtype is used to determine the model file to load.\n\n    Example:\n    ```python\n    model = EmbeddingModel.from_pretrained_onnx(\n        model=WhichModel.Bert,\n        model_name=ONNXModel.BGESmallENV15Q,\n        dtype=Dtype.Q4F16\n    )\n\n    model = EmbeddingModel.from_pretrained_onnx(\n        model=WhichModel.Bert,\n        hf_model_id=\"jinaai/jina-embeddings-v3\",\n        path_in_repo=\"onnx/model_fp16.onnx\"\n    )\n    ```\n\n    Note:\n    This method loads a pre-trained model in ONNX format, which can offer improved inference speed\n    compared to standard PyTorch models. ONNX models are particularly useful for deployment\n    scenarios where performance is critical.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.ImageEmbedConfig","title":"<code>ImageEmbedConfig</code>","text":"<p>Represents the configuration for the Image Embedding model.</p> <p>Attributes:</p> Name Type Description <code>buffer_size</code> <code>int | None</code> <p>The buffer size for the Image Embedding model. Default is 100.</p> <code>batch_size</code> <code>int | None</code> <p>The batch size for processing the embeddings. Default is 32. Based on the memory, you can increase or decrease the batch size.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class ImageEmbedConfig:\n    \"\"\"\n    Represents the configuration for the Image Embedding model.\n\n    Attributes:\n        buffer_size: The buffer size for the Image Embedding model. Default is 100.\n        batch_size: The batch size for processing the embeddings. Default is 32. Based on the memory, you can increase or decrease the batch size.\n    \"\"\"\n\n    def __init__(self, buffer_size: int | None = None, batch_size: int | None = None):\n        self.buffer_size = buffer_size\n        self.batch_size = batch_size\n    buffer_size: int | None\n    batch_size: int | None\n</code></pre>"},{"location":"references/#python.python.embed_anything.ONNXModel","title":"<code>ONNXModel</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum representing various ONNX models.</p> <pre><code>| Enum Variant                     | Description                                      |\n|----------------------------------|--------------------------------------------------|\n| `AllMiniLML6V2`                  | sentence-transformers/all-MiniLM-L6-v2           |\n| `AllMiniLML6V2Q`                 | Quantized sentence-transformers/all-MiniLM-L6-v2 |\n| `AllMiniLML12V2`                 | sentence-transformers/all-MiniLM-L12-v2          |\n| `AllMiniLML12V2Q`                | Quantized sentence-transformers/all-MiniLM-L12-v2|\n| `ModernBERTBase`                 | nomic-ai/modernbert-embed-base                   |\n| `ModernBERTLarge`                | nomic-ai/modernbert-embed-large                  |\n| `BGEBaseENV15`                   | BAAI/bge-base-en-v1.5                            |\n| `BGEBaseENV15Q`                  | Quantized BAAI/bge-base-en-v1.5                  |\n| `BGELargeENV15`                  | BAAI/bge-large-en-v1.5                           |\n| `BGELargeENV15Q`                 | Quantized BAAI/bge-large-en-v1.5                 |\n| `BGESmallENV15`                  | BAAI/bge-small-en-v1.5 - Default                 |\n| `BGESmallENV15Q`                 | Quantized BAAI/bge-small-en-v1.5                 |\n| `NomicEmbedTextV1`               | nomic-ai/nomic-embed-text-v1                     |\n| `NomicEmbedTextV15`              | nomic-ai/nomic-embed-text-v1.5                   |\n| `NomicEmbedTextV15Q`             | Quantized nomic-ai/nomic-embed-text-v1.5         |\n| `ParaphraseMLMiniLML12V2`        | sentence-transformers/paraphrase-MiniLM-L6-v2    |\n| `ParaphraseMLMiniLML12V2Q`       | Quantized sentence-transformers/paraphrase-MiniLM-L6-v2 |\n| `ParaphraseMLMpnetBaseV2`        | sentence-transformers/paraphrase-mpnet-base-v2   |\n| `BGESmallZHV15`                  | BAAI/bge-small-zh-v1.5                           |\n| `MultilingualE5Small`            | intfloat/multilingual-e5-small                   |\n| `MultilingualE5Base`             | intfloat/multilingual-e5-base                    |\n| `MultilingualE5Large`            | intfloat/multilingual-e5-large                   |\n| `MxbaiEmbedLargeV1`              | mixedbread-ai/mxbai-embed-large-v1               |\n| `MxbaiEmbedLargeV1Q`             | Quantized mixedbread-ai/mxbai-embed-large-v1     |\n| `GTEBaseENV15`                   | Alibaba-NLP/gte-base-en-v1.5                     |\n| `GTEBaseENV15Q`                  | Quantized Alibaba-NLP/gte-base-en-v1.5           |\n| `GTELargeENV15`                  | Alibaba-NLP/gte-large-en-v1.5                    |\n| `GTELargeENV15Q`                 | Quantized Alibaba-NLP/gte-large-en-v1.5          |\n| `JINAV2SMALLEN`                  | jinaai/jina-embeddings-v2-small-en               |\n| `JINAV2BASEEN`                   | jinaai/jina-embeddings-v2-base-en                |\n| `JINAV3`                         | jinaai/jina-embeddings-v3                        |\n| `SPLADEPPENV1`                   | prithivida/Splade_PP_en_v1                      |\n| `SPLADEPPENV2`                   | prithivida/Splade_PP_en_v2                      |\n| `ModernBERTBase`                 | nomic-ai/modernbert-embed-base                   |\n</code></pre> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class ONNXModel(Enum):\n    \"\"\"\n    Enum representing various ONNX models.\n\n    ```markdown\n    | Enum Variant                     | Description                                      |\n    |----------------------------------|--------------------------------------------------|\n    | `AllMiniLML6V2`                  | sentence-transformers/all-MiniLM-L6-v2           |\n    | `AllMiniLML6V2Q`                 | Quantized sentence-transformers/all-MiniLM-L6-v2 |\n    | `AllMiniLML12V2`                 | sentence-transformers/all-MiniLM-L12-v2          |\n    | `AllMiniLML12V2Q`                | Quantized sentence-transformers/all-MiniLM-L12-v2|\n    | `ModernBERTBase`                 | nomic-ai/modernbert-embed-base                   |\n    | `ModernBERTLarge`                | nomic-ai/modernbert-embed-large                  |\n    | `BGEBaseENV15`                   | BAAI/bge-base-en-v1.5                            |\n    | `BGEBaseENV15Q`                  | Quantized BAAI/bge-base-en-v1.5                  |\n    | `BGELargeENV15`                  | BAAI/bge-large-en-v1.5                           |\n    | `BGELargeENV15Q`                 | Quantized BAAI/bge-large-en-v1.5                 |\n    | `BGESmallENV15`                  | BAAI/bge-small-en-v1.5 - Default                 |\n    | `BGESmallENV15Q`                 | Quantized BAAI/bge-small-en-v1.5                 |\n    | `NomicEmbedTextV1`               | nomic-ai/nomic-embed-text-v1                     |\n    | `NomicEmbedTextV15`              | nomic-ai/nomic-embed-text-v1.5                   |\n    | `NomicEmbedTextV15Q`             | Quantized nomic-ai/nomic-embed-text-v1.5         |\n    | `ParaphraseMLMiniLML12V2`        | sentence-transformers/paraphrase-MiniLM-L6-v2    |\n    | `ParaphraseMLMiniLML12V2Q`       | Quantized sentence-transformers/paraphrase-MiniLM-L6-v2 |\n    | `ParaphraseMLMpnetBaseV2`        | sentence-transformers/paraphrase-mpnet-base-v2   |\n    | `BGESmallZHV15`                  | BAAI/bge-small-zh-v1.5                           |\n    | `MultilingualE5Small`            | intfloat/multilingual-e5-small                   |\n    | `MultilingualE5Base`             | intfloat/multilingual-e5-base                    |\n    | `MultilingualE5Large`            | intfloat/multilingual-e5-large                   |\n    | `MxbaiEmbedLargeV1`              | mixedbread-ai/mxbai-embed-large-v1               |\n    | `MxbaiEmbedLargeV1Q`             | Quantized mixedbread-ai/mxbai-embed-large-v1     |\n    | `GTEBaseENV15`                   | Alibaba-NLP/gte-base-en-v1.5                     |\n    | `GTEBaseENV15Q`                  | Quantized Alibaba-NLP/gte-base-en-v1.5           |\n    | `GTELargeENV15`                  | Alibaba-NLP/gte-large-en-v1.5                    |\n    | `GTELargeENV15Q`                 | Quantized Alibaba-NLP/gte-large-en-v1.5          |\n    | `JINAV2SMALLEN`                  | jinaai/jina-embeddings-v2-small-en               |\n    | `JINAV2BASEEN`                   | jinaai/jina-embeddings-v2-base-en                |\n    | `JINAV3`                         | jinaai/jina-embeddings-v3                        |\n    | `SPLADEPPENV1`                   | prithivida/Splade_PP_en_v1                      |\n    | `SPLADEPPENV2`                   | prithivida/Splade_PP_en_v2                      |\n    | `ModernBERTBase`                 | nomic-ai/modernbert-embed-base                   |\n    ```\n    \"\"\"\n\n    AllMiniLML6V2 = \"AllMiniLML6V2\"\n\n    AllMiniLML6V2Q = \"AllMiniLML6V2Q\"\n\n    AllMiniLML12V2 = \"AllMiniLML12V2\"\n\n    AllMiniLML12V2Q = \"AllMiniLML12V2Q\"\n\n    ModernBERTBase = \"ModernBERTBase\"\n\n    ModernBERTLarge = \"ModernBERTLarge\"\n\n    BGEBaseENV15 = \"BGEBaseENV15\"\n\n    BGEBaseENV15Q = \"BGEBaseENV15Q\"\n\n    BGELargeENV15 = \"BGELargeENV15\"\n\n    BGELargeENV15Q = \"BGELargeENV15Q\"\n\n    BGESmallENV15 = \"BGESmallENV15\"\n\n    BGESmallENV15Q = \"BGESmallENV15Q\"\n\n    NomicEmbedTextV1 = \"NomicEmbedTextV1\"\n\n    NomicEmbedTextV15 = \"NomicEmbedTextV15\"\n\n    NomicEmbedTextV15Q = \"NomicEmbedTextV15Q\"\n\n    ParaphraseMLMiniLML12V2 = \"ParaphraseMLMiniLML12V2\"\n\n    ParaphraseMLMiniLML12V2Q = \"ParaphraseMLMiniLML12V2Q\"\n\n    ParaphraseMLMpnetBaseV2 = \"ParaphraseMLMpnetBaseV2\"\n\n    BGESmallZHV15 = \"BGESmallZHV15\"\n\n    MultilingualE5Small = \"MultilingualE5Small\"\n\n    MultilingualE5Base = \"MultilingualE5Base\"\n\n    MultilingualE5Large = \"MultilingualE5Large\"\n\n    MxbaiEmbedLargeV1 = \"MxbaiEmbedLargeV1\"\n\n    MxbaiEmbedLargeV1Q = \"MxbaiEmbedLargeV1Q\"\n\n    GTEBaseENV15 = \"GTEBaseENV15\"\n\n    GTEBaseENV15Q = \"GTEBaseENV15Q\"\n\n    GTELargeENV15 = \"GTELargeENV15\"\n\n    GTELargeENV15Q = \"GTELargeENV15Q\"\n\n    JINAV2SMALLEN = \"JINAV2SMALLEN\"\n\n    JINAV2BASEEN = \"JINAV2BASEEN\"\n\n    JINAV3 = \"JINAV3\"\n\n    SPLADEPPENV1 = \"SPLADEPPENV1\"\n\n    SPLADEPPENV2 = \"SPLADEPPENV2\"\n\n    ModernBERTBase = \"ModernBERTBase\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.Reranker","title":"<code>Reranker</code>","text":"<p>Represents the Reranker model.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class Reranker:\n    \"\"\"\n    Represents the Reranker model.\n    \"\"\"\n\n    def __init__(\n        self, model_id: str, revision: str | None = None, dtype: Dtype | None = None, path_in_repo: str | None = None\n    ):\n        \"\"\"\n        Initializes the Reranker object.\n        \"\"\"\n\n    def from_pretrained(\n        model_id: str, revision: str | None = None, dtype: Dtype | None = None, path_in_repo: str | None = None\n    ) -&gt; Reranker:\n        \"\"\"\n        Loads a pre-trained Reranker model from the Hugging Face model hub.\n\n        Args:\n            model_id: The ID of the model from Hugging Face.\n            revision: The revision of the model.\n            dtype: The dtype of the model.\n            path_in_repo: The path to the model in the repository.\n\n        \"\"\"\n\n    def rerank(\n        self, query: list[str], documents: list[str], top_k: int\n    ) -&gt; RerankerResult:\n        \"\"\"\n        Reranks the given documents for the query and returns a list of RerankerResult objects.\n\n        Args:\n            query: The query to rerank.\n            documents: The list of documents to rerank.\n            top_k: The number of documents to return.\n\n        Returns:\n            A list of RerankerResult objects.\n        \"\"\"\n\n    def compute_scores(\n        self, query: list[str], documents: list[str], batch_size: int\n    ) -&gt; list[list[float]]:\n        \"\"\"\n        Computes the scores for the given query and documents.\n\n        Args:\n            query: The query to compute the scores for.\n            documents: The list of documents to compute the scores for.\n            batch_size: The batch size for processing the scores.\n\n        Returns:\n            A list of scores for the given query and documents.\n        \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.Reranker.__init__","title":"<code>__init__(model_id, revision=None, dtype=None, path_in_repo=None)</code>","text":"<p>Initializes the Reranker object.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def __init__(\n    self, model_id: str, revision: str | None = None, dtype: Dtype | None = None, path_in_repo: str | None = None\n):\n    \"\"\"\n    Initializes the Reranker object.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.Reranker.compute_scores","title":"<code>compute_scores(query, documents, batch_size)</code>","text":"<p>Computes the scores for the given query and documents.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>list[str]</code> <p>The query to compute the scores for.</p> required <code>documents</code> <code>list[str]</code> <p>The list of documents to compute the scores for.</p> required <code>batch_size</code> <code>int</code> <p>The batch size for processing the scores.</p> required <p>Returns:</p> Type Description <code>list[list[float]]</code> <p>A list of scores for the given query and documents.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def compute_scores(\n    self, query: list[str], documents: list[str], batch_size: int\n) -&gt; list[list[float]]:\n    \"\"\"\n    Computes the scores for the given query and documents.\n\n    Args:\n        query: The query to compute the scores for.\n        documents: The list of documents to compute the scores for.\n        batch_size: The batch size for processing the scores.\n\n    Returns:\n        A list of scores for the given query and documents.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.Reranker.from_pretrained","title":"<code>from_pretrained(model_id, revision=None, dtype=None, path_in_repo=None)</code>","text":"<p>Loads a pre-trained Reranker model from the Hugging Face model hub.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The ID of the model from Hugging Face.</p> required <code>revision</code> <code>str | None</code> <p>The revision of the model.</p> <code>None</code> <code>dtype</code> <code>Dtype | None</code> <p>The dtype of the model.</p> <code>None</code> <code>path_in_repo</code> <code>str | None</code> <p>The path to the model in the repository.</p> <code>None</code> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def from_pretrained(\n    model_id: str, revision: str | None = None, dtype: Dtype | None = None, path_in_repo: str | None = None\n) -&gt; Reranker:\n    \"\"\"\n    Loads a pre-trained Reranker model from the Hugging Face model hub.\n\n    Args:\n        model_id: The ID of the model from Hugging Face.\n        revision: The revision of the model.\n        dtype: The dtype of the model.\n        path_in_repo: The path to the model in the repository.\n\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.Reranker.rerank","title":"<code>rerank(query, documents, top_k)</code>","text":"<p>Reranks the given documents for the query and returns a list of RerankerResult objects.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>list[str]</code> <p>The query to rerank.</p> required <code>documents</code> <code>list[str]</code> <p>The list of documents to rerank.</p> required <code>top_k</code> <code>int</code> <p>The number of documents to return.</p> required <p>Returns:</p> Type Description <code>RerankerResult</code> <p>A list of RerankerResult objects.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def rerank(\n    self, query: list[str], documents: list[str], top_k: int\n) -&gt; RerankerResult:\n    \"\"\"\n    Reranks the given documents for the query and returns a list of RerankerResult objects.\n\n    Args:\n        query: The query to rerank.\n        documents: The list of documents to rerank.\n        top_k: The number of documents to return.\n\n    Returns:\n        A list of RerankerResult objects.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.RerankerResult","title":"<code>RerankerResult</code>","text":"<p>Represents the result of the reranking process.</p> <p>Attributes:</p> Name Type Description <code>query</code> <code>str</code> <p>The query to rerank.</p> <code>documents</code> <code>list[DocumentRank]</code> <p>The list of documents to rerank.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class RerankerResult:\n    \"\"\"\n    Represents the result of the reranking process.\n\n    Attributes:\n        query: The query to rerank.\n        documents: The list of documents to rerank.\n    \"\"\"\n\n    query: str\n    documents: list[DocumentRank]\n</code></pre>"},{"location":"references/#python.python.embed_anything.TextEmbedConfig","title":"<code>TextEmbedConfig</code>","text":"<p>Represents the configuration for the Text Embedding model.</p> <p>Attributes:</p> Name Type Description <code>chunk_size</code> <code>int | None</code> <p>The chunk size for the Text Embedding model. Default is 1000 Characters.</p> <code>batch_size</code> <code>int | None</code> <p>The batch size for processing the embeddings. Default is 32. Based on the memory, you can increase or decrease the batch size.</p> <code>buffer_size</code> <code>int | None</code> <p>The buffer size for the Text Embedding model. Default is 100.</p> <code>late_chunking</code> <code>bool | None</code> <p>A flag indicating whether to use late chunking for the Text Embedding model. Use late chunking to increase the context that is taken into account for each chunk.  Default is False.</p> <code>splitting_strategy</code> <code>str | None</code> <p>The strategy to use for splitting the text into chunks. Default is \"sentence\". If semantic splitting is used, semantic_encoder is required.</p> <code>semantic_encoder</code> <code>EmbeddingModel | None</code> <p>The semantic encoder for the Text Embedding model. Default is None.</p> <code>use_ocr</code> <code>bool | None</code> <p>A flag indicating whether to use OCR for the Text Embedding model. Default is False.</p> <code>tesseract_path</code> <code>str | None</code> <p>The path to the Tesseract OCR executable. Default is None and uses the system path.</p> <code>pdf_backend</code> <code>str | None</code> <p>The backend to use for PDF text extraction. Currently only <code>lopdf</code> is supported. Default is <code>lopdf</code>.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class TextEmbedConfig:\n    \"\"\"\n    Represents the configuration for the Text Embedding model.\n\n    Attributes:\n        chunk_size: The chunk size for the Text Embedding model. Default is 1000 Characters.\n        batch_size: The batch size for processing the embeddings. Default is 32. Based on the memory, you can increase or decrease the batch size.\n        buffer_size: The buffer size for the Text Embedding model. Default is 100.\n        late_chunking: A flag indicating whether to use late chunking for the Text Embedding model. Use late chunking to increase the context that is taken into account for each chunk.  Default is False.\n        splitting_strategy: The strategy to use for splitting the text into chunks. Default is \"sentence\". If semantic splitting is used, semantic_encoder is required.\n        semantic_encoder: The semantic encoder for the Text Embedding model. Default is None.\n        use_ocr: A flag indicating whether to use OCR for the Text Embedding model. Default is False.\n        tesseract_path: The path to the Tesseract OCR executable. Default is None and uses the system path.\n        pdf_backend: The backend to use for PDF text extraction. Currently only `lopdf` is supported. Default is `lopdf`.\n    \"\"\"\n\n    def __init__(\n        self,\n        chunk_size: int | None = 1000,\n        overlap_ratio: float | None = 0.0,\n        batch_size: int | None = 32,\n        late_chunking: bool | None = False,\n        buffer_size: int | None = 100,\n        splitting_strategy: str | None = \"sentence\",\n        semantic_encoder: EmbeddingModel | None = None,\n        use_ocr: bool | None = False,\n        tesseract_path: str | None = None,\n        pdf_backend: str | None = \"lopdf\",\n    ):\n        self.chunk_size = chunk_size\n        self.overlap_ratio = overlap_ratio\n        self.batch_size = batch_size\n        self.late_chunking = late_chunking\n        self.buffer_size = buffer_size\n        self.splitting_strategy = splitting_strategy\n        self.semantic_encoder = semantic_encoder\n        self.use_ocr = use_ocr\n        self.tesseract_path = tesseract_path\n        self.pdf_backend = pdf_backend\n    chunk_size: int | None\n    overlap_ratio: float | None\n    batch_size: int | None\n    late_chunking: bool | None\n    buffer_size: int | None\n    splitting_strategy: str | None\n    semantic_encoder: EmbeddingModel | None\n    use_ocr: bool | None\n    tesseract_path: str | None\n    pdf_backend: str | None\n</code></pre>"},{"location":"references/#python.python.embed_anything.embed_audio_file","title":"<code>embed_audio_file(file_path, audio_decoder, embedder, text_embed_config=TextEmbedConfig(chunk_size=1000, batch_size=32))</code>","text":"<p>Embeds the given audio file and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the audio file to embed.</p> required <code>audio_decoder</code> <code>AudioDecoderModel</code> <p>The audio decoder model to use.</p> required <code>embedder</code> <code>EmbeddingModel</code> <p>The embedding model to use.</p> required <code>text_embed_config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding model.</p> <code>TextEmbedConfig(chunk_size=1000, batch_size=32)</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> <p>Example: <pre><code>import embed_anything\naudio_decoder = embed_anything.AudioDecoderModel.from_pretrained_hf(\n    \"openai/whisper-tiny.en\", revision=\"main\", model_type=\"tiny-en\", quantized=False\n)\n\nembedder = embed_anything.EmbeddingModel.from_pretrained_hf(\n    embed_anything.WhichModel.Bert,\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    revision=\"main\",\n)\n\nconfig = embed_anything.TextEmbedConfig(chunk_size=1000, batch_size=32)\ndata = embed_anything.embed_audio_file(\n    \"test_files/audio/samples_hp0.wav\",\n    audio_decoder=audio_decoder,\n    embedder=embedder,\n    text_embed_config=config,\n)\n</code></pre></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_audio_file(\n    file_path: str,\n    audio_decoder: AudioDecoderModel,\n    embedder: EmbeddingModel,\n    text_embed_config: TextEmbedConfig | None = TextEmbedConfig(\n        chunk_size=1000, batch_size=32\n    ),\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given audio file and returns a list of EmbedData objects.\n\n    Args:\n        file_path: The path to the audio file to embed.\n        audio_decoder: The audio decoder model to use.\n        embedder: The embedding model to use.\n        text_embed_config: The configuration for the embedding model.\n\n    Returns:\n        A list of EmbedData objects.\n\n    Example:\n    ```python\n\n    import embed_anything\n    audio_decoder = embed_anything.AudioDecoderModel.from_pretrained_hf(\n        \"openai/whisper-tiny.en\", revision=\"main\", model_type=\"tiny-en\", quantized=False\n    )\n\n    embedder = embed_anything.EmbeddingModel.from_pretrained_hf(\n        embed_anything.WhichModel.Bert,\n        model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n        revision=\"main\",\n    )\n\n    config = embed_anything.TextEmbedConfig(chunk_size=1000, batch_size=32)\n    data = embed_anything.embed_audio_file(\n        \"test_files/audio/samples_hp0.wav\",\n        audio_decoder=audio_decoder,\n        embedder=embedder,\n        text_embed_config=config,\n    )\n    ```\n\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.embed_directory","title":"<code>embed_directory(file_path, embedder, extensions, config=None, adapter=None)</code>","text":"<p>Embeds the files in the given directory and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the directory containing the files to embed.</p> required <code>embedder</code> <code>EmbeddingModel</code> <p>The embedding model to use.</p> required <code>extensions</code> <code>list[str]</code> <p>The list of file extensions to consider for embedding.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding model.</p> <code>None</code> <code>adapter</code> <code>Adapter | None</code> <p>The adapter to use for storing the embeddings in a vector database.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> <p>Example: <pre><code>import embed_anything\nmodel = embed_anything.EmbeddingModel.from_pretrained_hf(\n    embed_anything.WhichModel.Bert,\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    revision=\"main\",\n)\ndata = embed_anything.embed_directory(\"test_files\", embedder=model, extensions=[\".pdf\"])\n</code></pre></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_directory(\n    file_path: str,\n    embedder: EmbeddingModel,\n    extensions: list[str],\n    config: TextEmbedConfig | None = None,\n    adapter: Adapter | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the files in the given directory and returns a list of EmbedData objects.\n\n    Args:\n        file_path: The path to the directory containing the files to embed.\n        embedder: The embedding model to use.\n        extensions: The list of file extensions to consider for embedding.\n        config: The configuration for the embedding model.\n        adapter: The adapter to use for storing the embeddings in a vector database.\n\n    Returns:\n        A list of EmbedData objects.\n\n    Example:\n    ```python\n    import embed_anything\n    model = embed_anything.EmbeddingModel.from_pretrained_hf(\n        embed_anything.WhichModel.Bert,\n        model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n        revision=\"main\",\n    )\n    data = embed_anything.embed_directory(\"test_files\", embedder=model, extensions=[\".pdf\"])\n    ```\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.embed_file","title":"<code>embed_file(file_path, embedder, config=None, adapter=None)</code>","text":"<p>Embeds the given file and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the file to embed.</p> required <code>embedder</code> <code>EmbeddingModel</code> <p>The embedding model to use.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding model.</p> <code>None</code> <code>adapter</code> <code>Adapter | None</code> <p>The adapter to use for storing the embeddings in a vector database.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> <p>Example: <pre><code>import embed_anything\nmodel = embed_anything.EmbeddingModel.from_pretrained_hf(\n    embed_anything.WhichModel.Bert,\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    revision=\"main\",\n)\ndata = embed_anything.embed_file(\"test_files/test.pdf\", embedder=model)\n</code></pre></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_file(\n    file_path: str,\n    embedder: EmbeddingModel,\n    config: TextEmbedConfig | None = None,\n    adapter: Adapter | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given file and returns a list of EmbedData objects.\n\n    Args:\n        file_path: The path to the file to embed.\n        embedder: The embedding model to use.\n        config: The configuration for the embedding model.\n        adapter: The adapter to use for storing the embeddings in a vector database.\n\n    Returns:\n        A list of EmbedData objects.\n\n    Example:\n    ```python\n    import embed_anything\n    model = embed_anything.EmbeddingModel.from_pretrained_hf(\n        embed_anything.WhichModel.Bert,\n        model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n        revision=\"main\",\n    )\n    data = embed_anything.embed_file(\"test_files/test.pdf\", embedder=model)\n    ```\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.embed_files_batch","title":"<code>embed_files_batch(files, embedder, config=None, adapter=None)</code>","text":"<p>Embeds the given files and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>list[str]</code> <p>The list of files to embed.</p> required <code>embedder</code> <code>EmbeddingModel</code> <p>The embedding model to use.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding model.</p> <code>None</code> <code>adapter</code> <code>Adapter | None</code> <p>The adapter to use for storing the embeddings in a vector database.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> <p>Example: <pre><code>import embed_anything\nmodel = embed_anything.EmbeddingModel.from_pretrained_hf(\n    embed_anything.WhichModel.Bert,\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    revision=\"main\",\n)\ndata = embed_anything.embed_files_batch(\n    [\"test_files/test.pdf\", \"test_files/test.txt\"],\n    embedder=model,\n    config=embed_anything.TextEmbedConfig(),\n    adapter=None,\n)\n</code></pre></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_files_batch(\n    files: list[str],\n    embedder: EmbeddingModel,\n    config: TextEmbedConfig | None = None,\n    adapter: Adapter | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given files and returns a list of EmbedData objects.\n\n    Args:\n        files: The list of files to embed.\n        embedder: The embedding model to use.\n        config: The configuration for the embedding model.\n        adapter: The adapter to use for storing the embeddings in a vector database.\n\n    Returns:\n        A list of EmbedData objects.\n\n    Example:\n    ```python\n    import embed_anything\n    model = embed_anything.EmbeddingModel.from_pretrained_hf(\n        embed_anything.WhichModel.Bert,\n        model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n        revision=\"main\",\n    )\n    data = embed_anything.embed_files_batch(\n        [\"test_files/test.pdf\", \"test_files/test.txt\"],\n        embedder=model,\n        config=embed_anything.TextEmbedConfig(),\n        adapter=None,\n    )\n    ```\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.embed_html","title":"<code>embed_html(file_name, embedder, origin=None, config=None, adapter=None)</code>","text":"<p>Embeds the given HTML file and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The path to the HTML file to embed.</p> required <code>embedder</code> <code>EmbeddingModel</code> <p>The embedding model to use.</p> required <code>origin</code> <code>str | None</code> <p>The origin of the HTML file.</p> <code>None</code> <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding model.</p> <code>None</code> <code>adapter</code> <code>Adapter | None</code> <p>The adapter to use for storing the embeddings.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> <p>Example: <pre><code>import embed_anything\nmodel = embed_anything.EmbeddingModel.from_pretrained_hf(\n    embed_anything.WhichModel.Bert,\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    revision=\"main\",\n)\ndata = embed_anything.embed_html(\n    \"test_files/test.html\", embedder=model, origin=\"https://www.akshaymakes.com/\"\n)\n</code></pre></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_html(\n    file_name: str,\n    embedder: EmbeddingModel,\n    origin: str | None = None,\n    config: TextEmbedConfig | None = None,\n    adapter: Adapter | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given HTML file and returns a list of EmbedData objects.\n\n    Args:\n        file_name: The path to the HTML file to embed.\n        embedder: The embedding model to use.\n        origin: The origin of the HTML file.\n        config: The configuration for the embedding model.\n        adapter: The adapter to use for storing the embeddings.\n\n    Returns:\n        A list of EmbedData objects.\n\n    Example:\n    ```python\n    import embed_anything\n    model = embed_anything.EmbeddingModel.from_pretrained_hf(\n        embed_anything.WhichModel.Bert,\n        model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n        revision=\"main\",\n    )\n    data = embed_anything.embed_html(\n        \"test_files/test.html\", embedder=model, origin=\"https://www.akshaymakes.com/\"\n    )\n    ```\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.embed_image_directory","title":"<code>embed_image_directory(file_path, embedder, config=None, adapter=None)</code>","text":"<p>Embeds the images in the given directory and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the directory containing the images to embed.</p> required <code>embedder</code> <code>EmbeddingModel</code> <p>The embedding model to use.</p> required <code>config</code> <code>ImageEmbedConfig | None</code> <p>The configuration for the embedding model.</p> <code>None</code> <code>adapter</code> <code>Adapter | None</code> <p>The adapter to use for storing the embeddings in a vector database.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_image_directory(\n    file_path: str,\n    embedder: EmbeddingModel,\n    config: ImageEmbedConfig | None = None,\n    adapter: Adapter | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the images in the given directory and returns a list of EmbedData objects.\n\n    Args:\n        file_path: The path to the directory containing the images to embed.\n        embedder: The embedding model to use.\n        config: The configuration for the embedding model.\n        adapter: The adapter to use for storing the embeddings in a vector database.\n\n    Returns:\n        A list of EmbedData objects.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.embed_query","title":"<code>embed_query(query, embedder, config=None)</code>","text":"<p>Embeds the given query and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>list[str]</code> <p>The query to embed.</p> required <code>embedder</code> <code>EmbeddingModel</code> <p>The embedding model to use.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding model.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> <p>Example:</p> <pre><code>import embed_anything\nmodel = embed_anything.EmbeddingModel.from_pretrained_hf(\n    embed_anything.WhichModel.Bert,\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    revision=\"main\",\n)\n</code></pre> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_query(\n    query: list[str], embedder: EmbeddingModel, config: TextEmbedConfig | None = None\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given query and returns a list of EmbedData objects.\n\n    Args:\n        query: The query to embed.\n        embedder: The embedding model to use.\n        config: The configuration for the embedding model.\n\n    Returns:\n        A list of EmbedData objects.\n\n    Example:\n\n    ```python\n    import embed_anything\n    model = embed_anything.EmbeddingModel.from_pretrained_hf(\n        embed_anything.WhichModel.Bert,\n        model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n        revision=\"main\",\n    )\n    ```\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.embed_webpage","title":"<code>embed_webpage(url, embedder, config, adapter)</code>","text":"<p>Embeds the webpage at the given URL and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the webpage to embed.</p> required <code>embedder</code> <code>EmbeddingModel</code> <p>The name of the embedding model to use. Choose between \"OpenAI\", \"Jina\", \"Bert\"</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding model.</p> required <code>adapter</code> <code>Adapter | None</code> <p>The adapter to use for storing the embeddings.</p> required <p>Returns:</p> Type Description <code>list[EmbedData] | None</code> <p>A list of EmbedData objects</p> <p>Example: <pre><code>import embed_anything\n\nconfig = embed_anything.EmbedConfig(\n    openai_config=embed_anything.OpenAIConfig(model=\"text-embedding-3-small\")\n)\ndata = embed_anything.embed_webpage(\n    \"https://www.akshaymakes.com/\", embedder=\"OpenAI\", config=config\n)\n</code></pre></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_webpage(\n    url: str,\n    embedder: EmbeddingModel,\n    config: TextEmbedConfig | None,\n    adapter: Adapter | None,\n) -&gt; list[EmbedData] | None:\n    \"\"\"Embeds the webpage at the given URL and returns a list of EmbedData\n    objects.\n\n    Args:\n        url: The URL of the webpage to embed.\n        embedder: The name of the embedding model to use. Choose between \"OpenAI\", \"Jina\", \"Bert\"\n        config: The configuration for the embedding model.\n        adapter: The adapter to use for storing the embeddings.\n\n    Returns:\n        A list of EmbedData objects\n\n    Example:\n    ```python\n    import embed_anything\n\n    config = embed_anything.EmbedConfig(\n        openai_config=embed_anything.OpenAIConfig(model=\"text-embedding-3-small\")\n    )\n    data = embed_anything.embed_webpage(\n        \"https://www.akshaymakes.com/\", embedder=\"OpenAI\", config=config\n    )\n    ```\n    \"\"\"\n</code></pre>"},{"location":"blog/","title":"\ud83d\udcf0 All Posts","text":""},{"location":"blog/2024/03/31/gemini/","title":"Gemini as a Research Agent: Fusion Deep Research for Intelligent Information Retrieval","text":"<p>In today's data-saturated world, effective research isn't just about accessing information\u2014it's about finding the right information efficiently. Let's explore how Gemini can function as a sophisticated research agent, using a fusion deep research approach to intelligently determine what information is necessary and when to search external sources.</p> <p>Please find the repository here.</p>"},{"location":"blog/2024/03/31/gemini/#the-fusion-deep-research-architecture","title":"The Fusion Deep Research Architecture","text":"<p>The system I've been building employs Gemini 2.0 Flash as an intelligent intermediary between user queries and multiple information sources. This fusion approach combines:</p> <ol> <li>Local vector database knowledge</li> <li>Web-based information retrieval</li> <li>Intelligent query reformulation</li> <li>Comprehensive information evaluation</li> </ol>"},{"location":"blog/2024/03/31/gemini/#purpose-of-done-flag","title":"Purpose of done flag","text":"<p>The <code>\"done\": false</code> flag is a crucial control mechanism in the Gemini research agent system. It indicates whether the research process should continue or if sufficient information has been gathered. Here's a deeper breakdown of how it works: Purpose of the \"done\" Flag The \"done\" flag serves as the agent's decision-making output that determines whether:</p> <pre><code>More research iterations are needed (false)\nEnough information has been collected (true)\n</code></pre> <p></p>"},{"location":"blog/2024/03/31/gemini/#how-the-agent-sets-this-flag","title":"How the Agent Sets This Flag","text":"<p>After each research iteration, Gemini evaluates the collected information against specific criteria: CopyBased on this observations, you have two options: <code>1. Find knowledge gaps that still need to be explored and write 3 different queries that explore different perspectives of the topic. If this is the case set the done flag to False. 2. If there are no more knowledge gaps and you have enough information related to the topic, you dont have to provide any more queries and you can set the done flag to True. When Gemini sets \"done\": false, it's essentially saying: \"I've analyzed what we know so far, and there are still important aspects of this topic we haven't covered adequately.\" Evaluation Criteria The system uses sophisticated criteria to make this determination: CopyBefore setting the done flag to true, make sure that the following conditions are met:  1. You have explored different perspectives of the topic 2. You have collected some opposing views 3. You have collected some supporting views 4. You have collected some views that are not directly related to the topic but can be used to</code></p>"},{"location":"blog/2024/03/31/gemini/#how-fusion-deep-research-works","title":"How Fusion Deep Research Works","text":"<p>Let's examine a specific example where a user asks: \"What are the differences between SSM models and transformer models?\"</p>"},{"location":"blog/2024/03/31/gemini/#step-1-query-diversification","title":"Step 1: Query Diversification","text":"<p>When receiving this query, Gemini first analyzes it to create focused search terms. Instead of using the raw query, it generates three distinct perspectives to explore:</p> <pre><code>{\n  \"querys\": [\n    \"Key architectural differences between SSM models and transformer models\",\n    \"Computational efficiency comparison between SSM models and transformer models\",\n    \"When to use SSM models versus transformer models\"\n  ],\n  \"done\": false\n}\n</code></pre> <p>This diversification ensures comprehensive coverage of the topic from multiple angles\u2014a key principle of fusion deep research.</p>"},{"location":"blog/2024/03/31/gemini/#step-2-multi-source-information-gathering","title":"Step 2: Multi-Source Information Gathering","text":"<p>The agent uses a fusion approach to information retrieval, intelligently determining when to: - Query the local vector database for trusted information - Expand to web sources when local information is insufficient</p> <pre><code>def get_observations(queries: List[str]) -&gt; List[str]:\n    local_observations = []\n    web_observations = []\n    for query in queries:\n        local_observation = my_store.forward(query)\n        local_observations.extend(list(local_observation))\n\n        if web_search:\n            web_result = exa.search_and_contents(query, type=\"auto\", text=True, num_results=3)\n            # Process web results...\n</code></pre> <p>This fusion of information sources creates a more robust research foundation.</p>"},{"location":"blog/2024/03/31/gemini/#step-3-continuous-knowledge-gap-analysis","title":"Step 3: Continuous Knowledge Gap Analysis","text":"<p>After each search iteration, Gemini evaluates the collected information against specific criteria:</p> <pre><code>Step Number: 1\nSearching with queries: \nKey architectural differences between SSM models and transformer models\nComputational efficiency comparison between SSM models and transformer models\nWhen to use SSM models versus transformer models\nDone: False\n</code></pre> <p>Importantly, the system doesn't just blindly collect information\u2014it actively identifies knowledge gaps:</p> <pre><code>Based on this observations, you have two options:\n1. Find knowledge gaps that still need to be explored and write 3 different queries that explore different perspectives of the topic. If this is the case set the done flag to False.\n2. If there are no more knowledge gaps and you have enough information related to the topic, you dont have to provide any more queries and you can set the done flag to True.\n</code></pre> <p>This continuous evaluation represents the \"deep\" aspect of fusion deep research\u2014digging beyond surface-level information to ensure comprehensive understanding.</p>"},{"location":"blog/2024/03/31/gemini/#step-4-multi-perspective-completion-criteria","title":"Step 4: Multi-Perspective Completion Criteria","text":"<p>Perhaps most impressively, Gemini autonomously determines when sufficient information has been gathered based on robust criteria:</p> <pre><code>Before setting the done flag to true, make sure that the following conditions are met: \n1. You have explored different perspectives of the topic\n2. You have collected some opposing views\n3. You have collected some supporting views\n4. You have collected some views that are not directly related to the topic but can be used to explore the topic further.\n</code></pre> <p>This creates a research process that is both broad (multiple perspectives) and deep (thorough exploration of each perspective).</p>"},{"location":"blog/2024/03/31/gemini/#step-5-information-synthesis-and-report-generation","title":"Step 5: Information Synthesis and Report Generation","text":"<p>Once fusion deep research is complete, Gemini synthesizes the collected information into a cohesive report, intelligently blending insights from all sources without explicitly highlighting their origins:</p> <pre><code>Do not explicitly mention if the output is from local or web observations. Just write the report as if you have all the information available.\n</code></pre> <p>This seamless integration of multiple information sources is the culmination of the fusion deep research approach.</p>"},{"location":"blog/2024/03/31/gemini/#why-fusion-deep-research-matters","title":"Why Fusion Deep Research Matters","text":"<p>Traditional research methods often: - Rely too heavily on a single information source - Miss important perspectives - Require manual reformulation of queries - Don't know when \"enough is enough\" - Need Evaluation setup manually</p> <p>By implementing fusion deep research with Gemini as an intelligent agent, we transform research from a mechanical process into an adaptive methodology that:</p> <ol> <li>Ensures comprehensive coverage through multiple information sources</li> <li>Identifies and fills knowledge gaps through intelligent query reformulation</li> <li>Balances efficiency and thoroughness by prioritizing local knowledge before web searches</li> <li>Delivers multi-perspective insights rather than single-viewpoint information</li> <li>Knows when to stop searching based on sophisticated completion criteria</li> </ol>"},{"location":"blog/2024/03/31/gemini/#building-your-own-fusion-deep-research-system","title":"Building Your Own Fusion Deep Research System","text":"<p>The implementation requires:</p> <ol> <li>A vector database for local document storage</li> <li>Access to Gemini API (specifically gemini-2.0-flash)</li> <li>A web search API (the example uses Exa)</li> <li>Clear evaluation criteria for information sufficiency</li> </ol> <p>The most critical component is the prompt engineering that enables Gemini to make intelligent decisions about information adequacy and query formulation within the fusion deep research framework.</p>"},{"location":"blog/2024/03/31/gemini/#conclusion","title":"Conclusion","text":"<p>As information continues to expand exponentially, fusion deep research represents the future of intelligent information retrieval. By combining multiple information sources with sophisticated evaluation criteria and autonomous decision-making about research sufficiency, we can dramatically improve both the efficiency and quality of our research processes.</p> <p>Gemini's ability to act as an intelligent intermediary in the fusion deep research process\u2014reformulating queries, evaluating information completeness across sources, and knowing when to stop\u2014transforms it from a mere language model into a true research assistant. As these systems evolve, they promise to redefine how we interact with the growing sea of information around us, making fusion deep research an essential tool for knowledge workers across all domains.</p> <p>Please find the repository here.</p> <p>Please give a \u2b50 to our repo.</p>"},{"location":"blog/2024/12/15/embed-anything/","title":"The path ahead of EmbedAnything","text":"<p>In March, we set out to build a local file search app. We aimed to create a tool that would make file searching faster, more innovative, and more efficient. However, we quickly hit a roadblock: no high-performance backend fit our needs.</p> <p></p>"},{"location":"blog/2024/12/15/embed-anything/#short-of-backend","title":"Short of backend","text":"<p>Initially, we experimented with LlamaIndex, hoping it would provide the required speed and reliability. Unfortunately, it fell short. Its performance didn\u2019t meet our expectations, and its heavy dependencies added unnecessary complexity to our stack. We realized we needed a better solution.</p> <p>Around the same time, we discovered Candle, a Rust-based framework for transformer model inference. Candle stood out with its remarkable speed and minimal dependency footprint. It was exactly what we were looking for a high-performing, lightweight backend that aligned with our vision for a seamless file search experience.</p>"},{"location":"blog/2024/12/15/embed-anything/#experimentation-and-breakthroughs","title":"Experimentation and Breakthroughs","text":"<p>Excited by Candle\u2019s potential, we experimented to see how well it could handle our use case. The results were outstanding. Candle\u2019s blazing-fast inference speeds and low resource demands enabled us to build a prototype that surpassed our initial performance goals.</p> <p>With a working prototype, we decided to share it with the world. We knew a compelling demonstration could capture attention and validate our efforts. The next step was to make a splash with our launch.</p>"},{"location":"blog/2024/12/15/embed-anything/#demo-released","title":"Demo Released","text":"<p>On April 2nd, we unveiled our demo online, carefully choosing the date to avoid confusion with April Fool\u2019s Day. We created an engaging demo video to highlight the app\u2019s capabilities and shared it on Twitter. What happened next exceeded all our expectations.</p> <p>The demo received an overwhelming response. What began as a simple showcase of our prototype transformed into a pivotal moment for our project. In the next 30 days, we released it as an open-source project, seeing the demand and people\u2019s interest.</p> <p>Demo</p>"},{"location":"blog/2024/12/15/embed-anything/#02-released","title":"0.2 released","text":"<p>Since then, we have never looked back. We kept embedding anything better and better. In the next three months, we released a more stable version, 0.2, with all the Python versions. It was running amazingly on AWS and could support multimodality.</p> <p>At the same time, we realized that people wanted an end-to-end solution, not just an embedding generation platform. So we tried to integrate a vector database, but we realized that it would just make our library heavier and not give the value we were looking for, which was confirmed by this discussion opened on our GitHub.</p> <p>\u2014GitHub discussion</p> <p>Akshay started looking for ways to index embeddings without being dependent on vector databases as a dependency, and he came up with a brilliant method that enhanced performance and made indexing extremely memory efficient.</p> <p>And thus, vector streaming was born.</p> <p>\u2014 vector streaming blog</p>"},{"location":"blog/2024/12/15/embed-anything/#03-release","title":"0.3 release","text":"<p>It's time to release 0.3 because we underwent major code refactoring. All the major functions are refactored, making calling models more intuitive and optimized. Check out our docs and usage. We also added audio modality and different types of ingestions.</p> <p>We only supported dense, so we expanded the types of embedding we could support. We went for sparse and started supporting ColPali, ColBert, ModernBert, Reranker, Jina V3.</p>"},{"location":"blog/2024/12/15/embed-anything/#what-we-got-right","title":"What We Got Right","text":"<p>We actively listened to our community and prioritized their needs in the library's development. When users requested support for sparse matrices in hybrid models, we delivered. When they wanted advanced indexing, we made it happen. During the critical three-month period between versions 0.2 and 0.4, our efforts were laser-focused on enhancing the product to meet and exceed expectations. </p> <p>We also released benches comparing it with other inference and to our suprise it's faster than libraries like sentence transformer and fastembed. Check out Benches.</p> <p>We presented Embedanything at many conferences, like Pydata Global, Elastic, voxel 51 meetups, AI builders, etc. Additionally, we forged collaborations with major brands like Weaviate and Elastic, a strategy we\u2019re excited to continue expanding in 2025.</p> <p>Elastic Collab</p>"},{"location":"blog/2024/12/15/embed-anything/#what-we-initially-got-wrong","title":"What We Initially Got Wrong","text":"<p>In hindsight, one significant mistake was prematurely releasing the library before it was ready for production. As the saying goes, \u201cYou never get a second chance to make a first impression,\u201d and this holds true even for open-source projects.</p> <p>The library was unusable on macOS for the first three months, and we only released compatibility with Python 10. We didn\u2019t focus enough on how we were rolling out updates, partly because we never anticipated the overwhelming rate of experimentation and interest it would receive right from the start.</p> <p>I intended to foster a \u201cbuild in public\u201d project, encouraging collaboration and rapid iteration. I wanted to showcase how quickly we could improve and refine this amazing library. </p>"},{"location":"blog/2024/12/15/embed-anything/#in-the-year-2025","title":"In the year 2025","text":"<p>We are committed to applying everything we\u2019ve learned from this journey and doubling down on what truly matters: our hero, the product. In the grand scheme of things, nothing else is as important. Moving forward, we\u2019re also excited to announce even more collaborations with amazing brands, further expanding the impact and reach of our work.</p> <p>Heartfelt thanks to all our amazing contributors and stargazers for your unwavering support and dedication to embedanything. Your continuous experimentation and feedback inspire us to keep refining and enhancing the library with every iteration. We deeply appreciate your efforts in making this journey truly collaborative. Let\u2019s go from 100k+ to a million downloads this year!</p>"},{"location":"blog/2025/05/25/release-notes-6/","title":"Release Notes 6.0","text":"<p>Super Excited to share the latest development in our library, which essentially giving you more embedding choices -- Cohere and siglip, new chunking method-- late chunking and more crates that facilitates amazing modality and maintainability for our rust codebase, --processor crate. so let's dive in.</p>"},{"location":"blog/2025/05/25/release-notes-6/#late-chunking","title":"Late Chunking","text":"<p>The new 0.5.6 version adds Late Chunking to EmbedAnything, a technique introduced by Jina AI and Weaviate.  Here's how we've implemented Late Chunking in EA:</p> <p>\ud835\uddd5\ud835\uddee\ud835\ude01\ud835\uddf0\ud835\uddf5 \ud835\uddee\ud835\ude00 \ud835\uddd6\ud835\uddf5\ud835\ude02\ud835\uddfb\ud835\uddf8 \ud835\uddda\ud835\uddff\ud835\uddfc\ud835\ude02\ud835\uddfd: In EmbedAnything, with late chunking enabled, the batch size determines the number of neighboring chunks that will be processed together.</p> <p>\ud835\udddd\ud835\uddfc\ud835\uddf6\ud835\uddfb\ud835\ude01 \ud835\uddd8\ud835\uddfa\ud835\uddef\ud835\uddf2\ud835\uddf1\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4: The grouped chunks are fed into the embedding model as a single, larger input. This allows the model to capture relationships and dependencies between adjacent chunks.</p> <p>\ud835\uddd8\ud835\uddfa\ud835\uddef\ud835\uddf2\ud835\uddf1\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4 \ud835\udde6\ud835\uddfd\ud835\uddf9\ud835\uddf6\ud835\ude01: After embedding, the combined output is divided back into the embeddings for the original, individual chunks.</p> <p>\ud835\udde0\ud835\uddf2\ud835\uddee\ud835\uddfb \ud835\udde3\ud835\uddfc\ud835\uddfc\ud835\uddf9\ud835\uddf6\ud835\uddfb\ud835\uddf4 (\ud835\uddfd\ud835\uddf2\ud835\uddff \ud835\uddd6\ud835\uddf5\ud835\ude02\ud835\uddfb\ud835\uddf8): Mean pooling is then applied to each individual chunk's embedding, incorporating the contextual information learned during the joint embedding phase.</p> <p>\ud835\udc3e\ud835\udc52\ud835\udc66 \ud835\udc35\ud835\udc52\ud835\udc5b\ud835\udc52\ud835\udc53\ud835\udc56\ud835\udc61\ud835\udc60:</p> <p>\ud835\uddd6\ud835\uddfc\ud835\uddfb\ud835\ude01\ud835\uddf2\ud835\ude05\ud835\ude01-\ud835\uddd4\ud835\ude04\ud835\uddee\ud835\uddff\ud835\uddf2 \ud835\uddd8\ud835\uddfa\ud835\uddef\ud835\uddf2\ud835\uddf1\ud835\uddf1\ud835\uddf6\ud835\uddfb\ud835\uddf4\ud835\ude00: By embedding neighboring chunks together, we capture crucial contextual information that would be lost with independent chunking.</p> <p>\ud835\udde2\ud835\uddfd\ud835\ude01\ud835\uddf6\ud835\uddfa\ud835\uddf6\ud835\ude07\ud835\uddf2\ud835\uddf1 \ud835\udde5\ud835\uddf2\ud835\ude01\ud835\uddff\ud835\uddf6\ud835\uddf2\ud835\ude03\ud835\uddee\ud835\uddf9 \ud835\udde3\ud835\uddf2\ud835\uddff\ud835\uddf3\ud835\uddfc\ud835\uddff\ud835\uddfa\ud835\uddee\ud835\uddfb\ud835\uddf0\ud835\uddf2: Expect a significant improvement in the accuracy and relevance of your search results.</p> <pre><code>model:EmbeddingModel = EmbeddingModel.from_pretrained_onnx(\n    WhichModel.Jina, hf_model_id=\"jinaai/jina-embeddings-v2-small-en\", path_in_repo=\"model.onnx\"\n)\nconfig = TextEmbedConfig(\n    chunk_size=1000,\n    batch_size=8,\n    splitting_strategy=\"sentence\",\n    late_chunking=True,\n)\n\n# Embed a single file\ndata: list[EmbedData] = model.embed_file(\"test_files/attention.pdf\", config=config)\n</code></pre>"},{"location":"blog/2025/05/25/release-notes-6/#cohere-embed-4","title":"Cohere Embed 4:","text":"<p>\ud83e\uddca Single embedding per document, even for multimodal inputs \ud83d\udcda Handles up to 128K tokens \u2013 perfect for long-form business documents \ud83d\uddc3\ufe0f Supports compressed vector formats (int8, binary) for real-world scalability \ud83c\udf10 Multilingual across 100+ languages</p> <p>The catch? It\u2019s not open-source\u2014and even if it were, the model would be quite hefty to run locally. But if you\u2019re already using cloud-based embeddings like OpenAI\u2019s, Embed v4 is worth testing.</p> <pre><code># Initialize the model once\nmodel: EmbeddingModel = EmbeddingModel.from_pretrained_cloud(\n    WhichModel.CohereVision, model_id=\"embed-v4.0\"\n)\n</code></pre>"},{"location":"blog/2025/05/25/release-notes-6/#siglip","title":"SigLIP","text":"<p>We already had Clip support but many of you asked for siglip support. It out performs clip for zero shot classification for smaller batch. It also has better memory efficinecy.</p> <pre><code># Load the model.\nmodel = embed_anything.EmbeddingModel.from_pretrained_hf(\n    embed_anything.WhichModel.Clip,\n    model_id=\"google/siglip-base-patch16-224\",\n)\n</code></pre>"},{"location":"blog/2025/05/25/release-notes-6/#processor-crate","title":"Processor Crate:","text":"<p>This crate contains various \"processors\" that accepts files and produces a chunked, metadata-rich document description. This is especially helpful for retrieval-augmented generation! </p> <p>We have also received some additional cool feature requests on GitHub, which we would like to implement. If you want to help out please check out EmbedAnything on GitHub. We would love to have a contribution. \ud83d\ude80</p>"},{"location":"blog/2025/01/25/smolagent/","title":"In-and-Out of domain query with EmbedAnything and SmolAgent","text":"<p>When working with domain-specific queries, we often struggle with the challenge of balancing in-domain and out-of-domain requests. But not anymore! With embedanything, you can leverage fine-tuned, domain-focused models while smolagent takes the lead in smart decision-making. Whether you're handling queries from different domains or need to combine their insights seamlessly, smolagent ensures smooth collaboration, merging responses for a unified, accurate answer.</p> <p>But first let\u2019s discuss what is SmolAgent and then we can discuss each retrieval :</p> <p>According to Hugging-face\u2019s official release agents are:</p> <pre><code>AI Agents are\u00a0**programs where LLM outputs control the workflow**.\n</code></pre> <p>Any system leveraging LLMs will integrate the LLM outputs into code. The influence of the LLM\u2019s input on the code workflow is the level of agency of LLMs in the system.</p>"},{"location":"blog/2025/01/25/smolagent/#an-example","title":"An Example:","text":"<p>This agentic system runs in a loop, executing a new action at each step (the action can involve calling some pre-determined\u00a0tools\u00a0that are just functions), until its observations make it apparent that a satisfactory state has been reached to solve the given task. Here\u2019s an example of how a multi-step agent can solve a simple math question:</p> <p></p>"},{"location":"blog/2025/01/25/smolagent/#how-its-working-with-embedanything","title":"How it\u2019s working with EmbedAnything","text":"<p>Embedanything is used to take docs from source, to inference domain specific and general models and generate embeddings. It takes care of chunking and cleaning for parsing documents.</p> <p>Embed anything is a rust-based framework for the Ingestion of any file type in any modality, Inference of any model present in HF with their HF link using a candle and some Onnx models, and then indexing them to the vector database. It excels at three core functions:</p> <ol> <li>Document Processing: Automatically handles document intake from various sources, cleaning the text and removing irrelevant content to ensure quality input.</li> <li>Intelligent Chunking: Breaks down documents into optimal segments while preserving context and meaning, ensuring the resulting embeddings capture the full semantic value of the content.</li> <li>Flexible Model Integration: Seamlessly works with both general-purpose language models and specialized domain-specific models, allowing users to generate embeddings that best suit their specific use case.</li> </ol> <p>This streamlined pipeline eliminates the usual complexity of document embedding workflows, making it easier to prepare data for downstream tasks like semantic search, document retrieval, or content recommendation systems.</p> <p></p>"},{"location":"blog/2025/01/25/smolagent/#lets-get-into-the-code","title":"Let\u2019s get into the code:","text":"<p>In the accompanying diagram, we showcase two distinct folders containing different types of documents: one for general information and the other for domain-specific content\u2014for example, medicine-related documents.</p> <p>For domain-specific queries, we use a PubMed fine-tuned model, while for general queries, we rely on an ONNX model through embedanything. When a query is received, smolagent intelligently decides which tool to use based on the query's nature. It then processes the relevant parts of the query, performs retrieval, and rephrases the results to deliver a final, cohesive answer.</p> <p>Now, let\u2019s dive into the retrieval code and explore how this process works behind the scenes!</p> <pre><code>class RetrieverTool(Tool):\n    name = \"retriever\"\n    description = \"Uses semantic search to retrieve policies about india that could be most relevant to answer your query.\"\n    inputs = {\n        \"query\": {\n            \"type\": \"string\",\n            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n        }\n    }\n    output_type = \"string\"\n\n    def __init__(self, directory, **kwargs):\n        super().__init__(**kwargs)\n        self.model = EmbeddingModel.from_pretrained_onnx(WhichModel.Bert, ONNXModel.AllMiniLML6V2Q)\n        self.connection = lancedb.connect(\"tmp/general\")\n        if \"docs\" in self.connection.table_names():\n            self.table = self.connection.open_table(\"docs\")\n        else:\n            self.embeddings = embed_anything.embed_directory(directory, embedder = self.model)\n            docs = []\n            for e in self.embeddings:\n                docs.append({\n                    \"vector\": e.embedding,\n                    \"text\": e.text,\n                    \"id\": str(uuid4())\n                })\n            self.table = self.connection.create_table(\"docs\", docs)\n\n    def forward(self, query: str) -&gt; str:\n        assert isinstance(query, str), \"Your search query must be a string\"\n\n        query_vec = embed_anything.embed_query([query], embedder = self.model)[0].embedding\n        docs = self.table.search(query_vec).limit(5).to_pandas()[\"text\"]\n        return \"\\nRetrieved documents:\\n\" + \"\".join(\n            [f\"\\n\\n===== Document {str(i)} =====\\n\" + doc for i, doc in enumerate(docs)]\n        )\n</code></pre> <p>Let\u2019s begin by setting up a general query retrieval tool. This process generates embeddings through embed anything using an ONNX model for inference and then saves these embeddings to a LanceDB database. One key point to keep in mind: ensure you create separate tables for different domains in LanceDB. This structure allows for better organization and efficient retrieval. For each domain, you can also fine-tune different models tailored to that specific domain. In the next steps, we\u2019ll explore how to effectively handle in-domain queries to achieve precise and context-aware results.</p>"},{"location":"blog/2025/01/25/smolagent/#for-domain-specific-models","title":"For Domain Specific models","text":"<p>For domain-specific models, we are using Candle because it allows any fine-tuned model to run if it has a similar architecture. Three things have changed.</p> <ol> <li> <p>Model used is 'NeuML/pubmedbert-base-embeddings'</p> </li> <li> <p>EmbedAnything function: from pretrained hf</p> </li> <li>Lance DB table: tmp/medical</li> </ol> <p>``</p> <pre><code>self.model =EmbeddingModel.from_pretrained_hf(WhichModel.Bert, model_id='NeuML/pubmedbert-base-embeddings')\nself.connection = lancedb.connect(\"tmp/medical\")\n</code></pre>"},{"location":"blog/2025/01/25/smolagent/#run-smolagent","title":"Run SmolAgent","text":"<p>Finally, you'll need to provide the necessary tools and downloaded folders to smolagent. Here's an example: when given a single sentence containing two distinct queries\u2014one general and the other specific to radiology\u2014smolagent breaks it down intelligently. It retrieves answers for the general query from policy-related documents and addresses the radiology-specific query using relevant medical documents. Once processed, it seamlessly merges the results into a cohesive and accurate response.</p> <pre><code>retriever_tool = RetrieverTool(\"downloaded_docs\")\nmedical_tool = MedicalRetrieverTool(\"medical_docs\")\n\nagent = CodeAgent(\n    tools=[retriever_tool, medical_tool],\n    model=OpenAIServerModel(model_id = \"gpt-4o-mini\", api_base = \"https://api.openai.com/v1/\", api_key = api_key),\n    verbosity_level=2,\n)\n\nagent_output = agent.run(\"What are the different policies for indian manufacturing and what are the medical risks of radiotherapy?\")\n</code></pre>"},{"location":"blog/2025/01/25/smolagent/#output","title":"Output","text":"<p>The output generated involves multiple well-defined steps:</p> <ol> <li>Query Breakdown: The system first analyzes the query and breaks it into relevant components. </li> <li>Understanding Retrieval Needs: It identifies that the first part of the query requires general retrieval. </li> <li>Running General Retrieval Tools: The system runs general retrieval tools to gather context for the general query.</li> <li>Domain-Specific Retrieval: After obtaining the general context, it processes the second part of the query, performing retrieval on domain-specific data to gather the necessary insights.</li> <li>Answer Merging: Finally, it combines the results from both retrievals into a unified, coherent answer.</li> </ol> <p>This seamless workflow ensures precise handling of complex, multi-domain queries while maintaining context relevance across all steps.</p> <p></p> <p>Check out our </p> <p></p>"},{"location":"blog/2025/02/25/vector%20database/","title":"How to write adapters for your vector database.","text":"<p>We have received multiple requests to add different vector database for our vector streaming. So We have decided to put a detailed guide for different vector databases out there. We are happy to accept pull-request.</p>"},{"location":"blog/2025/02/25/vector%20database/#creating-custom-adapters-for-embedanything-a-step-by-step-guide","title":"Creating Custom Adapters for EmbedAnything: A Step-by-Step Guide","text":"<p>In the world of machine learning and natural language processing, working with embeddings has become a fundamental task. The EmbedAnything library simplifies the process of generating embeddings from various data sources, but what if you want to store these embeddings in a specific database or service? This is where adapters come in. In this blog post, we'll walk through the process of creating a custom adapter for the EmbedAnything library, using the Pinecone vector database as an example.</p>"},{"location":"blog/2025/02/25/vector%20database/#understanding-adapters-in-embedanything","title":"Understanding Adapters in EmbedAnything","text":"<p>Adapters serve as bridges between the EmbedAnything library and external services or databases. They handle the conversion and storage of embeddings, allowing you to seamlessly integrate EmbedAnything with your preferred storage solution.</p>"},{"location":"blog/2025/02/25/vector%20database/#the-anatomy-of-an-adapter","title":"The Anatomy of an Adapter","text":"<p>Before diving into the code, let's understand the key components of an adapter:</p> <ol> <li>Initialization: Setting up the connection to the external service</li> <li>Index Management: Creating and deleting indices in the external service</li> <li>Data Conversion: Transforming EmbedAnything's embedding format to the format required by the external service</li> <li>Data Storage: Storing the converted embeddings in the external service</li> </ol>"},{"location":"blog/2025/02/25/vector%20database/#creating-a-pinecone-adapter-step-by-step","title":"Creating a Pinecone Adapter: Step by Step","text":"<p>Let's break down the process of creating a Pinecone adapter for EmbedAnything:</p>"},{"location":"blog/2025/02/25/vector%20database/#step-1-set-up-the-basic-class-structure","title":"Step 1: Set Up the Basic Class Structure","text":"<pre><code>from embed_anything import EmbedData, EmbeddingModel, WhichModel, TextEmbedConfig\n\nclass PineconeAdapter(Adapter):\n    \"\"\"\n    Adapter class for interacting with Pinecone, a vector database service.\n    \"\"\"\n    def __init__(self, api_key: str):\n        \"\"\"\n        Initializes a new instance of the PineconeAdapter class.\n\n        Args:\n            api_key (str): The API key for accessing the Pinecone service.\n        \"\"\"\n        super().__init__(api_key)\n        self.pc = Pinecone(api_key=self.api_key)\n        self.index_name = None\n</code></pre> <p>In this step, we're: - Inheriting from the base <code>Adapter</code> class provided by EmbedAnything - Initializing the Pinecone client using the provided API key - Setting up an attribute to track the current index name</p>"},{"location":"blog/2025/02/25/vector%20database/#step-2-implement-index-management-methods","title":"Step 2: Implement Index Management Methods","text":"<pre><code>def create_index(\n    self, \n    dimension: int, \n    metric: str = \"cosine\", \n    index_name: str = \"anything\",\n    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n):\n    \"\"\"\n    Creates a new index in Pinecone.\n\n    Args:\n        dimension (int): The dimensionality of the embeddings.\n        metric (str, optional): The distance metric to use for similarity search. Defaults to \"cosine\".\n        index_name (str, optional): The name of the index. Defaults to \"anything\".\n        spec (ServerlessSpec, optional): The serverless specification for the index. Defaults to AWS in us-east-1 region.\n    \"\"\"\n    self.index_name = index_name\n    self.pc.create_index(\n        name=index_name,\n        dimension=dimension,\n        metric=metric,\n        spec=spec\n    )\n\ndef delete_index(self, index_name: str):\n    \"\"\"\n    Deletes an existing index from Pinecone.\n\n    Args:\n        index_name (str): The name of the index to delete.\n    \"\"\"\n    self.pc.delete_index(name=index_name)\n</code></pre> <p>These methods handle: - Creating a new index in Pinecone with the specified dimensions and distance metric - Deleting an existing index when needed - Storing the current index name for later use</p>"},{"location":"blog/2025/02/25/vector%20database/#step-3-implement-data-conversion-logic","title":"Step 3: Implement Data Conversion Logic","text":"<pre><code>def convert(self, embeddings: List[EmbedData]) -&gt; List[Dict]:\n    \"\"\"\n    Converts a list of embeddings into the required format for upserting into Pinecone.\n\n    Args:\n        embeddings (List[EmbedData]): The list of embeddings to convert.\n\n    Returns:\n        List[Dict]: The converted data in the required format for upserting into Pinecone.\n    \"\"\"\n    data_emb = []\n    for embedding in embeddings:\n        data_emb.append(\n            {\n                \"id\": str(uuid.uuid4()),\n                \"values\": embedding.embedding,\n                \"metadata\": {\n                    \"text\": embedding.text,\n                    \"file\": re.split(\n                        r\"/|\\\\\", embedding.metadata.get(\"file_name\", \"\")\n                    )[-1],\n                },\n            }\n        )\n    return data_emb\n</code></pre> <p>This method: - Takes a list of <code>EmbedData</code> objects from EmbedAnything - Converts each embedding into the format expected by Pinecone - Generates a unique ID for each embedding - Extracts and formats metadata from the original embedding</p>"},{"location":"blog/2025/02/25/vector%20database/#step-4-implement-storage-logic","title":"Step 4: Implement Storage Logic","text":"<pre><code>def upsert(self, data: List[Dict]):\n    \"\"\"\n    Upserts data into the specified index in Pinecone.\n\n    Args:\n        data (List[Dict]): The data to upsert into Pinecone.\n\n    Raises:\n        ValueError: If the index has not been created before upserting data.\n    \"\"\"\n    data = self.convert(data)\n    if not self.index_name:\n        raise ValueError(\"Index must be created before upserting data\")\n    self.pc.Index(name=self.index_name).upsert(data)\n</code></pre> <p>This method: - Converts the input data using the <code>convert</code> method - Checks if an index has been created before attempting to upsert data - Upserts the converted data into the specified Pinecone index</p>"},{"location":"blog/2025/02/25/vector%20database/#using-your-custom-adapter","title":"Using Your Custom Adapter","text":"<p>Once you've created your adapter, you can use it with EmbedAnything like this:</p> <pre><code># Initialize the PineconeEmbedder class\napi_key = os.environ.get(\"PINECONE_API_KEY\")\nindex_name = \"anything\"\npinecone_adapter = PineconeAdapter(api_key)\n\n# Delete existing index if it exists\ntry:\n    pinecone_adapter.delete_index(\"anything\")\nexcept:\n    pass\n\n# Create a new index\npinecone_adapter.create_index(dimension=512, metric=\"cosine\")\n\n# Initialize the embedding model\nmodel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Bert, \n    model_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n)\n\n# Embed a PDF file\ndata = embed_anything.embed_file(\n    \"test-file\",\n    embedder=model,\n    adapter=pinecone_adapter,\n)\n\n# Embed all images in a directory\ndata = embed_anything.embed_image_directory(\n    \"test_files\",\n    embedder=model,\n    adapter=pinecone_adapter\n)\n\nprint(data)\n</code></pre>"},{"location":"blog/2025/02/25/vector%20database/#conclusion","title":"Conclusion","text":"<p>Creating custom adapters for EmbedAnything allows you to seamlessly integrate the library with your preferred storage solutions. By following the step-by-step guide and best practices outlined in this blog post, you can create robust and efficient adapters that enhance your embedding workflow.</p> <p>Remember, the key to a good adapter is clear documentation, robust error handling, and efficient data conversion. With these principles in mind, you can extend EmbedAnything to work with virtually any storage solution.</p> <p>Happy coding!</p>"},{"location":"blog/2024/12/31/colpali-vision-rag/","title":"Optimize VLM Tokens with EmbedAnything x ColPali","text":"<p>ColPali, a late-interaction vision model, leverages this power to enable text searches within images. This means you can pinpoint the exact pages in a PDF containing relevant text, even if the text exists only as part of an image. For example, suppose you have hundreds of pages in a PDF and even hundreds of PDFs. In that case, ColPali can identify the specific pages matching a query\u2014an impressive feat for streamlining information retrieval. This system is widely come to be known as Vision RAG. </p> <p></p> <p>However, due to its computational demands, running the ColPali model directly on a local machine might not always be feasible. To address this, we developed a quantized version of ColPali. Quantization reduces the precision of the model's weights, significantly lowering computational and memory requirements. Despite this optimization, the quantized model maintains performance nearly equivalent to the original.</p>"},{"location":"blog/2024/12/31/colpali-vision-rag/#what-is-vision-rag","title":"What is Vision RAG?","text":"<p>Let\u2019s look a bit deeper into what Vision RAG is. Traditional RAG methods use text throughout the pipeline. They store text chunks and their embeddings in a vector database and then retrieve these chunks for further downstream tasks. A simplest / naive RAG attaches these chunks as context to the original query and aims to provide more information to the model. There are two problems here. One is that getting text from many data sources may not be possible. Think about scanned PDFs or documents with many graphics, like design pamphlets, etc. The traditional RAG falls apart if any documents you work with are like this. A bandaid to the problem is to use OCR engines to somehow extract text. This adds additional moving parts to the process, and OCR engines are pretty fragile.  The second problem, even if you manage to get the text, is the chunking process. Again, how do you decide what the chunk size should be and what the overlap should be? Even if you find optimal parameters for a few documents, will they hold for new ones? All these parameters add to the design space, and the RAG performance needs to be continuously evaluated based on these design choices. Vision RAG tries to solve this by removing the whole chunking process from the system and instead storing the image as a multi-vector embedding in the database. When there is a query, a Late Interaction Score (LIS), similar to the classical cosine similarity but for multi-vector, is measured, and the DB returns the document pages with the highest LIS scores. These documents can now be sent to a Vision Language Model (VLM) along with the original query to get the answer to the questions. The image below shows this process from start to end. Since vision language models are more expensive than text models, Vision RAG is even more important because you don\u2019t have to send complete PDFs to the model. You are just sending the relevant pages. This can save a lot of costs. The document embedding generation happens offline and is taken care of by EmbedAnything. One drawback with this approach is that not all vector databases today support storing multi-vectors. A few that support these are Qdrant and Vespa. </p> <p></p> <p>Let us look at how you can use Colpali models with EmbedAnything and convert PDFs into multi-vector embeddings. In this example, we will not use a vector database but find the late interaction score of the query against all the pages. </p>"},{"location":"blog/2024/12/31/colpali-vision-rag/#step-1-install-the-dependencies","title":"Step 1: Install the dependencies","text":"<p>Since we are going to convert pdfs into images, we need poppler-utils. </p> <p>EmbedAnything requires poppler to convert pdfs to images. So make sure you have it installed.</p> <ul> <li>For Linux:</li> </ul> <pre><code>apt install poppler-utils\n</code></pre> <ul> <li>For Mac</li> </ul> <pre><code>brew install poppler\n</code></pre> <ul> <li>For Windows</li> </ul> <p>https://github.com/oschwartz10612/poppler-windows/releases/tag/v24.08.0-0 Download the binary from here, unzip it and add the <code>bin</code> folder to your system path.</p> <p>Using the GPU version of EmbedAnything is highly recommended because ColPali is based on paligemma and requires a computation like any other small language model. </p> <pre><code>pip install embed-anything-gpu tabulate openai\n</code></pre> <p>Let\u2019s import EmbedAnything and the other dependencies:</p> <pre><code>import base64\nfrom embed_anything import EmbedData, ColpaliModel\nimport numpy as np\nfrom tabulate import tabulate\nfrom pathlib import Path\nfrom PIL import Image\nimport io\nimport matplotlib.pyplot as plt\nimport openai\nimport os\n</code></pre>"},{"location":"blog/2024/12/31/colpali-vision-rag/#step-2-get-the-files-that-need-to-be-indexed","title":"Step 2: Get the files that need to be indexed","text":"<p>For this demo, we will clone the EmbedAnything repo which has some test pdfs with the \u201cAttention is all you need\u201d and a Mistral paper. </p> <pre><code>if not os.path.exists(\"EmbedAnything\"):\n  !git clone https://github.com/StarlightSearch/EmbedAnything.gi\n</code></pre>"},{"location":"blog/2024/12/31/colpali-vision-rag/#step-3-load-the-colpali-onnx-model","title":"Step 3 : Load the ColPali Onnx Model","text":"<p>Use the <code>embed_anything</code> function with <code>from_pretrained_onnx</code> to load the ColPali Onnx model from the specified link. This initializes the model for embedding tasks. If you are using a python notebook, this can take some time because the model is being downloaded. Unfortunately, the progress bar is not visible on a notebook. You can also load the original Colpali model and not the <code>onnx</code> model using the <code>from_pretrained_hf</code> function. </p> <pre><code>model: ColpaliModel = ColpaliModel.from_pretrained_onnx(\"starlight-ai/colpali-v1.2-merged-onnx\", None)\n</code></pre>"},{"location":"blog/2024/12/31/colpali-vision-rag/#step-4-load-the-files-and-embed-them","title":"Step 4: Load the files and embed them.","text":"<p>Now, we just load all the files from the directory with a PDF extension. Then, for each file, we run the <code>embed_file</code> function with a <code>batch_size</code> of 1. You can increase the batch size if you have higher VRAM, but one works well. </p> <pre><code>directory = Path(\"EmbedAnything/test_files\")\nfiles = list(directory.glob(\"*.pdf\"))\nfile_embed_data: list[EmbedData] = []\nfor file in files:\n    try:\n        embedding: list[EmbedData] = model.embed_file(str(file), batch_size=1)\n        file_embed_data.extend(embedding)\n    except Exception as e:\n        print(f\"Error embedding file {file}: {e}\")\nfile_embeddings = np.array([e.embedding for e in file_embed_data])\nprint(\"Embedded Files: \", files)\n</code></pre> <p><code>file_embeddings</code> is a list of <code>EmbedData</code> object which contains other metadata along with the embeddings like page number, file name and the image of the page in string base64 format. You can now store these embeddings in a vector database of choice. </p>"},{"location":"blog/2024/12/31/colpali-vision-rag/#step-5-process-the-query","title":"Step 5: Process the query","text":"<p>We do the same for the query as well using <code>embed_query</code> function. </p> <pre><code>query = \"What is positional encoding?\"\nquery_embedding = model.embed_query(query)\nquery_embeddings = np.array([e.embedding for e in query_embedding])\n</code></pre>"},{"location":"blog/2024/12/31/colpali-vision-rag/#step-6-compute-similarity-scores","title":"Step 6: Compute Similarity Scores","text":"<p>We can calculate the Late Interaction Score between query and file embeddings using the Einstein summation function. This identifies the most relevant pages based on the highest scores. Extract the top 3 pages for further processing. We also take out the <code>image</code> field from the <code>EmbedData</code> object of the embeddings. This is a base64 string representation of the image that will send to GPT. </p> <pre><code>def score(query_embeddings, file_embed_data):\n    file_embeddings = np.array([e.embedding for e in file_embed_data])\n    scores = np.einsum(\"bnd,csd-&gt;bcns\", query_embeddings, file_embeddings).max(axis=3).sum(axis=2).squeeze()\n\n    # Get top pages\n    top_pages = np.argsort(scores)[::-1][:3]\n\n    # Extract file names and page numbers\n    table = [\n        [file_embed_data[page].metadata[\"file_path\"].split(\"/\")[-1], file_embed_data[page].metadata[\"page_number\"]]\n        for page in top_pages\n    ]\n\n    # Print the results in a table\n    print(tabulate(table, headers=[\"File Name\", \"Page Number\"], tablefmt=\"grid\"))\n    results_str = tabulate(table, headers=[\"File Name\", \"Page Number\"], tablefmt=\"grid\")\n\n    images = [file_embed_data[page].metadata[\"image\"] for page in top_pages]\n    images_pil = [Image.open(io.BytesIO(base64.b64decode(image))) for image in images]\n    return images_pil, results_str, images_str\n</code></pre> <p>The result will look something like this:</p> <pre><code>+----------------------------------------+---------------+\n| File Name                              |   Page Number |\n+========================================+===============+\n| EmbedAnything/test_files/attention.pdf |             6 |\n+----------------------------------------+---------------+\n| EmbedAnything/test_files/attention.pdf |             9 |\n+----------------------------------------+---------------+\n| EmbedAnything/test_files/linear.pdf    |            34 |\n+----------------------------------------+---------------+\n| EmbedAnything/test_files/attention.pdf |             3 |\n+----------------------------------------+---------------+\n| EmbedAnything/test_files/attention.pdf |            15 |\n+----------------------------------------+---------------+\n</code></pre> <p>We can visualize the top 3 pages using this command </p> <p></p>"},{"location":"blog/2024/12/31/colpali-vision-rag/#step-7-sent-these-images-to-openai","title":"Step 7: Sent these images to OpenAI","text":"<p>Now we can send these top 3 retrieved images to OpenAI gpt-4o-mini model along with the original query. You can add further instructions for the model here as per your needs. Don\u2019t forget to add your OpenAI key to the client. </p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(api_key = &lt;openai-key&gt; )\n\nimage_contents = [\n    {\n        \"type\": \"image_url\",\n        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_str}\"}\n    }\n    for image_str in images_str\n]\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": query},\n            ] + image_contents,\n        }\n    ],\n\n)\n</code></pre> <p>The output looks like this</p> <pre><code>Positional encoding is a critical concept in transformer models, which addresses the inherent limitation of self-attention mechanisms: they do not consider the order of input tokens. Since transformers process all tokens simultaneously, they require a way to encode the order of tokens in a sequence to maintain their relative positions.\n\n### Key Aspects of Positional Encoding:\n\n1. **Purpose**: It helps the model understand the sequence of data since transformers lack recurrence or convolution that traditionally encode this information.\n\n2. **Method**: \n   - Positional encodings are added to the input embeddings of tokens.\n   - A common approach is to use sine and cosine functions of different frequencies, defined mathematically as:\n\n     \\[\n     PE(pos, 2i) = \\sin\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n     \\]\n     \\[\n     PE(pos, 2i+1) = \\cos\\left(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}}\\right)\n     \\]\n\n   - Here, \\( pos \\) is the position of the token, \\( i \\) is the dimension, and \\( d_{model} \\) is the dimensionality of the embedding.\n\n3. **Frequency**: The functions allow for various wavelengths, making it possible to learn relationships at different scales, which enables the model to understand both short-range and long-range dependencies in the sequence.\n\n4. **Alternatives**: While sinusoidal encodings are widely used, learned positional embeddings can also be employed, which allows the model to learn the optimal way to encode positions during training.\ny\nIn summary, positional encoding is vital for allowing transformer models to grasp the order of tokens in sequences, facilitating effective learning from sequential data.\n</code></pre> <p>This response used a total of 2500 tokens which translates to $0.006. If we would have sent the entire <code>pdf</code> of 15 pages, without retrieval to the model, it would have cost about 12,500 tokens which is five times higher than this system. And this is assuming we know which <code>pdf</code> to send. Also the response may not be accurate because the model has too much unnecessary information to filter out.</p> <p>Check out the demo notebook at </p> <p></p>"},{"location":"blog/2025/09/15/semantic-late-chunking/","title":"How to Configure TextEmbedConfig in EmbedAnything","text":"<p>After presenting at Google, PyCon DE, Berlin Buzzwords, and GDG Berlin, I was surprised by how many people approached me with questions about writing configurations, chunk sizes, and batch sizes for EmbedAnything. Since I had never specifically covered this topic in my talks or blog posts, I decided to create this comprehensive guide to clarify these concepts and explain how we handle your chunking strategy with vector streaming.</p>"},{"location":"blog/2025/09/15/semantic-late-chunking/#understanding-textembedconfig","title":"Understanding TextEmbedConfig","text":"<p>TextEmbedConfig consists of three essential components that work together to optimize your text embedding process:</p> <ol> <li>The embedding model - defines how text is converted to vectors</li> <li>Splitting strategy with chunk size - determines how documents are divided</li> <li>Batch size - controls vector streaming performance</li> </ol> <p>Let's explore each component in detail.</p>"},{"location":"blog/2025/09/15/semantic-late-chunking/#setting-up-the-embedding-model","title":"Setting Up the Embedding Model","text":"<p>The foundation of any embedding configuration is the model itself. Here's how to initialize an embedding model:</p> <pre><code>model = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Bert, \n    model_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n)\n</code></pre> <p>This example uses a BERT-based model from Hugging Face, but you can choose from various architectures depending on your specific needs.</p>"},{"location":"blog/2025/09/15/semantic-late-chunking/#splitting-strategies-and-chunk-size","title":"Splitting Strategies and Chunk Size","text":"<p>EmbedAnything offers multiple splitting strategies, each designed for different use cases. The two primary approaches are semantic chunking and sentence-based splitting.</p>"},{"location":"blog/2025/09/15/semantic-late-chunking/#semantic-chunking","title":"Semantic Chunking","text":"<p>Semantic chunking groups similar content together based on meaning rather than arbitrary boundaries. This approach requires a semantic encoder to determine content similarity.</p> <p>First, set up your semantic encoder:</p> <pre><code>semantic_encoder = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Jina, \n    model_id=\"jinaai/jina-embeddings-v2-small-en\"\n)\n</code></pre> <p>Then configure TextEmbedConfig with semantic chunking:</p> <pre><code>config = TextEmbedConfig(\n    chunk_size=1000,\n    batch_size=32,\n    splitting_strategy=\"semantic\",\n    semantic_encoder=semantic_encoder\n)\n</code></pre>"},{"location":"blog/2025/09/15/semantic-late-chunking/#late-chunking-strategy","title":"Late Chunking Strategy","text":"<p>Late chunking is an advanced technique that preserves contextual relationships throughout the entire document during the embedding process. The document is embedded as a whole first, then divided into chunks while maintaining rich contextual information.</p> <pre><code>config = TextEmbedConfig(\n    chunk_size=1000,\n    batch_size=8,\n    splitting_strategy=\"sentence\",\n    late_chunking=True\n)\n</code></pre> <p>Key benefits of late chunking: - Maintains contextual relationships across the entire document - Produces more meaningful embeddings for each chunk - Particularly effective for longer documents with complex relationships</p>"},{"location":"blog/2025/09/15/semantic-late-chunking/#understanding-key-configuration-parameters","title":"Understanding Key Configuration Parameters","text":""},{"location":"blog/2025/09/15/semantic-late-chunking/#chunk-size","title":"Chunk Size","text":"<p>The <code>chunk_size</code> parameter defines the maximum number of characters (or tokens, depending on the model) allowed in each chunk. Consider these factors when setting chunk size:</p> <ul> <li>Smaller chunks: Better for precise retrieval, more granular search results</li> <li>Larger chunks: Better for maintaining context, fewer total chunks to process</li> <li>Model limitations: Ensure chunk size doesn't exceed your embedding model's maximum input length</li> </ul>"},{"location":"blog/2025/09/15/semantic-late-chunking/#batch-size-for-vector-streaming","title":"Batch Size for Vector Streaming","text":"<p>Batch size controls how many chunks the embedding model processes simultaneously. This directly impacts performance and memory usage:</p> <pre><code># Conservative approach for limited resources\nconfig = TextEmbedConfig(chunk_size=1000, batch_size=8, splitting_strategy=\"sentence\")\n\n# Aggressive approach for high-performance systems\nconfig = TextEmbedConfig(chunk_size=1000, batch_size=32, splitting_strategy=\"semantic\")\n</code></pre> <p>Choosing the right batch size: - Smaller batches (4-8): Lower memory usage, more stable processing - Larger batches (16-32): Faster processing, higher memory requirements - Experimentation is key: Test different batch sizes with your specific documents and hardware</p>"},{"location":"blog/2025/09/15/semantic-late-chunking/#splitting-strategy-options","title":"Splitting Strategy Options","text":"<p>EmbedAnything supports several splitting strategies:</p> <ul> <li>\"semantic\": Groups content by meaning (requires semantic encoder)</li> <li>\"sentence\": Splits at sentence boundaries</li> <li>Custom strategies: Can be implemented for specialized use cases</li> </ul>"},{"location":"blog/2025/09/15/semantic-late-chunking/#putting-it-all-together","title":"Putting It All Together","text":"<p>Here's a complete example showing different configuration approaches:</p> <pre><code># Basic semantic chunking configuration\nsemantic_encoder = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Jina, \n    model_id=\"jinaai/jina-embeddings-v2-small-en\"\n)\n\nconfig_semantic = TextEmbedConfig(\n    chunk_size=1000,\n    batch_size=16,\n    splitting_strategy=\"semantic\",\n    semantic_encoder=semantic_encoder\n)\n\n# Late chunking configuration for complex documents\nconfig_late_chunking = TextEmbedConfig(\n    chunk_size=1500,\n    batch_size=8,\n    splitting_strategy=\"sentence\",\n    late_chunking=True\n)\n\n# Simple sentence-based chunking\nconfig_simple = TextEmbedConfig(\n    chunk_size=800,\n    batch_size=24,\n    splitting_strategy=\"sentence\"\n)\n</code></pre>"},{"location":"blog/2025/09/15/semantic-late-chunking/#performance-optimization-tips","title":"Performance Optimization Tips","text":"<ol> <li>Start with default values (chunk_size=1000, batch_size=8) and adjust based on your specific use case</li> <li>Monitor memory usage when increasing batch size</li> <li>Consider your documents' structure when choosing splitting strategy</li> <li>Test retrieval quality with different chunk sizes</li> <li>Profile your pipeline to find the optimal batch size for your hardware</li> </ol>"},{"location":"blog/2025/09/15/semantic-late-chunking/#best-practices","title":"Best Practices","text":"<ul> <li>Use semantic chunking for documents where meaning preservation is crucial</li> <li>Implement late chunking for complex documents with intricate relationships</li> <li>Adjust batch size based on your available memory and processing requirements</li> <li>Regularly evaluate your configuration's impact on both performance and retrieval quality</li> </ul> <p>By understanding these configuration options and their trade-offs, you can optimize EmbedAnything for your specific use case, whether you're processing technical documentation, literary texts, or any other type of content that requires intelligent chunking and embedding.</p>"},{"location":"blog/2024/03/31/embed-anything/","title":"About Embed Anything","text":"<p>EmbedAnything is an open-source Rust/Python framework that lets you generate vector embeddings for any data (text, images, audio) with minimal code. It's blazing fast, memory-efficient, and can handle massive datasets through vector streaming - meaning you can process 10GB+ files without running out of RAM. Whether you're building a search engine or recommendation system, you can start with <code>pip install embed-anything</code> and a few lines of Python. </p> <p></p>"},{"location":"blog/2024/03/31/embed-anything/#introduction","title":"Introduction","text":"<p>Embedding models are essential today. They have become extremely important due to their use in Retrieval-Augmented Generation (RAG), Image Search, Recommendation Systems, and many other applications. EmbedAnything provides a way to embed data from different modalities ranging from audio, images and text in an fast and efficient way. The library is completely written in Rust and leverages the Huggingface Candle library to serve highly performant embedding models like Jina, ColPali, and others. The best part is that there is no dependence on PyTorch or libtorch, making deploying your applications very easy. In this article, we will look at more features EmbedAnything offers. But first, let's see what motivated us to make this library. </p>"},{"location":"blog/2024/03/31/embed-anything/#motivation","title":"Motivation","text":"<p>The AI landscape today is fantastic. New cutting-edge models pop up almost every month. They are more efficient than ever and have excellent capabilities. However, deploying these large models is quite a pain, and there are several frameworks like VLLM, LitServe, and more to make serving LLMS easy. However, most of these solutions do not provide a way to efficiently serve embedding models. Embedding models are challenging because they require a lot of pre-processing even to start the embedding process. For example, embedding a PDF requires extracting the text, chunking it,  embedding it, adding the required metadata, and then pushing these embeddings to a vector database. Solutions like Ollama and FastEmbed exist, which provide embedding features but have drawbacks. Other solutions require PyTorch or Libtorch, which makes the application footprint quite heavy, and thus, deployment is more complicated. Moreover, currently, there are no existing solutions to extract embeddings and custom metadata from various file formats. LangChain offers some solutions, but it is a bulky package, and extracting only the embedding data is difficult. Moreover, LangChain is not very suitable for vision-related tasks. </p> <p>This is where EmbedAnything comes in. It is a lightweight library that allows you to generate embeddings from different file formats and modalities. Currently, EmbedAnything supports text documents, images and audio, with many more formats like video in the pipeline. The idea is to provide an end-to-end solution where you can give the file and get the embeddings with the appropriate metadata.</p> <p>Development of EmbedAnything started with these goals in mind:</p> <ol> <li>Compatibility with Local and Cloud Models: Seamless integration with local and cloud-based embedding models.</li> <li>High-Speed Performance: Fast processing to meet demanding application requirements.</li> <li>Multimodal Capability: Flexibility to handle various modalities.</li> <li>CPU and GPU Compatibility: Performance optimization for both CPU and GPU environments.</li> <li>Lightweight Design: Minimized footprint for efficient resource utilization.</li> </ol>"},{"location":"blog/2024/03/31/embed-anything/#our-solution","title":"Our Solution","text":"<p>Now, let us look at the different ways we are tackling the problems associated with embedding data.</p>"},{"location":"blog/2024/03/31/embed-anything/#keeping-it-local","title":"Keeping it Local","text":"<p>While cloud-based embedding services like OpenAI, Jina, and Mistral offer convenience, many users require the flexibility and control of local embedding models. Here's why local models are crucial for some use cases:</p> <ul> <li>Cost-Effectiveness:\u00a0Cloud services often charge per API call or model usage. Running embeddings locally on your own hardware can significantly reduce costs, especially for projects with frequent or high-volume embedding needs.</li> <li>Data Privacy:\u00a0Certain data, like medical records or financial documents, might be too sensitive to upload to the cloud. Local embedding keeps your data confidential and under your control.</li> <li>Offline Functionality:\u00a0An internet connection isn't always guaranteed. Local models ensure your embedding tasks can run uninterrupted even without an internet connection.</li> </ul>"},{"location":"blog/2024/03/31/embed-anything/#performance","title":"Performance","text":"<p>EmbedAnything is built with Rust. This makes it faster and provides type safety and a much better development experience. But why is speed so crucial in this process?</p> <p>The need for speed:</p> <p>Creating embeddings from files involves two steps that demand significant computational power:</p> <ol> <li>Extracting Text from Files, Especially PDFs:\u00a0Text can exist in different formats such as markdown, PDFs, and Word documents. However, extracting text from PDFs can be challenging and often causes slowdowns. It is especially difficult to extract text in manageable batches as embedding models have a context limit. Breaking the text into paragraphs containing focused information can help. This task is even more compute intensive which using OCR models like Tesseract to extract text.</li> <li>Inferencing on the Transformer Embedding Model:\u00a0The transformer model is usually at the core of the embedding process, but it is known for being computationally expensive. To address this, EmbedAnything utilizes the Candle Framework by Hugging Face, a machine-learning framework built entirely in Rust for optimized performance.</li> </ol>"},{"location":"blog/2024/03/31/embed-anything/#the-benefit-of-rust-for-speed","title":"The Benefit of Rust for Speed","text":"<p>By leveraging Rust for its core functionalities, EmbedAnything offers significant speed advantages: </p> <ul> <li>Rust is Compiled: Unlike Python, Rust compiles directly to machine code, resulting in faster execution.</li> <li>Efficient Memory Management: Working with embedding models requires careful memory usage when handling models and embeddings. Rust's efficient data structures make this possible.</li> <li>True Concurrency: Rust enables genuine multi-threading and asynchronous programming. As illustrated in the image below, we can simultaneously extract text, split content, generate embeddings, and push data to the database like in the Vector Streaming feature discussed below.</li> </ul> <p>The image shows the speed of embedding documents with EmbedAnything compared to other libraries. You can find the source for the benchmark here. </p> <p></p>"},{"location":"blog/2024/03/31/embed-anything/#what-does-candle-bring-to-the-table","title":"What does Candle bring to the table?","text":"<p>Running language models or embedding models locally can be difficult, especially when you want to deploy a product that utilizes these models. If you use the transformers library from Hugging Face in Python, you will depend on PyTorch for tensor operations. This, in turn, depends on Libtorch, meaning you must include the entire Libtorch library with your product. Also, Candle allows inferences on CUDA-enabled GPUs right out of the box.</p>"},{"location":"blog/2024/03/31/embed-anything/#multiple-modalities","title":"Multiple Modalities","text":"<p>EmbedAnything supports different modalities. You can embed text documents like HTML Pages, PDFs, and Markdowns using text embedding models like Jina, AllMiniLM, and others. You can also embed images using CLIP. </p> <p>Audio files can also be embedded using Whisper. The best part about EmbedAnything for audio embedding is that you can connect a text embedding model with Whisper. Using this, you can embed the text that Whisper decodes in parallel. The metadata includes the time stamps of the texts. You can see this in action in this Huggingface Space. </p> <p>Moreover, with EmbedAnything, you can use late-interaction models like ColPali, which remove the need to do OCR or chunking by embedding the PDF pages as a whole and retrieving the relevant PDF and pages against a query. This has enormous potential. For example, you can reduce the number of tokens that a Vision Language Model uses for document reading by retrieving only the valid pages and then showing them to large VLMs like GPT-4o and Gemini Flash. This can save a lot of cost and time. </p>"},{"location":"blog/2024/03/31/embed-anything/#vector-streaming","title":"Vector Streaming","text":"<p>Vector streaming allows you to create an asynchronous chunking and embedding task. We can effectively spawn threads to handle this task using Rust's concurrency patterns and thread safety. This is done using Rust's MPSC (Multi-producer Single Consumer) module, which passes messages between threads. Thus, this creates a stream of chunks passed into the embedding thread with a buffer. Once the buffer is complete, it embeds the chunks and sends the embeddings back to the main thread, where they are sent to the vector database. This ensures time is well spent on a single operation and no bottlenecks. Moreover, only the chunks and embeddings in the buffer are stored in the system memory. They are erased from the memory once moved to the vector database.</p> <p>Thus, it effectively solves the problem by making many tasks done asynchronously and thus improving efficiency. </p> <p></p>"},{"location":"blog/2024/03/31/embed-anything/#real-world-use-cases","title":"Real-world Use Cases","text":"<ul> <li>Vector Streaming: This involves streaming live information from videos by breaking them into frames and generating embeddings of the images using multimodal embedding models like CLIP. This technique is particularly useful for live camera feeds, such as CCTV, to detect malicious activities or traffic violations.</li> <li>Search Applications: We generate embeddings from PDFs and enable direct searches for page numbers based on user queries, utilizing ColPali. All of this is integrated into our pipeline.</li> <li>Classification Problems: Embeddings can be employed for classification tasks and can scale through metric learning, allowing for the easy addition of new classes.</li> <li>RAG Applications: We view EmbedAnything as an ingestion pipeline for vector databases, which can be extensively utilized for Retrieval-Augmented Generation (RAG) in chatbots.</li> </ul>"},{"location":"blog/2024/03/31/embed-anything/#how-to-get-started","title":"How to get started?","text":"<p>To install our library, all you have to do is </p> <pre><code>pip install embed-anythin\n</code></pre> <p>To improve the speed further and to use large models like ColPali, use the GPU version of embed anything by running. </p> <pre><code>pip install embed-anything-gpu\n</code></pre> <p>Then, with just a few lines of code, you can embed any files and directories. </p> <p><pre><code>model = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Bert, model_id=\"model link from huggingface\"\n)\nconfig = TextEmbedConfig(chunk_size=200, batch_size=32)\ndata = embed_anything.embed_file(\"file_address\", embedder=model, config=config)\n</code></pre> You can check out the documentation at  https://starlight-search.com/references/ </p>"},{"location":"blog/2024/03/31/embed-anything/#whats-next","title":"What\u2019s Next","text":"<p>Future work includes expanding our modalities and vector streaming adapters. We currently support unstructured sources like images, audio, and texts from different sources like PDFs, markdown, and jpegs, but we would also like to expand to graph embeddings and video embeddings. We currently support Weaviate and Elastic Cloud to stream the vectors, which we will expand to other vector databases. </p> <p>Moreover, we will also release methods to fine-tune embedding models easily using Candle, similar to how Sentence Transformers does, but with the speed and memory efficiency of Rust. </p> <p>With this, I would like to conclude this article on this amazing new library that I am building, and I hope to receive some great feedback from the readers. Try it out now and check out the GitHub Repo. </p>"},{"location":"blog/2025/05/01/mcp%2C%20agent/","title":"Easy MCP integration to our agentic framework; LUMO","text":""},{"location":"blog/2025/05/01/mcp%2C%20agent/#building-a-server-with-lumo-a-step-by-step-guide-to-mcp-integration","title":"Building a Server with Lumo: A Step-by-Step Guide to MCP Integration","text":"<p>Lumo, a powerful Rust-based agent, offers seamless integration with MCPs (Modular Control Protocols) and remarkable flexibility in implementation. While Lumo can be used as a library, CLI tool, or server, this guide will focus specifically on deploying Lumo in server mode for optimal MCP integration.</p>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#what-is-an-mcp","title":"What is an MCP?","text":"<p>Modular Control Protocol (MCP) is a standardized communication framework that allows different components of a system to interact efficiently. MCPs enable modular applications to communicate through a structured protocol, making it easier to build scalable, maintainable systems where components can be swapped or upgraded without disrupting the entire architecture.</p>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#architecture-of-mcp","title":"Architecture of MCP","text":"<p>MCP follows a client-server architecture with clearly defined roles:</p> <ul> <li>Hosts: LLM applications (like Claude Desktop or integrated development environments) that initiate connections</li> <li>Clients: Components that maintain one-to-one connections with servers inside the host application</li> <li>Servers: Systems that provide context, tools, and prompts to clients</li> </ul> <p>This architecture is built around three main concepts:</p> <ol> <li>Resources: Similar to GET endpoints, resources load information into the LLM's context</li> <li>Tools: Functioning like POST endpoints, tools execute code or produce side effects</li> <li>Prompts: Reusable templates that define interaction patterns for LLM communications</li> </ol> <p></p>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#setting-up-a-lumo-server-with-mcp-integration","title":"Setting Up a Lumo Server with MCP Integration","text":""},{"location":"blog/2025/05/01/mcp%2C%20agent/#server-usage","title":"\ud83d\udda5\ufe0f Server Usage","text":"<p>Lumo can also be run as a server, providing a REST API for agent interactions.</p>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#starting-the-server","title":"Starting the Server","text":"<pre><code>cargo install --git https://github.com/StarlightSearch/lumo.git --branch new-updates --features mcp lumo-server\n</code></pre>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#using-binary","title":"Using Binary","text":"<pre><code># Start the server (default port: 8080)\nlumo-server\n</code></pre>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#using-docker","title":"Using Docker","text":"<pre><code># Build the image\ndocker build -f server.Dockerfile -t lumo-server .\n\n# Run the container with required API keys\ndocker run -p 8080:8080 \\\n  -e OPENAI_API_KEY=your-openai-key \\\n  -e GOOGLE_API_KEY=your-google-key \\\n  -e GROQ_API_KEY=your-groq-key \\\n  -e ANTHROPIC_API_KEY=your-anthropic-key \\\n  -e EXA_API_KEY=your-exa-key \\\n  lumo-server\n</code></pre> <p>You can also use the pre-built image: <pre><code>docker pull akshayballal95/lumo-server:latest\n</code></pre></p>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#server-configuration","title":"Server Configuration","text":"<p>You can configure multiple servers in the configuration file for MCP agent usage. The configuration file location varies by operating system:</p> <pre><code>Linux: ~/.config/lumo-cli/servers.yaml\nmacOS: ~/Library/Application Support/lumo-cli/servers.yaml\nWindows: %APPDATA%\\Roaming\\lumo\\lumo-cli\\servers.yaml```\n\nExample config: \n\n```exa-search:\n  command: npx\n  args:\n    - \"exa-mcp-server\"\n  env: \n    EXA_API_KEY: \"your-api-key\"\n\nfetch:\n  command: uvx\n  args:\n    - \"mcp_server_fetch\"\n\nsystem_prompt: |-\n  You are a powerful agentic AI assistant...\n</code></pre>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#api-endpoints","title":"API Endpoints","text":""},{"location":"blog/2025/05/01/mcp%2C%20agent/#health-check","title":"Health Check","text":"<pre><code>curl http://localhost:8080/health_check\n</code></pre>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#run-task","title":"Run Task","text":"<pre><code>curl -X POST http://localhost:8080/run \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"task\": \"What are the files in the folder?\",\n    \"model\": \"gpt-4o-mini\",\n    \"base_url\": \"https://api.openai.com/v1/chat/completions\",\n    \"tools\": [\"DuckDuckGo\", \"VisitWebsite\"],\n    \"max_steps\": 5,\n    \"agent_type\": \"mcp\"\n  }'\n</code></pre>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#request-body-parameters","title":"Request Body Parameters","text":"<ul> <li><code>task</code> (required): The task to execute</li> <li><code>model</code> (required): Model ID (e.g., \"gpt-4\", \"qwen2.5\", \"gemini-2.0-flash\")</li> <li><code>base_url</code> (required): Base URL for the API</li> <li><code>tools</code> (optional): Array of tool names to use</li> <li><code>max_steps</code> (optional): Maximum number of steps to take</li> <li><code>agent_type</code> (optional): Type of agent to use (\"function-calling\" or \"mcp\")</li> <li><code>history</code> (optional): Array of previous messages for context</li> </ul>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#mcp-vs-traditional-function-calling","title":"MCP vs. Traditional Function-Calling","text":"<p>While both MCP and traditional function-calling allow LLMs to interact with external tools, they differ in several important ways:</p> <p>The only difference between them is that, in traditional function calling, you need to define the processes and then LLM chooses which is the right option for the given job. It\u2019s main purpose is to translate natural language into JSON format function calls. Meanwhile, MCP is the protocol that standardized the resources and tool calls for the LLM, that is why even though LLM still makes the decision to choose which MCP, it\u2019s the standard calls that makes it highly scalable</p>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#benefits-of-lumo-over-other-agentic-systems","title":"Benefits of lumo over other agentic systems","text":"<ol> <li>MCP Agent Support for multi-agent coordination</li> <li>Multi-modal support, can easily use OpenAI, Google or Anthropic</li> <li>Asynchronous tool calling.</li> <li>In-built Observability with langfuse</li> </ol> <p>Open discussions if you have any doubt and give us a star at repo.</p>"},{"location":"blog/2025/05/01/mcp%2C%20agent/#conclusion","title":"Conclusion","text":"<p>As agents evolve, standardized protocols like MCP will become increasingly important for enabling sophisticated AI applications. By providing a common language for AI systems to interact with external tools and data sources, MCP helps bridge the gap between powerful language models and the specific capabilities needed for real-world applications.</p> <p>For developers working with AI, understanding and adopting MCP offers a more sustainable, future-proof approach to building AI integrations compared to platform-specific function-calling implementations.</p>"},{"location":"blog/2025/04/01/embed-anything/","title":"PyCon Germany","text":"<p>The 2025 PyCon DE event highlighted a growing but cautious interest in AI agents among the Python community. While agent technology received significant attention, many speakers and attendees expressed skepticism about their practical utility in real-world applications.</p> <p>A notable trend was the gap between theoretical potential and actual implementation challenges. Blue Yonder teams, who work in supply chain management, shared how they overcome challenges with deploying agent-based solutions in production environments. Leonardo from Hugging Face presented a compelling mathematical limitation: even with models achieving 90% accuracy, a 20-step agent workflow would result in only about 12% end-to-end accuracy (due to compounding errors at each step). This mathematical reality presents a significant obstacle for complex agent workflows.</p> <p>Alexander Hendorf raised essential questions about the actual use cases where agents provide meaningful value in practical applications, suggesting that the technology might still be searching for its most effective applications.</p> <p>The conference reflected a Python community that remains interested in agent technology but is increasingly focused on practical implementation challenges rather than theoretical possibilities.</p> <p></p>"},{"location":"blog/2025/04/01/embed-anything/#community-culture","title":"Community &amp; Culture","text":"<ul> <li>Valerio Maggio moderated lightning talks described as \"playful chaos\" covering diverse topics.</li> <li>A Fireside Chat emphasized that technology should solve real problems, not exist for its own sake, and contrasted American startup culture with Germany's focus on sustainable development.</li> <li>Feminist AI Lan party, I had a boxing workshop, along with Ines and others. It was fun and frankly much needed after all the serious talks.</li> </ul> <p> The conference reflected a Python community balancing excitement about new technology</p> <p>PyConDE 2025: Building the Future of AI Agents - A Report from the Trenches</p>"},{"location":"blog/2025/04/01/embed-anything/#beyond-agents-what-ai-strategy-really-needs-in-2025","title":"Beyond Agents: What AI Strategy Really Needs in 2025","text":"<p>I was at a talk by Alexander C.S. Hendorf \ud83d\udc4b where he shared his insights on Nvidia GTC and shed light on a bunch of amazing topics , robotics , simulation, and open source. I appreciate some of the points he made regarding the commercialization of open-source software. And a deep question: \"Are agents actually helpful?\"</p>"},{"location":"blog/2025/04/01/embed-anything/#beyond-brute-force-smarter-data-ingestion-for-rag-systems","title":"Beyond Brute Force: Smarter Data Ingestion for RAG Systems","text":"<p>The first talk that really grabbed my attention was \"PDFs - When a thousand words are worth more than a picture (or table).\" The presenter highlighted a critical bottleneck in RAG (Retrieval Augmented Generation) systems: the inherent limitations of PDF parsing. We often take for granted the visual fidelity of PDFs, but their structure presents a real challenge for computers trying to extract meaningful information. Tables and figures, in particular, become nightmares for traditional parsers, leading to unreliable knowledge being fed into vector databases and ultimately, unreliable outputs from our RAG systems.</p> <p>The solution? Embracing multimodal models. The speaker advocated for decomposing tables and figures into plain language descriptions, mimicking how a human would explain them. This approach, focusing on semantics rather than visual representation, promises to significantly improve retrieval accuracy and pave the way for more robust RAG-based agents. The key takeaway here is that building effective agents requires intelligent data ingestion strategies that go beyond simple text extraction.</p>"},{"location":"blog/2025/04/01/embed-anything/#federated-learning-training-agents-in-a-privacy-conscious-world","title":"Federated Learning: Training Agents in a Privacy-Conscious World","text":"<p>Another compelling session, \"The future of AI training is federated,\" addressed the growing need for privacy-preserving AI development. With increasing data privacy regulations (like GDPR) and logistical challenges in centralizing data, federated learning is emerging as a crucial paradigm. This talk provided a practical introduction to building FL systems using the open-source Flower framework.</p> <p>The core concept is simple: instead of bringing the data to the model, we bring the model to the data. This allows us to train AI models on decentralized datasets without compromising sensitive information. The presenter walked us through converting a centralized ML workflow into a federated one, highlighting the specific steps involved in configuring clients, persisting state, and evaluating models. The biggest insight? Federated learning is no longer a niche research area; it's a practical solution for building AI agents that respect data privacy and comply with evolving regulations.</p>"},{"location":"blog/2025/04/01/embed-anything/#mcp-bridging-the-gap-between-agents-and-complex-systems","title":"MCP: Bridging the Gap Between Agents and Complex Systems","text":"<p>\"From Idea to Integration: An Intro to the Model Context Protocol (MCP)\" introduced a powerful standard for connecting Large Language Models with diverse data sources. The Model Context Protocol (MCP) acts as a bridge, enabling agents to interact with complex systems and access real-time data. The talk showcased how to build an MCP server and demonstrated its potential for empowering both developers and non-technical users. Imagine an agent that can access and interpret data from your smart home, your CRM, or any other complex system \u2013 that's the power of MCP.</p> <p>The key takeaway here is that building intelligent agents requires seamless integration with the real world. MCP provides a standardized way to achieve this, opening up a world of possibilities for contextual AI applications.</p>"},{"location":"blog/2025/04/01/embed-anything/#beyond-the-hype-strategic-ai-in-2025-and-beyond","title":"Beyond the Hype: Strategic AI in 2025 and Beyond","text":"<p>The talk \"Beyond Agents: What AI Strategy Really Needs in 2025\" offered a high-level perspective on the evolving AI landscape. Drawing insights from NVIDIA's GTC 2025, the speaker emphasized the convergence of AI with simulation, synthetic data, and robotics. He urged technical leaders to think beyond individual tools and embrace a more holistic approach to AI development.</p> <p>The session highlighted the importance of interdisciplinary collaboration and the rise of powerful, local AI systems. The message was clear: building the future of AI requires strategic thinking, a focus on convergence, and a willingness to collaborate across domains.</p>"},{"location":"blog/2025/04/01/embed-anything/#the-open-source-imperative-building-trust-and-control-in-ai","title":"The Open Source Imperative: Building Trust and Control in AI","text":"<p>Finally, \"The Future of AI: Building the Most Impactful Technology Together\" emphasized the crucial role of open-source principles in AI development. The presenter argued that openness in language models is essential for building trust, mitigating biases, and achieving true alignment. He highlighted the growing momentum of open models and called on the community to build the next generation of AI tools collaboratively.</p> <p>The core message resonated deeply: the future of AI is open. By embracing open-source principles, we can foster innovation, ensure transparency, and build AI systems that are truly beneficial to society.</p>"},{"location":"blog/2025/04/01/embed-anything/#conclusion-a-call-to-action","title":"Conclusion: A Call to Action","text":"<p>PyConDE 2025 left me feeling inspired and energized. The talks I attended highlighted the diverse challenges and exciting opportunities in the field of AI agents. From improving data ingestion to embracing federated learning and championing open-source models, the path forward is clear: building the future of AI requires a collaborative, strategic, and ethical approach.</p>"},{"location":"blog/2025/05/01/observability/","title":"Easy Observability to our agentic framework; LUMO","text":"<p>In the rapidly evolving landscape of AI agents, particularly those employing Large Language Models (LLMs), observability and tracing have emerged as fundamental requirements rather than optional features. As agents become more complex and handle increasingly critical tasks, understanding their inner workings, debugging issues, and establishing accountability becomes paramount.</p>"},{"location":"blog/2025/05/01/observability/#understanding-observability-in-ai-agents","title":"Understanding Observability in AI Agents","text":"<p>Observability refers to the ability to understand the internal state of a system through its external outputs. In AI agents, comprehensive observability encompasses:</p> <ol> <li>Decision Visibility: Transparency into how and why an agent made specific decisions</li> <li>State Tracking: Monitoring the agent's internal state as it evolves throughout task execution</li> <li>Resource Utilization: Measuring computational resources, API calls, and external interactions</li> <li>Performance Metrics: Capturing response times, completion rates, and quality indicators</li> </ol>"},{"location":"blog/2025/05/01/observability/#the-multi-faceted-value-of-tracing-and-observability","title":"The Multi-Faceted Value of Tracing and Observability","text":""},{"location":"blog/2025/05/01/observability/#1-debugging-and-troubleshooting","title":"1. Debugging and Troubleshooting","text":"<p>AI agents, especially those leveraging LLMs, operate with inherent complexity and sometimes unpredictability. Without proper observability:</p> <ul> <li>Silent Failures become common, where agents fail without clear indications of what went wrong</li> <li>Root Cause Analysis becomes nearly impossible as there's no trace of the execution path</li> </ul>"},{"location":"blog/2025/05/01/observability/#2-performance-optimization","title":"2. Performance Optimization","text":"<p>Observability provides crucial insights for optimizing agent performance:</p> <ul> <li>Caching Opportunities: Recognize repeated patterns that could benefit from caching</li> </ul>"},{"location":"blog/2025/05/01/observability/#3-security-and-compliance","title":"3. Security and Compliance","text":"<p>As agents gain more capabilities and autonomy, security becomes increasingly critical:</p> <ul> <li>Audit Trails: Maintain comprehensive logs of all agent actions for compliance and security reviews</li> <li>Prompt Injection Detection: Identify potential attempts to manipulate the agent's behavior</li> </ul>"},{"location":"blog/2025/05/01/observability/#4-user-trust-and-transparency","title":"4. User Trust and Transparency","text":"<p>For end-users working with AI agents, transparency builds trust:</p> <ul> <li>Action Justification: Provide clear explanations for why the agent took specific actions</li> <li>Confidence Indicators: Show reliability metrics for different types of responses</li> </ul>"},{"location":"blog/2025/05/01/observability/#5-continuous-improvement","title":"5. Continuous Improvement","text":"<p>Observability creates a foundation for systematic improvement:</p> <ul> <li>Pattern Recognition: Identify standard failure modes or suboptimal behaviors</li> <li>A/B Testing: Compare different agent configurations with detailed performance metrics</li> </ul>"},{"location":"blog/2025/05/01/observability/#implementing-effective-observability-in-lumo","title":"Implementing Effective Observability in Lumo","text":"<p>For Tracing and Observability</p> <p><pre><code>vim ~/.bashrc\n</code></pre> Add the three keys from Langfuse:</p> <pre><code>LANGFUSE_PUBLIC_KEY_DEV=your-dev-public-key\nLANGFUSE_SECRET_KEY_DEV=your-dev-secret-key\nLANGFUSE_HOST_DEV=http://localhost:3000  # Or your dev Langfuse instance URL\n</code></pre> <p>Start lumo-cli or lumo server then press:</p> <p><pre><code>CTRL + C\n</code></pre> And it\u2019s added to the dashboard</p> <p></p>"},{"location":"blog/2025/05/01/observability/#conclusion","title":"Conclusion","text":"<p>Observability and tracing are no longer optional components for serious AI agent implementations. They form the foundation for reliable, secure, and continuously improving systems. As agents take on more responsibility and autonomy, the ability to observe, understand, and explain their behavior becomes not just a technical requirement but an ethical imperative.</p> <p>Organizations building or deploying AI agents should invest early in robust observability infrastructure, treating it as a core capability rather than an afterthought. The insights gained will improve current systems and also inform the development of better, more trustworthy agents in the future.</p>"},{"location":"blog/2025/01/01/modernBERT/","title":"version 0.5","text":"<p>We are thrilled to share that EmbedAnything version 0.5 is out now and comprise of insane development like support for ModernBert and ReRanker models. Along with Ingestion pipeline support for DocX, and HTML let\u2019s get in details.</p> <p>The best of all have been support for late-interaction model, both ColPali and ColBERT on onnx.</p> <ol> <li>ModernBert Support: Well it made quite a splash, and we were obliged to add it, in the fastest inference engine, embedanything. In addition to being faster and more accurate, ModernBERT also increases context length to 8k tokens (compared to just 512 for most encoders), and is the first encoder-only model that includes a large amount of code in its training data.</li> <li>ColPali- Onnx : \u00a0Running the ColPali model directly on a local machine might not always be feasible. To address this, we developed a\u00a0quantized version of ColPali. Find it on our hugging face, link here. You could also run it both on Candle and on ONNX.</li> <li>ColBERT: ColBERT is a\u00a0fast\u00a0and\u00a0accurate\u00a0retrieval model, enabling scalable BERT-based search over large text collections in tens of milliseconds.</li> <li>ReRankers: EmbedAnything recently contributed for the support of reranking models to Candle so as to add it in our own library. It can support any kind of reranking models. Precision meets performance! Use reranking models to refine your retrieval results for even greater accuracy.</li> <li>Jina V3: Also contributed to V3 models, for Jina can seamlessly integrate any V3 model.</li> <li> <p>\ud835\uddd7\ud835\udde2\ud835\uddd6\ud835\uddeb \ud835\udde3\ud835\uddff\ud835\uddfc\ud835\uddf0\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\uddf6\ud835\uddfb\ud835\uddf4</p> <p>Effortlessly extract text from .docx files and convert it into embeddings. Simplify your document workflows like never before!</p> </li> <li> <p>\ud835\udddb\ud835\udde7\ud835\udde0\ud835\udddf \ud835\udde3\ud835\uddff\ud835\uddfc\ud835\uddf0\ud835\uddf2\ud835\ude00\ud835\ude00\ud835\uddf6\ud835\uddfb\ud835\uddf4:</p> </li> </ol> <p>Parsing and embedding HTML documents just got easier!</p> <pre><code>\u2705 Extract rich metadata with embeddings\n\u2705 Handle code blocks separately for better context\n</code></pre> <p>Supercharge your documentation retrieval with these advanced capabilities.</p>"},{"location":"blog/2024/03/31/vector-streaming/","title":"Vector Streaming","text":"<p>Introducing vector streaming in EmbedAnything, a feature designed to optimize large-scale document embedding. By enabling asynchronous chunking and embedding using Rust\u2019s concurrency, it reduces memory usage and speeds up the process. We also show how to integrate it with the Weaviate Vector Database for seamless image embedding and search.</p> <p>In my previous article Supercharge Your Embeddings Pipeline with EmbedAnything, I discussed the idea behind EmbedAnything and how it makes creating embeddings from multiple modalities easy. In this article, I want to introduce a new feature of EmbedAnything called vector streaming and see how it works with Weaviate Vector Database. </p>"},{"location":"blog/2024/03/31/vector-streaming/#what-is-the-problem","title":"What is the problem?","text":"<p>First, let's examine the current problem with creating embeddings, especially in large-scale documents. The current embedding frameworks operate on a two-step process: chunking and embedding. First, the text is extracted from all the files, and chunks/nodes are created. Then, these chunks are fed to an embedding model with a specific batch size to process the embeddings. While this is done, the chunks and the embeddings stay on the system memory. This is not a problem when the files are small, and the embedding dimensions are small. But this becomes a problem when there are many files and you are working with large models and, even worse, multi-vector embeddings. Thus, to work with this, a high RAM is required to process the embeddings. Also, if this is done synchronously, a lot of time is wasted while the chunks are being created, as chunking is not a compute-heavy operation. As the chunks are being made, passing them to the embedding model would be efficient. </p>"},{"location":"blog/2024/03/31/vector-streaming/#our-solution","title":"Our Solution","text":"<p>The solution is to create an asynchronous chunking and embedding task. We can effectively spawn threads to handle this task using Rust's concurrency patterns and thread safety. This is done using Rust's MPSC (Multi-producer Single Consumer) module, which passes messages between threads. Thus, this creates a stream of chunks passed into the embedding thread with a buffer. Once the buffer is complete, it embeds the chunks and sends the embeddings back to the main thread, where they are sent to the vector database. This ensures no time is wasted on a single operation and no bottlenecks. Moreover, only the chunks and embeddings in the buffer are stored in the system memory. They are erased from the memory once moved to the vector database. </p> <p></p>"},{"location":"blog/2024/03/31/vector-streaming/#example-use-case","title":"Example Use Case","text":"<p>Now, let's see this feature in action. </p> <p>With EmbedAnything, streaming the vectors from a directory of files to the vector database is a simple three-step process. </p> <ol> <li> <p>Create an adapter for your vector database: This is a wrapper around the database's functions that allows you to create an index, convert metadata from EmbedAnything's format to the format required by the database, and the function to insert the embeddings in the index. Adapters for the prominent databases are already created and present here: </p> </li> <li> <p>Initiate an embedding model of your choice: You can choose from different local models or even cloud models. The configuration can also be determined to set the chunk size and buffer size for how many embeddings need to be streamed at once. Ideally, this should be as high as possible, but the system RAM limits this. </p> </li> <li> <p>Call the embedding function from EmbedAnything: Just pass the directory path to be embedded, the embedding model, the adapter, and the configuration. </p> </li> </ol> <p>In this example, we will embed a directory of images and send it to the vector databases. </p>"},{"location":"blog/2024/03/31/vector-streaming/#step-1-create-the-adapter","title":"Step 1: Create the Adapter","text":"<p>In EmbedAnything, the adapters are created outside so as to not make the library heavy and you get to choose which database you want to work with. Here is a simple adapter for Weaviate.</p> <pre><code>from embed_anything import EmbedData\nfrom embed_anything.vectordb import Adapter\n\nclass WeaviateAdapter(Adapter):\n    def __init__(self, api_key, url):\n        super().__init__(api_key)\n        self.client = weaviate.connect_to_weaviate_cloud(\n            cluster_url=url, auth_credentials=wvc.init.Auth.api_key(api_key)\n        )\n        if self.client.is_ready():\n            print(\"Weaviate is ready\")\n\n    def create_index(self, index_name: str):\n        self.index_name = index_name\n        self.collection = self.client.collections.create(\n            index_name, vectorizer_config=wvc.config.Configure.Vectorizer.none()\n        )\n        return self.collection\n\n    def convert(self, embeddings: List[EmbedData]):\n        data = []\n        for embedding in embeddings:\n            property = embedding.metadata\n            property[\"text\"] = embedding.text\n            data.append(\n                wvc.data.DataObject(properties=property, vector=embedding.embedding)\n            )\n        return data\n\n    def upsert(self, embeddings):\n        data = self.convert(embeddings)\n        self.client.collections.get(self.index_name).data.insert_many(data)\n\n    def delete_index(self, index_name: str):\n        self.client.collections.delete(index_name)\n\n### Start the client and index\n\nURL = \"your-weaviate-url\"\nAPI_KEY = \"your-weaviate-api-key\"\nweaviate_adapter = WeaviateAdapter(API_KEY, URL)\n\nindex_name = \"Test_index\"\nif index_name in weaviate_adapter.client.collections.list_all():\n    weaviate_adapter.delete_index(index_name)\nweaviate_adapter.create_index(\"Test_index\")\n</code></pre>"},{"location":"blog/2024/03/31/vector-streaming/#step-2-create-the-embedding-model","title":"Step 2: Create the Embedding Model","text":"<p>Here, since we are embedding images, we can use the clip model </p> <pre><code>import embed_anything import WhichModel\n\nmodel = embed_anything.EmbeddingModel.from_pretrained_hf(\n    embed_anything.WhichModel.Clip,\n    model_id=\"openai/clip-vit-base-patch16\",\n    # revision=\"refs/pr/15\",\n)\n</code></pre>"},{"location":"blog/2024/03/31/vector-streaming/#step-3-embed-the-directory","title":"Step 3: Embed the Directory","text":"<pre><code>data= embed_anything.embed_image_directory(\n    \"/content/EmbedAnything/test_files/clip\", embedder=model, adapter=weaviate_adapter\n)\n</code></pre>"},{"location":"blog/2024/03/31/vector-streaming/#step-4-query-the-vector-database","title":"Step 4: Query the Vector Database","text":"<pre><code>query_vector = embed_anything.embed_query([\"image of a cat\"], embedder=model)[0].embedding\n</code></pre>"},{"location":"blog/2024/03/31/vector-streaming/#step-5-query-the-vector-database","title":"Step 5: Query the Vector Database","text":"<pre><code>response = weaviate_adapter.collection.query.near_vector(\n    near_vector=query_vector,\n    limit=2,\n    return_metadata=wvc.query.MetadataQuery(certainty=True),\n)\n</code></pre> <p>Check the response;</p> <p></p> <p>Check out the notebook for the code here on colab</p> <p></p>"},{"location":"blog/2024/03/31/vector-streaming/#conclusion","title":"Conclusion","text":"<p>We think vector streaming is one of the features that will empower many engineers to opt for a more optimized and no-tech debt solution. Instead of using bulky frameworks on the cloud, you can use a lightweight streaming option. Please don't forget to give us a \u2b50 on our GitHub repo over here: EmbedAnything Repo</p>"},{"location":"guides/Qwen/","title":"Using Qwen","text":"<p>This example leverages the Qwen3-Embedding model, \ud835\ude34\ud835\ude31\ud835\ude26\ud835\ude24\ud835\ude2a\ud835\ude27\ud835\ude2a\ud835\ude24\ud835\ude22\ud835\ude2d\ud835\ude2d\ud835\ude3a \ud835\ude25\ud835\ude26\ud835\ude34\ud835\ude2a\ud835\ude28\ud835\ude2f\ud835\ude26\ud835\ude25 \ud835\ude27\ud835\ude30\ud835\ude33 \ud835\ude35\ud835\ude26\ud835\ude39\ud835\ude35 \ud835\ude26\ud835\ude2e\ud835\ude23\ud835\ude26\ud835\ude25\ud835\ude25\ud835\ude2a\ud835\ude2f\ud835\ude28, \ud835\ude33\ud835\ude26\ud835\ude35\ud835\ude33\ud835\ude2a\ud835\ude26\ud835\ude37\ud835\ude22\ud835\ude2d, \ud835\ude22\ud835\ude2f\ud835\ude25 \ud835\ude33\ud835\ude26\ud835\ude33\ud835\ude22\ud835\ude2f\ud835\ude2c\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude35\ud835\ude22\ud835\ude34\ud835\ude2c\ud835\ude34. \ud835\ude10\ud835\ude35 \ud835\ude24\ud835\ude30\ud835\ude2e\ud835\ude26\ud835\ude34 \ud835\ude38\ud835\ude2a\ud835\ude35\ud835\ude29 \ud835\ude34\ud835\ude36\ud835\ude31\ud835\ude31\ud835\ude30\ud835\ude33\ud835\ude35 \ud835\ude27\ud835\ude30\ud835\ude33 \ud835\ude30\ud835\ude37\ud835\ude26\ud835\ude33 100 \ud835\ude2d\ud835\ude22\ud835\ude2f\ud835\ude28\ud835\ude36\ud835\ude22\ud835\ude28\ud835\ude26\ud835\ude34, \ud835\ude2a\ud835\ude2f\ud835\ude24\ud835\ude2d\ud835\ude36\ud835\ude25\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude37\ud835\ude22\ud835\ude33\ud835\ude2a\ud835\ude30\ud835\ude36\ud835\ude34 \ud835\ude31\ud835\ude33\ud835\ude30\ud835\ude28\ud835\ude33\ud835\ude22\ud835\ude2e\ud835\ude2e\ud835\ude2a\ud835\ude2f\ud835\ude28 \ud835\ude2d\ud835\ude22\ud835\ude2f\ud835\ude28\ud835\ude36\ud835\ude22\ud835\ude28\ud835\ude26\ud835\ude34. </p>"},{"location":"guides/Qwen3_Reranker/","title":"Qwen3 Reranker in EmbedAnything","text":"<p>The Qwen3 Reranker is a powerful document relevance scoring and ranking model that has been integrated into EmbedAnything. It provides high-quality relevance scores for documents based on user queries, making it ideal for search and retrieval applications.</p>"},{"location":"guides/Qwen3_Reranker/#overview","title":"Overview","text":"<p>The Qwen3 Reranker is based on the Qwen3 architecture and has been optimized with ONNX for efficient inference. It's specifically designed for:</p> <ul> <li>Document Relevance Scoring: Assigning relevance scores to documents based on queries</li> <li>Search Result Reranking: Improving search result quality by reranking candidates</li> <li>Information Retrieval: Enhancing retrieval systems with semantic understanding</li> <li>Question-Answering: Ranking candidate answers by relevance to questions</li> </ul>"},{"location":"guides/Qwen3_Reranker/#key-features","title":"Key Features","text":"<ul> <li>High Quality: State-of-the-art relevance scoring capabilities</li> <li>ONNX Optimized: Fast inference using ONNX Runtime</li> <li>Batch Processing: Efficient handling of multiple queries and documents</li> <li>Flexible Scoring: Both ranked results and raw scores available</li> <li>Easy Integration: Simple Python API for quick implementation</li> </ul>"},{"location":"guides/Qwen3_Reranker/#installation","title":"Installation","text":"<p>The Qwen3 Reranker is included with EmbedAnything. Install the package:</p> <pre><code>pip install embed-anything\n</code></pre> <p>Additional dependencies: <pre><code>pip install onnxruntime  # For ONNX inference\n</code></pre></p>"},{"location":"guides/Qwen3_Reranker/#quick-start","title":"Quick Start","text":""},{"location":"guides/Qwen3_Reranker/#basic-usage","title":"Basic Usage","text":"<pre><code>from embed_anything import Reranker, Dtype\n\n# Qwen3 Reranker requires additional formatting\ndef format_query(query: str, instruction=None):\n    \"\"\"You may add instruction to get better results in specific fields.\"\"\"\n    if instruction is None:\n        instruction = \"Given a web search query, retrieve relevant passages that answer the query\"\n    return f\"&lt;Instruct&gt;: {instruction}\\n&lt;Query&gt;: {query}\\n\"\n\ndef format_document(doc: str):\n    return f\"&lt;Document&gt;: {doc}\"\n\n# Initialize the Qwen3 reranker\nreranker = Reranker.from_pretrained(\n    \"zhiqing/Qwen3-Reranker-0.6B-ONNX\", \n    dtype=Dtype.F32\n)\n\n# Rerank documents for a query\nquery = [\"What is machine learning?\"]\ndocuments = [\n    \"Machine learning is a subset of AI.\",\n    \"The weather is sunny today.\",\n    \"ML algorithms can learn from data.\",\n    \"Pizza is a popular food.\"\n]\n\n# Format query and documents\nquery = [format_query(x) for x in query]\ndocuments = [format_document(x) for x in documents]\n\nresults = reranker.rerank(query, documents, top_k=2)\n\n# Display results\nfor result in results:\n    print(f\"Query: {result.query}\")\n    for doc in result.documents:\n        print(f\"  Rank {doc.rank}: {doc.document}\")\n        print(f\"    Score: {doc.relevance_score:.4f}\")\n</code></pre>"},{"location":"guides/Qwen3_Reranker/#multiple-queries","title":"Multiple Queries","text":"<pre><code># Rerank for multiple queries simultaneously\nqueries = [\n    \"How to make coffee?\",\n    \"What is Python programming?\"\n]\n\ndocuments = [\n    \"Coffee is brewed from beans.\",\n    \"Python is a programming language.\",\n    \"The weather is nice.\",\n    \"Coffee can be made with a French press.\",\n    \"Python is great for beginners.\"\n]\n\n# Format queries and documents\nqueries = [format_query(x) for x in queries]\ndocuments = [format_document(x) for x in documents]\n\nresults = reranker.rerank(queries, documents, top_k=3)\n\nfor result in results:\n    print(f\"Query: {result.query}\")\n    for doc in result.documents:\n        print(f\"  {doc.document} (Score: {doc.relevance_score:.4f})\")\n</code></pre>"},{"location":"guides/Qwen3_Reranker/#custom-scoring","title":"Custom Scoring","text":"<pre><code># Get raw scores for custom processing\nscores = reranker.compute_scores(query, documents, batch_size=4)\n\n# Custom ranking logic\ndoc_scores = list(zip(documents, scores[0]))\ndoc_scores.sort(key=lambda x: x[1], reverse=True)\n\nfor doc, score in doc_scores:\n    print(f\"Score: {score:.4f} | {doc}\")\n</code></pre>"},{"location":"guides/Qwen3_Reranker/#api-reference","title":"API Reference","text":""},{"location":"guides/Qwen3_Reranker/#reranker-class","title":"Reranker Class","text":""},{"location":"guides/Qwen3_Reranker/#rerankerfrom_pretrained","title":"<code>Reranker.from_pretrained()</code>","text":"<p>Loads a pre-trained reranker model.</p> <p>Parameters: - <code>model_id</code> (str): Hugging Face model ID (e.g., \"zhiqing/Qwen3-Reranker-0.6B-ONNX\") - <code>revision</code> (str, optional): Model revision/branch - <code>dtype</code> (Dtype, optional): Model data type (F32, F16, etc.) - <code>path_in_repo</code> (str, optional): Path to model files in repository</p> <p>Returns: - <code>Reranker</code>: Initialized reranker instance</p>"},{"location":"guides/Qwen3_Reranker/#rerank","title":"<code>rerank()</code>","text":"<p>Reranks documents for given queries.</p> <p>Parameters: - <code>query</code> (List[str]): List of query strings - <code>documents</code> (List[str]): List of document strings to rank - <code>top_k</code> (int): Number of top documents to return</p> <p>Returns: - <code>List[RerankerResult]</code>: List of reranking results</p>"},{"location":"guides/Qwen3_Reranker/#compute_scores","title":"<code>compute_scores()</code>","text":"<p>Computes raw relevance scores.</p> <p>Parameters: - <code>query</code> (List[str]): List of query strings - <code>documents</code> (List[str]): List of document strings - <code>batch_size</code> (int): Batch size for processing</p> <p>Returns: - <code>List[List[float]]</code>: Raw relevance scores for each query-document pair</p>"},{"location":"guides/Qwen3_Reranker/#data-structures","title":"Data Structures","text":""},{"location":"guides/Qwen3_Reranker/#rerankerresult","title":"<code>RerankerResult</code>","text":"<pre><code>class RerankerResult:\n    query: str                    # The query string\n    documents: List[DocumentRank] # Ranked documents\n</code></pre>"},{"location":"guides/Qwen3_Reranker/#documentrank","title":"<code>DocumentRank</code>","text":"<pre><code>class DocumentRank:\n    document: str        # Document text\n    relevance_score: float # Relevance score (0.0 to 1.0)\n    rank: int           # Rank position (1-based)\n</code></pre>"},{"location":"guides/Qwen3_Reranker/#model-variants","title":"Model Variants","text":""},{"location":"guides/Qwen3_Reranker/#available-models","title":"Available Models","text":"<ul> <li>zhiqing/Qwen3-Reranker-0.6B-ONNX: 0.6B parameter model, ONNX optimized</li> <li>zhiqing/Qwen3-Reranker-0.6B: Original PyTorch model</li> </ul>"},{"location":"guides/Qwen3_Reranker/#data-types","title":"Data Types","text":"<ul> <li>F32: Full precision (default, best quality)</li> <li>F16: Half precision (faster, slightly lower quality)</li> <li>INT8: 8-bit quantization (fastest, lower quality)</li> </ul>"},{"location":"guides/Qwen3_Reranker/#performance-considerations","title":"Performance Considerations","text":""},{"location":"guides/Qwen3_Reranker/#batch-processing","title":"Batch Processing","text":"<p>For optimal performance when processing multiple documents:</p> <pre><code># Use appropriate batch sizes\nscores = reranker.compute_scores(queries, documents, batch_size=8)\n</code></pre>"},{"location":"guides/Qwen3_Reranker/#memory-usage","title":"Memory Usage","text":"<ul> <li>F32: ~2.4GB memory usage</li> <li>F16: ~1.2GB memory usage</li> <li>INT8: ~600MB memory usage</li> </ul>"},{"location":"guides/Qwen3_Reranker/#speed-vs-quality-trade-offs","title":"Speed vs Quality Trade-offs","text":"<ul> <li>F32: Best quality, slower inference</li> <li>F16: Good quality, balanced performance</li> <li>INT8: Lower quality, fastest inference</li> </ul>"},{"location":"guides/Qwen3_Reranker/#use-cases","title":"Use Cases","text":""},{"location":"guides/Qwen3_Reranker/#1-search-engine-reranking","title":"1. Search Engine Reranking","text":"<pre><code># After vector search, rerank results\nvector_results = vector_search(query, top_k=100)\nreranked_results = reranker.rerank([query], vector_results, top_k=10)\n</code></pre>"},{"location":"guides/Qwen3_Reranker/#2-question-answering","title":"2. Question Answering","text":"<pre><code># Rank candidate answers\nquestion = \"What is the capital of France?\"\ncandidate_answers = [\n    \"Paris is the capital of France.\",\n    \"France is a country in Europe.\",\n    \"The Eiffel Tower is in Paris.\"\n]\n\nranked_answers = reranker.rerank([question], candidate_answers, top_k=1)\nbest_answer = ranked_answers[0].documents[0].document\n</code></pre>"},{"location":"guides/Qwen3_Reranker/#3-document-filtering","title":"3. Document Filtering","text":"<pre><code># Filter documents by relevance threshold\nscores = reranker.compute_scores([query], documents, batch_size=4)\nrelevant_docs = [\n    doc for doc, score in zip(documents, scores[0]) \n    if score &gt; 0.5  # Threshold\n]\n</code></pre>"},{"location":"guides/Qwen3_Reranker/#4-content-recommendation","title":"4. Content Recommendation","text":"<pre><code># Rank content by user query relevance\nuser_interest = \"machine learning\"\ncontent_items = [\n    \"Introduction to ML\",\n    \"Python programming basics\",\n    \"Advanced ML algorithms\",\n    \"Web development tutorial\"\n]\n\nrecommendations = reranker.rerank([user_interest], content_items, top_k=3)\n</code></pre>"},{"location":"guides/Qwen3_Reranker/#best-practices","title":"Best Practices","text":""},{"location":"guides/Qwen3_Reranker/#1-model-selection","title":"1. Model Selection","text":"<ul> <li>Use ONNX models for production (faster inference)</li> <li>Choose data type based on quality vs. speed requirements</li> <li>Consider model size for memory constraints</li> </ul>"},{"location":"guides/Qwen3_Reranker/#2-query-formulation","title":"2. Query Formulation","text":"<ul> <li>Keep queries clear and specific</li> <li>Use natural language (the model understands context)</li> <li>Avoid overly long or complex queries</li> </ul>"},{"location":"guides/Qwen3_Reranker/#3-document-preparation","title":"3. Document Preparation","text":"<ul> <li>Ensure documents are well-formatted</li> <li>Remove irrelevant content before reranking</li> <li>Consider document length (very long documents may affect performance)</li> </ul>"},{"location":"guides/Qwen3_Reranker/#4-performance-optimization","title":"4. Performance Optimization","text":"<ul> <li>Use batch processing for multiple queries</li> <li>Implement caching for repeated queries</li> <li>Consider async processing for web applications</li> </ul>"},{"location":"guides/Qwen3_Reranker/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/Qwen3_Reranker/#common-issues","title":"Common Issues","text":"<ol> <li>Model Loading Errors</li> <li>Ensure ONNX Runtime is installed</li> <li>Check internet connection for model download</li> <li> <p>Verify model ID is correct</p> </li> <li> <p>Memory Issues</p> </li> <li>Use smaller data types (F16, INT8)</li> <li>Reduce batch size</li> <li> <p>Process documents in smaller chunks</p> </li> <li> <p>Performance Issues</p> </li> <li>Use ONNX models</li> <li>Optimize batch sizes</li> <li>Consider hardware acceleration (GPU)</li> </ol>"},{"location":"guides/Qwen3_Reranker/#error-messages","title":"Error Messages","text":"<ul> <li>\"Model not found\": Check model ID and internet connection</li> <li>\"ONNX Runtime error\": Ensure onnxruntime is properly installed</li> <li>\"Memory allocation failed\": Reduce batch size or use smaller data type</li> </ul>"},{"location":"guides/Qwen3_Reranker/#examples","title":"Examples","text":"<p>See the following example files for complete working examples:</p> <ul> <li><code>examples/qwen3_reranker.py</code> - Comprehensive examples</li> <li><code>examples/reranker.py</code> - Basic usage examples</li> <li><code>rust/examples/reranker.rs</code> - Rust implementation examples</li> </ul>"},{"location":"guides/Qwen3_Reranker/#contributing","title":"Contributing","text":"<p>The Qwen3 reranker implementation is part of the EmbedAnything project. Contributions are welcome! See the main project documentation for contribution guidelines.</p>"},{"location":"guides/Qwen3_Reranker/#license","title":"License","text":"<p>The Qwen3 reranker is subject to the same license as the EmbedAnything project. Please refer to the project LICENSE file for details.</p>"},{"location":"guides/actix_server/","title":"EmbedAnything OpenAI-Compatible Server","text":"<p>This server provides an OpenAI-compatible API for generating embeddings using the EmbedAnything library. We choose Actix Server for:</p> <ol> <li>Blazing fast: Consistently ranks among the fastest web frameworks in benchmarks like TechEmpower.</li> <li>Asynchronous by default: Built on Rust\u2019s async/await, enabling efficient I/O-bound workloads.</li> <li>Lightweight &amp; modular: Minimal core with extensible middleware, plugins, and integrations.</li> <li>Type-safe: Strong type guarantees ensure fewer runtime surprises.</li> <li>Production-ready: Stable, mature, and already used in industries like fintech, IoT, and SaaS platforms.</li> </ol> <p>For benchmarks between python and rust servers, you check out this blog: https://www.jonvet.com/blog/benchmarking-python-rust-web-servers</p>"},{"location":"guides/actix_server/#features","title":"Features","text":"<ul> <li>OpenAI-compatible <code>/v1/embeddings</code> endpoint</li> <li>Support for multiple embedding models (Jina, BERT, etc.)</li> <li>Health check endpoint</li> </ul>"},{"location":"guides/actix_server/#running-the-server","title":"Running the Server","text":"<pre><code>cargo run -p server --release\n</code></pre> <p>The server will start on <code>http://0.0.0.0:8080</code>.</p>"},{"location":"guides/actix_server/#api-usage","title":"API Usage","text":""},{"location":"guides/actix_server/#create-embeddings","title":"Create Embeddings","text":"<p>Endpoint: <code>POST /v1/embeddings</code></p> <p>Request: <pre><code>{\n  \"model\": \"sentence-transformers/all-MiniLM-L12-v2\",\n  \"input\": [\"The quick brown fox jumps over the lazy dog\"]\n}\n</code></pre></p> <p>Response: <pre><code>{\n  \"object\": \"list\",\n  \"data\": [\n    {\n      \"object\": \"embedding\",\n      \"index\": 0,\n      \"embedding\": [0.0023064255, -0.009327292, ...]\n    }\n  ],\n  \"model\": \"sentence-transformers/all-MiniLM-L12-v2\",\n  \"usage\": {\n    \"prompt_tokens\": 9,\n    \"total_tokens\": 9\n  }\n}\n</code></pre></p>"},{"location":"guides/actix_server/#health-check","title":"Health Check","text":"<p>Endpoint: <code>GET /health_check</code></p> <p>Returns a 200 OK status if the server is running.</p>"},{"location":"guides/actix_server/#supported-models","title":"Supported Models","text":"<p>The server maps model names to EmbedAnything model architectures:</p> <ul> <li><code>text-embedding-ada-002</code> \u2192 Jina embeddings</li> <li><code>text-embedding-3-small</code> \u2192 Jina embeddings  </li> <li><code>text-embedding-3-large</code> \u2192 Jina embeddings</li> <li><code>text-embedding-ada-001</code> \u2192 BERT embeddings</li> <li>Unknown models \u2192 Default to Jina embeddings</li> </ul>"},{"location":"guides/actix_server/#error-handling","title":"Error Handling","text":"<p>The API returns OpenAI-compatible error responses:</p> <pre><code>{\n  \"error\": {\n    \"message\": \"Error description\",\n    \"type\": \"error_type\",\n    \"code\": \"error_code\"\n  }\n}\n</code></pre>"},{"location":"guides/actix_server/#example-usage-with-curl","title":"Example Usage with curl","text":"<pre><code># Create embeddings\ncurl -X POST http://localhost:8080/v1/embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"sentence-transformers/all-MiniLM-L12-v2\",\n    \"input\": [\"Hello world\", \"How are you?\"]\n  }'\n\n# Health check\ncurl http://localhost:8080/health_check\n</code></pre>"},{"location":"guides/actix_server/#example-usage-with-python","title":"Example Usage with Python","text":"<pre><code>import requests\n\n# Create embeddings\nresponse = requests.post(\n    \"http://localhost:8080/v1/embeddings\",\n    json={\n        \"model\": \"sentence-transformers/all-MiniLM-L12-v2\",\n        \"input\": [\"The quick brown fox jumps over the lazy dog\"]\n    }\n)\n\nif response.status_code == 200:\n    data = response.json()\n    print(f\"Generated {len(data['data'])} embeddings\")\n    print(f\"First embedding dimension: {len(data['data'][0]['embedding'])}\")\nelse:\n    print(f\"Error: {response.json()}\")\n</code></pre>"},{"location":"guides/adapters/","title":"Using Vector Database Adapters","text":""},{"location":"guides/adapters/#using-elasticsearch","title":"Using Elasticsearch","text":"<p>To use Elasticsearch, you need to install the <code>elasticsearch</code> package. <pre><code>pip install elasticsearch\n</code></pre></p> <pre><code>import embed_anything\nimport os\n\nfrom typing import Dict, List\nfrom embed_anything import EmbedData\nfrom embed_anything.vectordb import Adapter\nfrom embed_anything import EmbedData, EmbeddingModel, TextEmbedConfig, WhichModel\n\nfrom elasticsearch import Elasticsearch\nfrom elasticsearch.helpers import bulk\n\n\nclass ElasticsearchAdapter(Adapter):\n\n    def __init__(self, api_key: str, cloud_id: str, index_name: str = \"anything\"):\n        self.es = Elasticsearch(cloud_id=cloud_id, api_key=api_key)\n        self.index_name = index_name\n\n    def create_index(\n        self, dimension: int, metric: str, mappings={}, settings={}, **kwargs\n    ):\n\n        if \"index_name\" in kwargs:\n            self.index_name = kwargs[\"index_name\"]\n\n        self.es.indices.create(\n            index=self.index_name, mappings=mappings, settings=settings\n        )\n\n    def convert(self, embeddings: List[List[EmbedData]]) -&gt; List[Dict]:\n        data = []\n        for embedding in embeddings:\n            data.append(\n                {\n                    \"text\": embedding.text,\n                    \"embeddings\": embedding.embedding,\n                    \"metadata\": {\n                        \"file_name\": embedding.metadata[\"file_name\"],\n                        \"modified\": embedding.metadata[\"modified\"],\n                        \"created\": embedding.metadata[\"created\"],\n                    },\n                }\n            )\n        return data\n\n    def delete_index(self, index_name: str):\n        self.es.indices.delete(index=index_name)\n\n    def gendata(self, data):\n        for doc in data:\n            yield doc\n\n    def upsert(self, data: List[Dict]):\n        data = self.convert(data)\n        bulk(client=self.es, index=\"anything\", actions=self.gendata(data))\n\n\nindex_name = \"anything\"\nelastic_api_key = os.environ.get(\"ELASTIC_API_KEY\")\nelastic_cloud_id = os.environ.get(\"ELASTIC_CLOUD_ID\")\n\n# Initialize the ElasticsearchAdapter Class\nelasticsearch_adapter = ElasticsearchAdapter(\n    api_key=elastic_api_key,\n    cloud_id=elastic_cloud_id,\n    index_name=index_name,\n)\n\n# Prase PDF and insert documents into Elasticsearch.\nmodel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Bert, model_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n)\n\n\ndata = embed_anything.embed_file(\n    \"/home/sonamAI/projects/EmbedAnything/test_files/attention.pdf\",\n    embedder=model,\n    adapter=elasticsearch_adapter\n)\n\n# Create an Index with explicit mappings.\nmappings = {\n    \"properties\": {\n        \"embeddings\": {\"type\": \"dense_vector\", \"dims\": 384},\n        \"text\": {\"type\": \"text\"},\n    }\n}\nsettings = {}\n\nelasticsearch_adapter.create_index(\n    dimension=384,\n    metric=\"cosine\",\n    mappings=mappings,\n    settings=settings,\n)\n\n# Delete an Index\nelasticsearch_adapter.delete_index(index_name=index_name)\n</code></pre>"},{"location":"guides/adapters/#using-weaviate","title":"Using Weaviate","text":"<p>To use Weaviate, you need to install the <code>weaviate-client</code> package.</p> <pre><code>pip install weaviate-client\n</code></pre> <pre><code>import weaviate, os\nimport weaviate.classes as wvc\nfrom tqdm.auto import tqdm\nimport embed_anything\nfrom embed_anything import EmbedData, EmbeddingModel, TextEmbedConfig, WhichModel\nfrom embed_anything.vectordb import Adapter\nimport textwrap\n\n## Weaviate Adapter\n\nfrom typing import List\n\n\nclass WeaviateAdapter(Adapter):\n    def __init__(self, api_key, url):\n        super().__init__(api_key)\n        self.client = weaviate.connect_to_weaviate_cloud(\n            cluster_url=url, auth_credentials=wvc.init.Auth.api_key(api_key)\n        )\n        if self.client.is_ready():\n            print(\"Weaviate is ready\")\n\n    def create_index(self, index_name: str):\n        self.index_name = index_name\n        self.collection = self.client.collections.create(\n            index_name, vectorizer_config=wvc.config.Configure.Vectorizer.none()\n        )\n        return self.collection\n\n    def convert(self, embeddings: List[EmbedData]):\n        data = []\n        for embedding in embeddings:\n            property = embedding.metadata\n            property[\"text\"] = embedding.text\n            data.append(\n                wvc.data.DataObject(properties=property, vector=embedding.embedding)\n            )\n        return data\n\n    def upsert(self, data_):\n        data_ = self.convert(data_)\n        self.client.collections.get(self.index_name).data.insert_many(data_)\n\n    def delete_index(self, index_name: str):\n        self.client.collections.delete(index_name)\n\n\nURL = \"URL\"\nAPI_KEY = \"API_KEY\"\nweaviate_adapter = WeaviateAdapter(API_KEY, URL)\n\n\n# create index\nindex_name = \"Test_index\"\nif index_name in weaviate_adapter.client.collections.list_all():\n    weaviate_adapter.delete_index(index_name)\nweaviate_adapter.create_index(\"Test_index\")\n\n\nmodel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Bert, model_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n)\n\n\ndata = embed_anything.embed_file(\n    \"/home/sonamAI/projects/EmbedAnything/test_files/attention.pdf\",\n    embedder=model,\n    adapter=weaviate_adapter,\n)\n\nquery_vector = embed_anything.embed_query([\"What is attention\"], embedder=model)[\n    0\n].embedding\n\n\nresponse = weaviate_adapter.collection.query.near_vector(\n    near_vector=query_vector,\n    limit=2,\n    return_metadata=wvc.query.MetadataQuery(certainty=True),\n)\n\nfor i in range(len(response.objects)):\n    print(response.objects[i].properties[\"text\"])\n\n\nfor res in response.objects:\n    print(textwrap.fill(res.properties[\"text\"], width=120), end=\"\\n\\n\")\n</code></pre>"},{"location":"guides/adapters/#using-pinecone","title":"Using Pinecone","text":"<p>To use Pinecone, you need to install the <code>pinecone</code> package.</p> <pre><code>pip install pinecone\n</code></pre> <pre><code>import re\nfrom typing import Dict, List\nimport uuid\nimport embed_anything\nimport os\n\nfrom embed_anything.vectordb import Adapter\nfrom pinecone import Pinecone, ServerlessSpec\n\nfrom embed_anything import EmbedData, EmbeddingModel, WhichModel, TextEmbedConfig\n\n\nclass PineconeAdapter(Adapter):\n    \"\"\"\n    Adapter class for interacting with Pinecone, a vector database service.\n    \"\"\"\n\n    def __init__(self, api_key: str):\n        \"\"\"\n        Initializes a new instance of the PineconeAdapter class.\n\n        Args:\n            api_key (str): The API key for accessing the Pinecone service.\n        \"\"\"\n        super().__init__(api_key)\n        self.pc = Pinecone(api_key=self.api_key)\n        self.index_name = None\n\n    def create_index(\n        self,\n        dimension: int,\n        metric: str = \"cosine\",\n        index_name: str = \"anything\",\n        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n    ):\n        \"\"\"\n        Creates a new index in Pinecone.\n\n        Args:\n            dimension (int): The dimensionality of the embeddings.\n            metric (str, optional): The distance metric to use for similarity search. Defaults to \"cosine\".\n            index_name (str, optional): The name of the index. Defaults to \"anything\".\n            spec (ServerlessSpec, optional): The serverless specification for the index. Defaults to AWS in us-east-1 region.\n        \"\"\"\n        self.index_name = index_name\n        self.pc.create_index(\n            name=index_name, dimension=dimension, metric=metric, spec=spec\n        )\n\n    def delete_index(self, index_name: str):\n        \"\"\"\n        Deletes an existing index from Pinecone.\n\n        Args:\n            index_name (str): The name of the index to delete.\n        \"\"\"\n        self.pc.delete_index(name=index_name)\n\n    def convert(self, embeddings: List[EmbedData]) -&gt; List[Dict]:\n        \"\"\"\n        Converts a list of embeddings into the required format for upserting into Pinecone.\n\n        Args:\n            embeddings (List[EmbedData]): The list of embeddings to convert.\n\n        Returns:\n            List[Dict]: The converted data in the required format for upserting into Pinecone.\n        \"\"\"\n        data_emb = []\n\n        for embedding in embeddings:\n            data_emb.append(\n                {\n                    \"id\": str(uuid.uuid4()),\n                    \"values\": embedding.embedding,\n                    \"metadata\": {\n                        \"text\": embedding.text,\n                        \"file\": re.split(\n                            r\"/|\\\\\", embedding.metadata.get(\"file_name\", \"\")\n                        )[-1],\n                    },\n                }\n            )\n        return data_emb\n\n    def upsert(self, data: List[Dict]):\n        \"\"\"\n        Upserts data into the specified index in Pinecone.\n\n        Args:\n            data (List[Dict]): The data to upsert into Pinecone.\n\n        Raises:\n            ValueError: If the index has not been created before upserting data.\n        \"\"\"\n        data = self.convert(data)\n        if not self.index_name:\n            raise ValueError(\"Index must be created before upserting data\")\n        self.pc.Index(name=self.index_name).upsert(data)\n\n\n# Initialize the PineconeEmbedder class\napi_key = os.environ.get(\"PINECONE_API_KEY\")\nindex_name = \"anything\"\npinecone_adapter = PineconeAdapter(api_key)\n\ntry:\n    pinecone_adapter.delete_index(\"anything\")\nexcept:\n    pass\n\n# Initialize the PineconeEmbedder class\n\npinecone_adapter.create_index(dimension=512, metric=\"cosine\")\n\n\nmodel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Bert, model_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n)\n\n\ndata = embed_anything.embed_file(\n    \"/home/sonamAI/projects/EmbedAnything/test_files/attention.pdf\",\n    embedder=model,\n    adapter=pinecone_adapter,\n)\n\n\n\ndata = embed_anything.embed_image_directory(\n    \"test_files\",\n    embedder=model,\n    adapter=pinecone_adapter\n)\nprint(data)\n</code></pre>"},{"location":"guides/adapters/#using-qdrant","title":"Using Qdrant","text":"<p>To use Qdrant, you need to install the <code>qdrant-client</code> package.</p> <pre><code>pip install qdrant-client\n</code></pre> <pre><code>import uuid\nfrom typing import List, Dict\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import (\n    Distance,\n    VectorParams,\n    PointStruct,\n)\nimport embed_anything\nfrom embed_anything import EmbedData, EmbeddingModel, WhichModel\nfrom embed_anything.vectordb import Adapter\n\n\nclass QdrantAdapter(Adapter):\n    \"\"\"\n    Adapter class for interacting with [Qdrant](https://qdrant.tech/).\n    \"\"\"\n\n    def __init__(self, client: QdrantClient):\n        \"\"\"\n        Initializes a new instance of the QdrantAdapter class.\n\n        Args:\n            client : An instance of qdrant_client.QdrantClient\n        \"\"\"\n        self.client = client\n\n    def create_index(\n        self,\n        dimension: int,\n        metric: Distance = Distance.COSINE,\n        index_name: str = \"embed-anything\",\n        **kwargs,\n    ):\n        self.collection_name = index_name\n\n        if not self.client.collection_exists(index_name):\n            self.client.create_collection(\n                collection_name=index_name,\n                vectors_config=VectorParams(size=dimension, distance=metric),\n            )\n\n    def delete_index(self, index_name: str):\n        self.client.delete_collection(collection_name=index_name)\n\n    def convert(self, embeddings: List[EmbedData]) -&gt; List[PointStruct]:\n        points = []\n        for embedding in embeddings:\n            points.append(\n                PointStruct(\n                    id=str(uuid.uuid4()),\n                    vector=embedding.embedding,\n                    payload={\n                        \"text\": embedding.text,\n                        \"file_name\": embedding.metadata[\"file_name\"],\n                        \"modified\": embedding.metadata[\"modified\"],\n                        \"created\": embedding.metadata[\"created\"],\n                    },\n                )\n            )\n        return points\n\n    def upsert(self, data: List[Dict]):\n        points = self.convert(data)\n        self.client.upsert(\n            collection_name=self.collection_name,\n            points=points,\n        )\n\n\ndef main():\n    adapter = QdrantAdapter(QdrantClient(location=\":memory:\"))\n    adapter.create_index(dimension=384)\n\n    model = EmbeddingModel.from_pretrained_hf(\n        WhichModel.Bert, model_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n    )\n\n    embed_anything.embed_file(\n        \"test_files/attention.pdf\",\n        embedder=model,\n        adapter=adapter,\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"guides/adapters/#using-milvus","title":"Using Milvus","text":"<p>To use Milvus, you need to install the <code>pymilvus</code> package.</p> <pre><code>pip install pymilvus\n</code></pre> <pre><code>from pymilvus import MilvusClient, DataType\nimport os\nfrom typing import Dict, List\n\nimport embed_anything\nfrom embed_anything.vectordb import Adapter\nfrom embed_anything import EmbedData, EmbeddingModel, WhichModel\n\nprint(\"Milvus Vector DB - Adapter\")\n\n# Default embedding dimension\nEMBEDDINGS_DIM = 384\n# Maximum VARCHAR field length for text content\nTEXT_CONTENT_VARCHARS = 4098\n\n# Type annotation for embeddings\nVectorEmbeddings = List[List[EmbedData]]\n\nclass MilvusVectorAdapter(Adapter):\n    def __init__(self, uri: str = './milvus.db', token: str = '', collection_name: str = \"embed_anything_collection\"):\n        \"\"\"\n        Initialize the MilvusVectorAdapter.\n\n        Args:\n            uri (str): The URI to connect to, comes in the form of\n                \"https://address:port\" for Milvus or Zilliz Cloud service,\n                or \"path/to/local/milvus.db\" for the lite local Milvus. Defaults to\n                \"./milvus.db\".\n            token (str): The token for log in. Defaults to \"\".\n            collection_name (str): Name of the collection to use. Defaults to\n                \"embed_anything_collection\".\n        \"\"\"\n        self.collection_name = collection_name\n        self.client = MilvusClient(uri=uri, token=token)\n        print(\"Ok - Milvus DB connection established.\")\n\n    def create_index(self, dimension: int = EMBEDDINGS_DIM):\n        \"\"\"\n        Create a collection and index for embeddings.\n\n        Args:\n            dimension: Dimension of the embedding vectors.\n            **kwargs: Additional parameters for index creation.\n        \"\"\"\n        # Delete collection if it exists\n        if self.client.has_collection(self.collection_name):\n            self.delete_index()\n\n        # Create collection schema\n        schema = self.client.create_schema(auto_id=True)\n        schema.add_field(field_name=\"id\", datatype=DataType.INT64, is_primary=True)\n        schema.add_field(\n            field_name=\"embeddings\",\n            datatype=DataType.FLOAT_VECTOR,\n            dim=dimension\n        )\n        schema.add_field(\n            field_name=\"text\",\n            datatype=DataType.VARCHAR,\n            max_length=TEXT_CONTENT_VARCHARS\n        )\n        schema.add_field(\n            field_name=\"file_name\",\n            datatype=DataType.VARCHAR,\n            max_length=255\n        )\n        schema.add_field(\n            field_name=\"modified\",\n            datatype=DataType.VARCHAR,\n            max_length=50\n        )\n        schema.add_field(\n            field_name=\"created\",\n            datatype=DataType.VARCHAR,\n            max_length=50\n        )\n\n        # Create the collection\n        self.client.create_collection(\n            collection_name=self.collection_name,\n            schema=schema\n        )\n\n        # Create the index\n        index_params = self.client.prepare_index_params()\n        index_params.add_index(\n            field_name=\"embeddings\",\n            index_type=\"IVF_FLAT\",\n            metric_type=\"L2\",\n            params={\"nlist\": 1024}\n        )\n\n        # Apply the index\n        self.client.create_index(\n            collection_name=self.collection_name,\n            index_params=index_params\n        )\n\n        # Load the collection\n        self.client.load_collection(\n            collection_name=self.collection_name\n        )\n\n        print(f\"Collection '{self.collection_name}' created with index.\")\n\n    def convert(self, embeddings: List[EmbedData]) -&gt; List[Dict]:\n        \"\"\"\n        Convert EmbedData objects to a format compatible with Milvus.\n\n        Args:\n            embeddings: List of EmbedData objects.\n\n        Returns:\n            List of dictionaries with data formatted for Milvus.\n        \"\"\"\n        ret_data = []\n        for i, embedding in enumerate(embeddings):\n            data_dict = {\n                \"embeddings\": embedding.embedding,\n                \"text\": embedding.text,\n                \"file_name\": embedding.metadata[\"file_name\"],\n                \"modified\": embedding.metadata[\"modified\"],\n                \"created\": embedding.metadata[\"created\"],\n            }\n            ret_data.append(data_dict)\n\n        print(f\"Converted {len(ret_data)} embeddings for insertion.\")\n        return ret_data\n\n    def delete_index(self):\n        \"\"\"\n        Delete the collection and its index.\n        \"\"\"\n        try:\n            self.client.drop_collection(self.collection_name)\n            print(f\"Collection '{self.collection_name}' dropped.\")\n        except Exception as e:\n            print(f\"Failed to drop collection: {e}\")\n\n\n    def upsert(self, data: List[EmbedData]):\n        \"\"\"\n        Insert or update embeddings in the collection.\n\n        Args:\n            data: List of EmbedData objects to insert.\n        \"\"\"\n        # Convert data to Milvus format\n        formatted_data = self.convert(data)\n\n        # Insert data\n        self.client.insert(\n            collection_name=self.collection_name,\n            data=formatted_data\n        )\n\n        print(f\"Successfully inserted {len(formatted_data)} embeddings.\")\n\n\n\n\n\nif __name__ == \"__main__\":\n    # Initialize the MilvusVectorAdapter class\n    index_name = \"embed_anything_milvus_collection\"\n    milvus_adapter = MilvusVectorAdapter(uri='./milvus.db', collection_name=index_name)\n\n    # Delete existing index if it exists\n    try:\n        milvus_adapter.delete_index(index_name)\n    except:\n        pass\n\n    # Create a new index\n    milvus_adapter.create_index()\n\n    # Initialize the embedding model\n    model = EmbeddingModel.from_pretrained_hf(\n        WhichModel.Bert, \n        model_id=\"sentence-transformers/all-MiniLM-L12-v2\"\n    )\n\n    # Embed a PDF file\n    data = embed_anything.embed_file(\n        \"path/to/your/file.pdf\",\n        embedder=model,\n        adapter=milvus_adapter,\n    )\n</code></pre>"},{"location":"guides/colpali/","title":"Using Colpali","text":"<p>This example leverages the ColpaliModel from the EmbedAnything library, specifically designed for high-performance document embedding and semantic search. Colpali supports both native and ONNX formats, making it versatile for fast, efficient model loading.</p> <pre><code>from embed_anything import EmbedData, ColpaliModel\nimport numpy as np\nfrom tabulate import tabulate\nfrom pathlib import Path\n\n\n# Load the model\nmodel: ColpaliModel = ColpaliModel.from_pretrained(\"vidore/colpali-v1.2-merged\", None)\n\n# Load ONNX Model\n# model: ColpaliModel = ColpaliModel.from_pretrained_onnx(\n#     \"starlight-ai/colpali-v1.2-merged-onnx\", None\n# )\n\n# Get all PDF files in the directory\ndirectory = Path(\"test_files\")\nfiles = list(directory.glob(\"*.pdf\"))\n# files = [Path(\"test_files/attention.pdf\")]\n\nfile_embed_data: list[EmbedData] = []\nfor file in files:\n    try:\n        embedding: list[EmbedData] = model.embed_file(str(file), batch_size=1)\n        file_embed_data.extend(embedding)\n    except Exception as e:\n        print(f\"Error embedding file {file}: {e}\")\n\n# Define the query\nquery = \"What are Positional Encodings\"\n\n# Scoring\nfile_embeddings = np.array([e.embedding for e in file_embed_data])\nquery_embedding = model.embed_query(query)\nquery_embeddings = np.array([e.embedding for e in query_embedding])\nprint(file_embeddings.shape)\nprint(query_embeddings.shape)\n\nscores = (\n    np.einsum(\"bnd,csd-&gt;bcns\", query_embeddings, file_embeddings)\n    .max(axis=3)\n    .sum(axis=2)\n    .squeeze()\n)\n\n# Get top pages\ntop_pages = np.argsort(scores)[::-1][:5]\n\n# Extract file names and page numbers\ntable = [\n    [\n        file_embed_data[page].metadata[\"file_path\"],\n        file_embed_data[page].metadata[\"page_number\"],\n    ]\n    for page in top_pages\n]\n\n# Print the results in a table\nprint(tabulate(table, headers=[\"File Name\", \"Page Number\"], tablefmt=\"grid\"))\n\nimages = [file_embed_data[page].metadata[\"image\"] for page in top_pages]\n</code></pre>"},{"location":"guides/images/","title":"Searching Images","text":"<p>This example shows how to use the EmbeddingModel from EmbedAnything to perform semantic image search within a directory, leveraging the CLIP model for accurate, language-guided matching.</p> <pre><code>import numpy as np\nimport embed_anything\nfrom embed_anything import EmbedData\nimport time\n\nstart = time.time()\n\n# Load the model.\nmodel = embed_anything.EmbeddingModel.from_pretrained_hf(\n    embed_anything.WhichModel.Clip,\n    model_id=\"google/siglip-base-patch16-224\",\n)\ndata: list[EmbedData] = embed_anything.embed_image_directory(\n    \"test_files\", embedder=model\n)\n\n# Convert the embeddings to a numpy array\nembeddings = np.array([data.embedding for data in data])\n\n# Embed a query\nquery = [\"Photo of a monkey\"]\nquery_embedding = np.array(\n    embed_anything.embed_query(query, embedder=model)[0].embedding\n)\n\n# Calculate the similarities between the query embedding and all the embeddings\nsimilarities = np.dot(embeddings, query_embedding)\n\n# Find the index of the most similar embedding\nmax_index = np.argmax(similarities)\n\nprint(\"Descending order of similarity: \")\nindices = np.argsort(similarities)[::-1]\nfor idx in indices:\n    print(data[idx].text)\n\nprint(\"----------- \")\n\n# Print the most similar image\nprint(\"Most similar image: \", data[max_index].text)\nend = time.time()\nprint(\"Time taken: \", end - start)\n</code></pre>"},{"location":"guides/images/#supported-models","title":"Supported Models","text":"<p>EmbedAnything supports the following models for image search:</p> <ul> <li>openai/clip-vit-base-patch32</li> <li>openai/clip-vit-base-patch16</li> <li>openai/clip-vit-large-patch14-336</li> <li>openai/clip-vit-large-patch14</li> </ul>"},{"location":"guides/ocr/","title":"Use PDFs that need OCR","text":"<p>Embed Anything can be used to embed scanned documents using OCR. This is useful for tasks such as document search and retrieval. You can set <code>use_ocr=True</code> in the <code>TextEmbedConfig</code> to enable OCR. But this requires <code>tesseract</code> and <code>poppler</code> to be installed.</p> <p>You can install <code>tesseract</code> and <code>poppler</code> using the following commands:</p>"},{"location":"guides/ocr/#install-tesseract-and-poppler","title":"Install Tesseract and Poppler","text":""},{"location":"guides/ocr/#windows","title":"Windows","text":"<p>For Tesseract, download the installer from here and install it.</p> <p>For Poppler, download the installer from here and install it.</p>"},{"location":"guides/ocr/#macos","title":"MacOS","text":"<p>For Tesseract, you can install it using Homebrew.</p> <pre><code>brew install tesseract\n</code></pre> <p>For Poppler, you can install it using Homebrew.</p> <pre><code>brew install poppler\n</code></pre>"},{"location":"guides/ocr/#linux","title":"Linux","text":"<p>For Tesseract, you can install it using the package manager for your Linux distribution. For example, on Ubuntu, you can install it using:</p> <pre><code>sudo apt install tesseract-ocr\nsudo apt install libtesseract-dev\n</code></pre> <p>For Poppler, you can install it using the package manager for your Linux distribution. For example, on Ubuntu, you can install it using:</p> <pre><code>sudo apt install poppler-utils\n</code></pre> <p>For more information, refer to the Tesseract installation guide.</p>"},{"location":"guides/ocr/#example-usage","title":"Example Usage","text":"<pre><code># OCR Requires `tesseract` and `poppler` to be installed.\n\nimport time\nimport embed_anything\nfrom embed_anything import EmbedData, EmbeddingModel, TextEmbedConfig, WhichModel\nfrom time import time\n\n\nmodel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Jina, model_id=\"jinaai/jina-embeddings-v2-small-en\"\n)\n\nconfig = TextEmbedConfig(\n    chunk_size=1000,\n    batch_size=32,\n    buffer_size=64,\n    splitting_strategy=\"sentence\",\n    use_ocr=True,\n)\n\nstart = time()\n\ndata: list[EmbedData] = embed_anything.embed_file(\n    \"/home/akshay/projects/starlaw/src-server/test_files/court.pdf\",  # Replace with your file path\n    embedder=model,\n    config=config,\n)\nend = time()\n\nfor d in data:\n    print(d.text)\n    print(\"---\" * 20)\n\nprint(f\"Time taken: {end - start} seconds\")\n</code></pre>"},{"location":"guides/onnx_models/","title":"Using ONNX Models","text":""},{"location":"guides/onnx_models/#supported-models","title":"Supported Models","text":"Enum Variant Description <code>AllMiniLML6V2</code> sentence-transformers/all-MiniLM-L6-v2 <code>AllMiniLML6V2Q</code> Quantized sentence-transformers/all-MiniLM-L6-v2 <code>AllMiniLML12V2</code> sentence-transformers/all-MiniLM-L12-v2 <code>AllMiniLML12V2Q</code> Quantized sentence-transformers/all-MiniLM-L12-v2 <code>ModernBERTBase</code> nomic-ai/modernbert-embed-base <code>ModernBERTLarge</code> nomic-ai/modernbert-embed-large <code>BGEBaseENV15</code> BAAI/bge-base-en-v1.5 <code>BGEBaseENV15Q</code> Quantized BAAI/bge-base-en-v1.5 <code>BGELargeENV15</code> BAAI/bge-large-en-v1.5 <code>BGELargeENV15Q</code> Quantized BAAI/bge-large-en-v1.5 <code>BGESmallENV15</code> BAAI/bge-small-en-v1.5 - Default <code>BGESmallENV15Q</code> Quantized BAAI/bge-small-en-v1.5 <code>NomicEmbedTextV1</code> nomic-ai/nomic-embed-text-v1 <code>NomicEmbedTextV15</code> nomic-ai/nomic-embed-text-v1.5 <code>NomicEmbedTextV15Q</code> Quantized nomic-ai/nomic-embed-text-v1.5 <code>ParaphraseMLMiniLML12V2</code> sentence-transformers/paraphrase-MiniLM-L6-v2 <code>ParaphraseMLMiniLML12V2Q</code> Quantized sentence-transformers/paraphrase-MiniLM-L6-v2 <code>ParaphraseMLMpnetBaseV2</code> sentence-transformers/paraphrase-mpnet-base-v2 <code>BGESmallZHV15</code> BAAI/bge-small-zh-v1.5 <code>MultilingualE5Small</code> intfloat/multilingual-e5-small <code>MultilingualE5Base</code> intfloat/multilingual-e5-base <code>MultilingualE5Large</code> intfloat/multilingual-e5-large <code>MxbaiEmbedLargeV1</code> mixedbread-ai/mxbai-embed-large-v1 <code>MxbaiEmbedLargeV1Q</code> Quantized mixedbread-ai/mxbai-embed-large-v1 <code>GTEBaseENV15</code> Alibaba-NLP/gte-base-en-v1.5 <code>GTEBaseENV15Q</code> Quantized Alibaba-NLP/gte-base-en-v1.5 <code>GTELargeENV15</code> Alibaba-NLP/gte-large-en-v1.5 <code>GTELargeENV15Q</code> Quantized Alibaba-NLP/gte-large-en-v1.5 <code>JINAV2SMALLEN</code> jinaai/jina-embeddings-v2-small-en <code>JINAV2BASEEN</code> jinaai/jina-embeddings-v2-base-en <code>JINAV3</code> jinaai/jina-embeddings-v3"},{"location":"guides/onnx_models/#example-usage","title":"Example Usage","text":"<pre><code>import heapq\nfrom embed_anything import (\n    EmbeddingModel,\n    TextEmbedConfig,\n    WhichModel,\n    embed_query,\n    ONNXModel,\n    Dtype,\n)\nimport os\nfrom time import time\nimport numpy as np\n\nmodel = EmbeddingModel.from_pretrained_onnx(\n    WhichModel.Bert, ONNXModel.ModernBERTBase, dtype = Dtype.Q4F16\n)\n\n# model = EmbeddingModel.from_pretrained_hf(\n#     WhichModel.Bert, \"BAAI/bge-small-en-v1.5\"\n# )\n\nsentences = [\n    \"The quick brown fox jumps over the lazy dog\",\n    \"The cat is sleeping on the mat\",\n    \"The dog is barking at the moon\",\n    \"I love pizza\",\n    \"I like to have pasta\",\n    \"The dog is sitting in the park\",\n]\n\nembedddings = embed_query(sentences, embedder=model)\n\nembed_vector = np.array([e.embedding for e in embedddings])\n\nprint(\"shape of embed_vector\", embed_vector.shape)\nsimilarities = np.matmul(embed_vector, embed_vector.T)\n\n# get top 5 similarities and show the two sentences and their similarity scores\n# Flatten the upper triangle of the similarity matrix, excluding the diagonal\nsimilarity_scores = [\n    (similarities[i, j], i, j)\n    for i in range(len(sentences))\n    for j in range(i + 1, len(sentences))\n]\n\n# Get the top 5 similarity scores\ntop_5_similarities = heapq.nlargest(5, similarity_scores, key=lambda x: x[0])\n\n# Print the top 5 similarities with sentences\nfor score, i, j in top_5_similarities:\n    print(f\"Score: {score:.2} | {sentences[i]} | {sentences[j]}\")\n\n\nfrom embed_anything import EmbeddingModel, WhichModel, embed_query, TextEmbedConfig\nimport os\nimport pymupdf\nfrom semantic_text_splitter import TextSplitter\nimport os\n\nmodel = EmbeddingModel.from_pretrained_onnx(WhichModel.Bert, ONNXModel.BGESmallENV15Q)\nsplitter = TextSplitter(1000)\nconfig = TextEmbedConfig(batch_size=128)\n\n\ndef embed_anything():\n    # get all pdfs from test_files\n\n    for file in os.listdir(\"bench\"):\n        text = []\n        doc = pymupdf.open(\"bench/\" + file)\n\n        for page in doc:\n            text.append(page.get_text())\n\n        text = \" \".join(text)\n        chunks = splitter.chunks(text)\n        embeddings = embed_query(chunks, model, config)\n\n\nstart = time()\nembed_anything()\n\nprint(time() - start)\n</code></pre>"},{"location":"guides/semantic/","title":"Using Semantic Chunking","text":"<p>Semantic encoding is essential for applications where maintaining the logical flow and meaning of text is critical, such as in document retrieval, question answering, or summarization. This approach ensures that embeddings capture the full intent and nuance of the original content, enhancing downstream model performance.</p> <pre><code>import embed_anything\nfrom embed_anything import EmbeddingModel, TextEmbedConfig, WhichModel\n\nmodel = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Jina, model_id=\"jinaai/jina-embeddings-v2-small-en\"\n)\n\n# with semantic encoder\nsemantic_encoder = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Jina, model_id=\"jinaai/jina-embeddings-v2-small-en\"\n)\nconfig = TextEmbedConfig(\n    chunk_size=1000,\n    batch_size=32,\n    splitting_strategy=\"semantic\",\n    semantic_encoder=semantic_encoder,\n)\n\ndata = embed_anything.embed_file(\"test_files/bank.txt\", embedder=model, config=config)\n\nfor d in data:\n    print(d.text)\n    print(\"---\" * 20)\n</code></pre>"},{"location":"roadmap/contribution/","title":"Contribution Guidelines","text":""},{"location":"roadmap/contribution/#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>To get started, check the [Issues Section] for tasks labeled \"Good First Issue\" or \"Help Needed\". These issues are perfect for new contributors or those looking to make a valuable impact quickly.</p> <p>If you find an issue you want to tackle:</p> <p>Comment on the issue to let us know you\u2019d like to work on it. Wait for confirmation\u2014an admin will assign the issue to you. \ud83d\udcbb Setting Up Your Development Environment To start working on the project, follow these steps:</p> <ol> <li>Fork the Repository: Begin by forking the repository from the dev branch. We do not allow direct contributions to the main branch.</li> <li>Clone Your Fork: After forking, clone the repository to your local machine.</li> <li>Create a New Branch: For each contribution, create a new branch following the naming convention: feature/your-feature-name or bugfix/your-bug-name.</li> </ol>"},{"location":"roadmap/contribution/#contributing-guidelines","title":"\ud83d\udee0\ufe0f Contributing Guidelines","text":"<p>\ud83d\udd0d Reporting Bugs If you find a bug, here\u2019s how to report it effectively:</p> <p>Title: Use a clear and descriptive title, with appropriate labels. Description: Provide a detailed description of the issue, including: Steps to reproduce the problem. Expected and actual behavior.</p> <p>Any relevant logs, screenshots, or additional context. Submit the Bug Report: Open a new issue in the [Issues Section] and include all the details. This helps us understand and resolve the problem faster.</p>"},{"location":"roadmap/contribution/#contributing-to-python-code","title":"\ud83d\udc0d Contributing to Python Code","text":"<p>If you're contributing to the Python codebase, follow these steps:</p> <ol> <li>Create an Independent File: Write your code in a new file within the python folder. </li> <li>Build with Maturin: After writing your code, use maturin build to build the package. </li> <li>Import and Call the Function: </li> <li>Use the following import syntax: from embed_anything. import *  <li>Then, call the function using: from embed_anything import   Feel free to open an issue if you encounter any problems during the process. <p>\ud83e\udde9 Contributing to Adapters To contribute to adapters, follow these guidelines:</p> <ol> <li>Implement Adapter Class: Create an Adapter class that supports the create, add, and delete operations for your specific use case. </li> <li>Check Existing Adapters: Use the existing Pinecone and Weaviate adapters as references to maintain consistency in structure and functionality. </li> <li>Testing: Ensure your adapter is tested thoroughly before submitting a pull request.</li> </ol>"},{"location":"roadmap/contribution/#submitting-a-pull-request","title":"\ud83d\udd04 Submitting a Pull Request","text":"<p>Once your contribution is ready: </p> <p>Push Your Branch: Push your branch to your forked repository.</p> <p>Submit a Pull Request (PR): Open a PR from your branch to the dev branch of the main repository. Ensure your PR includes:</p> <ol> <li>A clear description of the changes.</li> <li>Any relevant issue numbers (e.g., \"Closes #123\").</li> <li>Wait for Review: A maintainer will review your PR. Please be responsive to any feedback or requested changes.</li> </ol>"},{"location":"roadmap/roadmap/","title":"\ud83c\udfce\ufe0f RoadMap","text":""},{"location":"roadmap/roadmap/#accomplishments","title":"Accomplishments","text":"<p>One of the aims of EmbedAnything is to allow AI engineers to easily use state of the art embedding models on typical files and documents. A lot has already been accomplished here and these are the formats that we support right now and a few more have to be done. </p>"},{"location":"roadmap/roadmap/#modalities-and-source","title":"\ud83d\uddbc\ufe0f Modalities and Source","text":"<p>We\u2019re excited to share that we've expanded our platform to support multiple modalities, including:</p> <ul> <li> <p> Audio files</p> </li> <li> <p> Markdowns</p> </li> <li> <p> Websites</p> </li> <li> <p> Images</p> </li> <li> <p> Videos</p> </li> <li> <p> Graph</p> </li> </ul> <p>This gives you the flexibility to work with various data types all in one place! \ud83c\udf10 </p>"},{"location":"roadmap/roadmap/#product","title":"\ud83d\udc9c Product","text":"<p>We\u2019ve rolled out some major updates in version 0.3 to improve both functionality and performance. Here\u2019s what\u2019s new:</p> <ul> <li> <p>Semantic Chunking: Optimized chunking strategy for better Retrieval-Augmented Generation (RAG) workflows.</p> </li> <li> <p>Streaming for Efficient Indexing: We\u2019ve introduced streaming for memory-efficient indexing in vector databases. Want to know more? Check out our article on this feature here: https://www.analyticsvidhya.com/blog/2024/09/vector-streaming/</p> </li> <li> <p>Zero-Shot Applications: Explore our zero-shot application demos to see the power of these updates in action.</p> </li> <li> <p>Intuitive Functions: Version 0.3 includes a complete refactor for more intuitive functions, making the platform easier to use.</p> </li> <li> <p>Chunkwise Streaming: Instead of file-by-file streaming, we now support chunkwise streaming, allowing for more flexible and efficient data processing.</p> </li> </ul> <p>Check out the latest release :  and see how these features can supercharge your GenerativeAI pipeline! \u2728</p>"},{"location":"roadmap/roadmap/#coming-soon","title":"\ud83d\ude80Coming Soon","text":""},{"location":"roadmap/roadmap/#performance","title":"\u2699\ufe0f Performance","text":"<p>We've received quite a few questions about why we're using Candle, so here's a quick explanation:</p> <p>One of the main reasons is that Candle doesn't require any specific ONNX format models, which means it can work seamlessly with any Hugging Face model. This flexibility has been a key factor for us. However, we also recognize that we\u2019ve been compromising a bit on speed in favor of that flexibility.</p> <p>What\u2019s Next? To address this, we\u2019re excited to announce that we\u2019re introducing Candle-ONNX along with our previous framework on hugging-face ,</p> <p>\u27a1\ufe0f Support for GGUF models  - Significantly faster performance - Stay tuned for these exciting updates! \ud83d\ude80</p>"},{"location":"roadmap/roadmap/#embeddings","title":"\ud83e\uded0Embeddings:","text":"<p>We had multimodality from day one for our infrastructure. We have already included it for websites, images and audios but we want to expand it further to.</p> <p>\u2611\ufe0fGraph embedding -- build deepwalks embeddings depth first and word to vec  \u2611\ufe0fVideo Embedding  \u2611\ufe0f Yolo Clip </p>"},{"location":"roadmap/roadmap/#expansion-to-other-vector-adapters","title":"\ud83c\udf0aExpansion to other Vector Adapters","text":"<p>We currently support a wide range of vector databases for streaming embeddings, including:</p> <ul> <li>Elastic: thanks to amazing and active Elastic team for the contribution </li> <li>Weaviate</li> <li>Pinecone</li> <li>Qdrant</li> <li>Milvus</li> </ul> <p>But we're not stopping there! We're actively working to expand this list.</p> <p>Want to Contribute? If you\u2019d like to add support for your favorite vector database, we\u2019d love to have your help! Check out our contribution.md for guidelines, or feel free to reach out directly starlight-search@proton.me. Let's build something amazing together! \ud83d\udca1</p>"},{"location":"blog/page/2/","title":"\ud83d\udcf0 All Posts","text":""}]}