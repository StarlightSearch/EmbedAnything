{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> Supercharge your embedding pipeline with minimalist and lightening fast framework built in rust \ud83e\udd80 Explore the docs \u00bb View Demo     \u00b7     Examples     \u00b7     Request Feature     .     Search in Audio Space </p> <p>EmbedAnything is a minimalist yet highly performant, lightweight, lightening fast, multisource, multimodal and local embedding pipeline, built in rust. Whether you're working with text, images, audio, PDFs, websites, or other media, EmbedAnything simplifies the process of generating embeddings from various sources and storing them in a vector database.</p> Table of Contents <ol> <li> About The Project <ul> <li>Built With Rust</li> <li>Why Candle?</li> </ul> </li> <li> Getting Started <ul> <li>Installation</li> </ul> </li> <li>Usage</li> <li>Roadmap</li> <li>Contributing</li> <li>How to add custom model and chunk size</li> </ol>"},{"location":"#key-features","title":"\ud83d\ude80 Key Features","text":"<ul> <li>Local Embedding : Works with local embedding models like BERT and JINA</li> <li>Cloud Embedding Models:: Supports OpenAI. Mistral and Cohere Support coming soon.  </li> <li>MultiModality : Works with text sources like PDFs, txt, md, Images JPG and Audio, .WAV</li> <li>Rust : All the file processing is done in rust for speed and efficiency</li> <li>Candle : We have taken care of hardware acceleration as well, with Candle.</li> <li>Python Interface: Packaged as a Python library for seamless integration into your existing projects.</li> <li>Scalable: Store embeddings in a vector database for easy retrieval and scalability. </li> </ul>"},{"location":"#why-embed-anything","title":"\ud83e\udd80 Why Embed Anything","text":"<p>\u27a1\ufe0fFaster execution.  \u27a1\ufe0fMemory Management: Rust enforces memory management simultaneously, preventing memory leaks and crashes that can plague other languages  \u27a1\ufe0fTrue multithreading  \u27a1\ufe0fRunning language models or embedding models locally and efficiently  \u27a1\ufe0fCandle allows inferences on CUDA-enabled GPUs right out of the box.  \u27a1\ufe0fDecrease the memory usage of EmbedAnything.</p>"},{"location":"#supported-models","title":"\u2b50 Supported Models","text":"<p>We support a range of models, that can be supported by Candle, We have given a set of tested models but if you have specific usecase do mention it in the issue.</p>"},{"location":"#getting-started","title":"\ud83e\uddd1\u200d\ud83d\ude80 Getting Started","text":""},{"location":"#installation","title":"\ud83d\udc9a Installation","text":"<p><code>pip install embed-anything</code></p>"},{"location":"#usage","title":"\ud83d\udcdd Usage","text":"<p>To use local embedding: we support Bert and Jina</p> <pre><code>import embed_anything\ndata = embed_anything.embed_file(\"file_path.pdf\", embeder= \"Bert\")\nembeddings = np.array([data.embedding for data in data])\n</code></pre>"},{"location":"#image-embeddings","title":"\ud83d\udcf7 Image Embeddings","text":"<p>Requirements: Directory with pictures you want to search for example we have <code>test_files</code> with images of cat, dogs etc</p> <pre><code>import embed_anything\ndata = embed_anything.embed_directory(\"directory_path\", embeder= \"Clip\")\nembeddings = np.array([data.embedding for data in data])\n\nquery = [\"photo of a dog\"]\nquery_embedding = np.array(embed_anything.embed_query(query, embeder= \"Clip\")[0].embedding)\nsimilarities = np.dot(embeddings, query_embedding)\nmax_index = np.argmax(similarities)\nImage.open(data[max_index].text).show()\n</code></pre>"},{"location":"#audio-embedding-using-whisper","title":"\ud83d\udd0a Audio Embedding using Whisper","text":"<p>requirements:  Audio .wav files.</p> <pre><code>import embed_anything\nfrom embed_anything import JinaConfig, EmbedConfig, AudioDecoderConfig\nimport time\n\nstart_time = time.time()\n\n# choose any whisper or distilwhisper model \n# from https://huggingface.co/distil-whisper or \n# https://huggingface.co/collections/openai/whisper-release-6501bba2cf999715fd953013\naudio_decoder_config = AudioDecoderConfig(\n    decoder_model_id=\"openai/whisper-tiny.en\",\n    decoder_revision=\"main\",\n    model_type=\"tiny-en\",\n    quantized=False,\n)\njina_config = JinaConfig(\n    model_id=\"jinaai/jina-embeddings-v2-small-en\", revision=\"main\", chunk_size=100\n)\n\nconfig = EmbedConfig(jina=jina_config, audio_decoder=audio_decoder_config)\ndata = embed_anything.embed_file(\n    \"test_files/audio/samples_hp0.wav\", embeder=\"Audio\", config=config\n)\nprint(data[0].metadata)\nend_time = time.time()\nprint(\"Time taken: \", end_time - start_time)\n</code></pre>"},{"location":"#using-embedding-models-from-hugging-face","title":"\ud83e\udd17 Using embedding models from Hugging Face","text":"<pre><code>jina_config = JinaConfig(\n    model_id=\"Custom link given below\", revision=\"main\", chunk_size=100\n)\nembed_config = EmbedConfig(jina=jina_config)\n</code></pre> Model Custom link Jina jinaai/jina-embeddings-v2-base-en jinaai/jina-embeddings-v2-small-en Bert sentence-transformers/all-MiniLM-L6-v2 sentence-transformers/all-MiniLM-L12-v2 sentence-transformers/paraphrase-MiniLM-L6-v2 Clip openai/clip-vit-base-patch32 Whisper Most OpenAI Whisper from huggingface supported."},{"location":"#contributing-to-embedanything","title":"\ud83d\udea7 Contributing to EmbedAnything","text":"<p>First of all, thank you for taking the time to contribute to this project. We truly appreciate your contributions, whether it's bug reports, feature suggestions, or pull requests. Your time and effort are highly valued in this project. \ud83d\ude80</p> <p>This document provides guidelines and best practices to help you to contribute effectively. These are meant to serve as guidelines, not strict rules. We encourage you to use your best judgment and feel comfortable proposing changes to this document through a pull request.</p> <li>Roadmap</li> <li>Quick Start</li> <li>Guidelines</li>"},{"location":"#roadmap","title":"RoadMap","text":"<p>One of the aims of EmbedAnything is to allow AI engineers to easily use state of the art embedding models on typical files and documents. A lot has already been accomplished here and these are the formats that we support right now and a few more have to be done.  \u2705 Markdown, PDFs, and Website  \u2705 WAV File  \u2705 JPG, PNG, webp  \u2705Add whisper for audio embeddings  \u2705Custom model upload, anything that is available in candle  \u2705Custom chunk size  \u2705Pinecone Adapter, to directly save it on it.  \u2705Zero-shot application </p> <p>Yet to do be done  \u2611\ufe0fVector Database: Add functionalities to integrate with any Vector Database  \u2611\ufe0fGraph embedding -- build deepwalks embeddings depth first and word to vec  \u2611\ufe0fAsynchronous chunks training</p>"},{"location":"#code-of-conduct","title":"\u2714\ufe0f Code of Conduct:","text":"<p>Please read our [Code of Conduct] to understand the expectations we have for all contributors participating in this project. By participating, you agree to abide by our Code of Conduct.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>You can quickly get started with contributing by searching for issues with the labels \"Good First Issue\" or \"Help Needed\" in the [Issues Section]. If you think you can contribute, comment on the issue and we will assign it to you.  </p> <p>To set up your development environment, please follow the steps mentioned below : </p> <ol> <li>Fork the repository from dev, We don't allow direct contribution to main</li> </ol>"},{"location":"#contributing-guidelines","title":"Contributing Guidelines","text":""},{"location":"#reporting-bugs","title":"\ud83d\udd0d Reporting Bugs","text":"<ol> <li>Title describing the issue clearly and concisely with relevant labels</li> <li>Provide a detailed description of the problem and the necessary steps to reproduce the issue.</li> <li>Include any relevant logs, screenshots, or other helpful information supporting the issue.</li> </ol>"},{"location":"references/","title":"References","text":"<p>This module provides functions and classes for embedding queries, files, and directories using different embedding models.</p> <p>The module includes the following functions:</p> <ul> <li><code>embed_query</code>: Embeds the given query and returns an EmbedData object.</li> <li><code>embed_file</code>: Embeds the file at the given path and returns a list of EmbedData objects.</li> <li><code>embed_directory</code>: Embeds all the files in the given directory and returns a list of EmbedData objects.</li> </ul> <p>The module also includes the <code>EmbedData</code> class, which represents the data of an embedded file.</p>"},{"location":"references/#python.python.embed_anything--usage","title":"Usage:","text":"<pre><code>import embed_anything\nfrom embed_anything import EmbedData\n\n#For text files\n\nmodel = EmbeddingModel.from_pretrained_local(\n    WhichModel.Bert, model_id=\"Hugging_face_link\"\n)\ndata = embed_anything.embed_file(\"test_files/test.pdf\", embeder=model)\n\n\n#For images\nmodel = embed_anything.EmbeddingModel.from_pretrained_local(\n    embed_anything.WhichModel.Clip,\n    model_id=\"openai/clip-vit-base-patch16\",\n    # revision=\"refs/pr/15\",\n)\ndata: list[EmbedData] = embed_anything.embed_directory(\"test_files\", embeder=model)\nembeddings = np.array([data.embedding for data in data])\nquery = [\"Photo of a monkey?\"]\nquery_embedding = np.array(\n    embed_anything.embed_query(query, embeder=model)[0].embedding\n)\n# For audio files\nfrom embed_anything import (\n    AudioDecoderModel,\n    EmbeddingModel,\n    embed_audio_file,\n    TextEmbedConfig,\n)\n# choose any whisper or distilwhisper model from https://huggingface.co/distil-whisper or https://huggingface.co/collections/openai/whisper-release-6501bba2cf999715fd953013\naudio_decoder = AudioDecoderModel.from_pretrained_hf(\n    \"openai/whisper-tiny.en\", revision=\"main\", model_type=\"tiny-en\", quantized=False\n)\nembeder = EmbeddingModel.from_pretrained_hf(\n    embed_anything.WhichModel.Bert,\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    revision=\"main\",\n)\nconfig = TextEmbedConfig(chunk_size=200, batch_size=32)\ndata = embed_anything.embed_audio_file(\n    \"test_files/audio/samples_hp0.wav\",\n    audio_decoder=audio_decoder,\n    embeder=embeder,\n    text_embed_config=config,\n)\n</code></pre> <p>You can also store the embeddings to a vector database and not keep them on memory. Here is an example of how to use the <code>PineconeAdapter</code> class:</p> <pre><code>import embed_anything\nimport os\n\nfrom embed_anything.vectordb import PineconeAdapter\n\n\n# Initialize the PineconeEmbedder class\napi_key = os.environ.get(\"PINECONE_API_KEY\")\nindex_name = \"anything\"\npinecone_adapter = PineconeAdapter(api_key)\n\ntry:\n    pinecone_adapter.delete_index(\"anything\")\nexcept:\n    pass\n\n# Initialize the PineconeEmbedder class\n\npinecone_adapter.create_index(dimension=512, metric=\"cosine\")\n\n# bert_model = EmbeddingModel.from_pretrained_hf(\n#     WhichModel.Bert, \"sentence-transformers/all-MiniLM-L12-v2\", revision=\"main\"\n# )\n\nclip_model = EmbeddingModel.from_pretrained_hf(\n    WhichModel.Clip, \"openai/clip-vit-base-patch16\", revision=\"main\"\n)\n\nembed_config = TextEmbedConfig(chunk_size=512, batch_size=32)\n\n\ndata = embed_anything.embed_image_directory(\n    \"test_files\",\n    embeder=clip_model,\n    adapter=pinecone_adapter,\n    # config=embed_config,\n</code></pre>"},{"location":"references/#python.python.embed_anything--supported-embedding-models","title":"Supported Embedding Models:","text":"<ul> <li> <p>Text Embedding Models:</p> <ul> <li>\"OpenAI\"</li> <li>\"Bert\"</li> <li>\"Jina\"</li> </ul> </li> <li> <p>Image Embedding Models:</p> <ul> <li>\"Clip\"</li> <li>\"SigLip\" (Coming Soon)</li> </ul> </li> <li> <p>Audio Embedding Models:</p> <ul> <li>\"Whisper\"</li> </ul> </li> </ul>"},{"location":"references/#python.python.embed_anything.AudioDecoderModel","title":"<code>AudioDecoderModel</code>","text":"<p>Represents an audio decoder model.</p> <p>Attributes:</p> Name Type Description <code>model_id</code> <code>str</code> <p>The ID of the audio decoder model.</p> <code>revision</code> <code>str</code> <p>The revision of the audio decoder model.</p> <code>model_type</code> <code>str</code> <p>The type of the audio decoder model.</p> <code>quantized</code> <code>bool</code> <p>A flag indicating whether the audio decoder model is quantized or not.</p> <p>Example: <pre><code>model = embed_anything.AudioDecoderModel.from_pretrained_hf(\n    model_id=\"openai/whisper-tiny.en\",\n    revision=\"main\",\n    model_type=\"tiny-en\",\n    quantized=False\n)\n</code></pre></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class AudioDecoderModel:\n    \"\"\"\n    Represents an audio decoder model.\n\n    Attributes:\n        model_id: The ID of the audio decoder model.\n        revision: The revision of the audio decoder model.\n        model_type: The type of the audio decoder model.\n        quantized: A flag indicating whether the audio decoder model is quantized or not.\n\n    Example:\n    ```python\n\n    model = embed_anything.AudioDecoderModel.from_pretrained_hf(\n        model_id=\"openai/whisper-tiny.en\",\n        revision=\"main\",\n        model_type=\"tiny-en\",\n        quantized=False\n    )\n    ```\n    \"\"\"\n\n    model_id: str\n    revision: str\n    model_type: str\n    quantized: bool\n\n    def from_pretrained_hf(\n        model_id: str | None = None,\n        revision: str | None = None,\n        model_type: str | None = None,\n        quantized: bool | None = None,\n    ): ...\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbedData","title":"<code>EmbedData</code>","text":"<p>Represents the data of an embedded file.</p> <p>Attributes:</p> Name Type Description <code>embedding</code> <code>list[float]</code> <p>The embedding of the file.</p> <code>text</code> <code>str</code> <p>The text for which the embedding is generated for.</p> <code>metadata</code> <code>dict[str, str]</code> <p>Additional metadata associated with the embedding.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class EmbedData:\n    \"\"\"Represents the data of an embedded file.\n\n    Attributes:\n        embedding: The embedding of the file.\n        text: The text for which the embedding is generated for.\n        metadata: Additional metadata associated with the embedding.\n    \"\"\"\n\n    def __init__(self, embedding: list[float], text: str, metadata: dict[str, str]):\n        self.embedding = embedding\n        self.text = text\n        self.metadata = metadata\n    embedding: list[float]\n    text: str\n    metadata: dict[str, str]\n</code></pre>"},{"location":"references/#python.python.embed_anything.EmbeddingModel","title":"<code>EmbeddingModel</code>","text":"<p>Represents an embedding model.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class EmbeddingModel:\n    \"\"\"\n    Represents an embedding model.\n    \"\"\"\n\n    \"\"\"\n    Loads an embedding model from the Hugging Face model hub.\n\n    Args:\n        model_id: The ID of the model.\n        revision: The revision of the model.\n\n    Returns:\n        An EmbeddingModel object.\n\n    Example:\n    ```python\n    model = EmbeddingModel.from_pretrained_hf(\n        model_id=\"prithivida/miniMiracle_te_v1\",\n        revision=\"main\"\n    )\n    ```\n\n    \"\"\"\n    def from_pretrained_hf(\n        model: WhichModel, model_id: str, revision: str | None = None\n    ) -&gt; EmbeddingModel: ...\n\n    \"\"\"\n    Loads an embedding model from a cloud-based service.\n\n    Args:\n        model (WhichModel): The cloud service to use. Currently supports WhichModel.OpenAI and WhichModel.Cohere.\n        model_id (str): The ID of the model to use.\n            - For OpenAI, see available models at https://platform.openai.com/docs/guides/embeddings/embedding-models\n            - For Cohere, see available models at https://docs.cohere.com/docs/cohere-embed\n        api_key (str | None, optional): The API key for accessing the model. If not provided, it is taken from the environment variable:\n            - For OpenAI: OPENAI_API_KEY\n            - For Cohere: CO_API_KEY\n\n    Returns:\n        EmbeddingModel: An initialized EmbeddingModel object.\n\n    Raises:\n        ValueError: If an unsupported model is specified.\n\n    Example:\n    ```python\n    # Using Cohere\n    model = EmbeddingModel.from_pretrained_cloud(\n        model=WhichModel.Cohere, \n        model_id=\"embed-english-v3.0\"\n    )\n\n    # Using OpenAI\n    model = EmbeddingModel.from_pretrained_cloud(\n        model=WhichModel.OpenAI, \n        model_id=\"text-embedding-3-small\"\n    )\n    ```\n    \"\"\"\n    def from_pretrained_cloud(\n        model: WhichModel, model_id: str, api_key: str | None = None\n    ) -&gt; EmbeddingModel: ...\n    \"\"\"\n    Loads an ONNX embedding model.\n\n    Args:\n        model_architecture (WhichModel): The architecture of the embedding model to use.\n        model_id (str): The ID of the model.\n        revision (str | None, optional): The revision of the model. Defaults to None.\n\n    Returns:\n        EmbeddingModel: An initialized EmbeddingModel object.\n\n    Example:\n    ```python\n    model = EmbeddingModel.from_pretrained_onnx(\n        model_architecture=WhichModel.Bert,\n        model_id=\"BGESmallENV15Q\"\n    )\n    ```\n\n    Note:\n    This method loads a pre-trained model in ONNX format, which can offer improved inference speed\n    compared to standard PyTorch models. ONNX models are particularly useful for deployment\n    scenarios where performance is critical.\n    \"\"\"\n    def from_pretrained_onnx(\n            model_architecture: WhichModel, model_id: str, revision: str | None = None\n    ) -&gt; EmbeddingModel: ...\n</code></pre>"},{"location":"references/#python.python.embed_anything.ImageEmbedConfig","title":"<code>ImageEmbedConfig</code>","text":"<p>Represents the configuration for the Image Embedding model.</p> <p>Attributes:</p> Name Type Description <code>buffer_size</code> <code>int | None</code> <p>The buffer size for the Image Embedding model. Default is 100.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class ImageEmbedConfig:\n    \"\"\"\n    Represents the configuration for the Image Embedding model.\n\n    Attributes:\n        buffer_size: The buffer size for the Image Embedding model. Default is 100.\n    \"\"\"\n\n    def __init__(self, buffer_size: int | None = None):\n        self.buffer_size = buffer_size\n    buffer_size: int | None\n</code></pre>"},{"location":"references/#python.python.embed_anything.TextEmbedConfig","title":"<code>TextEmbedConfig</code>","text":"<p>Represents the configuration for the Text Embedding model.</p> <p>Attributes:</p> Name Type Description <code>chunk_size</code> <code>int | None</code> <p>The chunk size for the Text Embedding model.</p> <code>batch_size</code> <code>int | None</code> <p>The batch size for processing the embeddings. Default is 32. Based on the memory, you can increase or decrease the batch size.</p> <code>splitting_strategy</code> <code>str | None</code> <p>The strategy to use for splitting the text into chunks. Default is \"sentence\".</p> <code>semantic_encoder</code> <code>EmbeddingModel | None</code> <p>The semantic encoder for the Text Embedding model. Default is None.</p> <code>sparse_embeddings</code> <code>bool | None</code> <p>Whether to use sparse embeddings. Default is False.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>class TextEmbedConfig:\n    \"\"\"\n    Represents the configuration for the Text Embedding model.\n\n    Attributes:\n        chunk_size: The chunk size for the Text Embedding model.\n        batch_size: The batch size for processing the embeddings. Default is 32. Based on the memory, you can increase or decrease the batch size.\n        splitting_strategy: The strategy to use for splitting the text into chunks. Default is \"sentence\".\n        semantic_encoder: The semantic encoder for the Text Embedding model. Default is None.\n        sparse_embeddings: Whether to use sparse embeddings. Default is False.\n    \"\"\"\n\n    def __init__(\n        self,\n        chunk_size: int | None = 256,\n        batch_size: int | None = 32,\n        splitting_strategy: str | None = \"sentence\",\n        semantic_encoder: EmbeddingModel | None = None,\n        sparse_embeddings: bool | None = False,\n    ):\n        self.chunk_size = chunk_size\n        self.batch_size = batch_size\n        self.buffer_size = buffer_size\n        self.splitting_strategy = splitting_strategy\n        self.semantic_encoder = semantic_encoder\n        self.sparse_embeddings = sparse_embeddings\n    chunk_size: int | None\n    batch_size: int | None\n    buffer_size: int | None\n    splitting_strategy: str | None\n    semantic_encoder: EmbeddingModel | None\n    sparse_embeddings: bool | None\n</code></pre>"},{"location":"references/#python.python.embed_anything.embed_audio_file","title":"<code>embed_audio_file(file_path, audio_decoder, embeder, text_embed_config=TextEmbedConfig(chunk_size=200, batch_size=32))</code>","text":"<p>Embeds the given audio file and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the audio file to embed.</p> required <code>audio_decoder</code> <code>AudioDecoderModel</code> <p>The audio decoder model to use.</p> required <code>embeder</code> <code>EmbeddingModel</code> <p>The embedding model to use.</p> required <code>text_embed_config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding model.</p> <code>TextEmbedConfig(chunk_size=200, batch_size=32)</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> <p>Example: <pre><code>import embed_anything\naudio_decoder = embed_anything.AudioDecoderModel.from_pretrained_hf(\n    \"openai/whisper-tiny.en\", revision=\"main\", model_type=\"tiny-en\", quantized=False\n)\n\nembeder = embed_anything.EmbeddingModel.from_pretrained_hf(\n    embed_anything.WhichModel.Bert,\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    revision=\"main\",\n)\n\nconfig = embed_anything.TextEmbedConfig(chunk_size=200, batch_size=32)\ndata = embed_anything.embed_audio_file(\n    \"test_files/audio/samples_hp0.wav\",\n    audio_decoder=audio_decoder,\n    embeder=embeder,\n    text_embed_config=config,\n)\n</code></pre></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_audio_file(\n    file_path: str,\n    audio_decoder: AudioDecoderModel,\n    embeder: EmbeddingModel,\n    text_embed_config: TextEmbedConfig | None = TextEmbedConfig(\n        chunk_size=200, batch_size=32\n    ),\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given audio file and returns a list of EmbedData objects.\n\n    Args:\n        file_path: The path to the audio file to embed.\n        audio_decoder: The audio decoder model to use.\n        embeder: The embedding model to use.\n        text_embed_config: The configuration for the embedding model.\n\n    Returns:\n        A list of EmbedData objects.\n\n    Example:\n    ```python\n\n    import embed_anything\n    audio_decoder = embed_anything.AudioDecoderModel.from_pretrained_hf(\n        \"openai/whisper-tiny.en\", revision=\"main\", model_type=\"tiny-en\", quantized=False\n    )\n\n    embeder = embed_anything.EmbeddingModel.from_pretrained_hf(\n        embed_anything.WhichModel.Bert,\n        model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n        revision=\"main\",\n    )\n\n    config = embed_anything.TextEmbedConfig(chunk_size=200, batch_size=32)\n    data = embed_anything.embed_audio_file(\n        \"test_files/audio/samples_hp0.wav\",\n        audio_decoder=audio_decoder,\n        embeder=embeder,\n        text_embed_config=config,\n    )\n    ```\n\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.embed_directory","title":"<code>embed_directory(file_path, embeder, extensions, config=None, adapter=None)</code>","text":"<p>Embeds the files in the given directory and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the directory containing the files to embed.</p> required <code>embeder</code> <code>EmbeddingModel</code> <p>The embedding model to use.</p> required <code>extensions</code> <code>list[str]</code> <p>The list of file extensions to consider for embedding.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding model.</p> <code>None</code> <code>adapter</code> <code>Adapter | None</code> <p>The adapter to use for storing the embeddings in a vector database.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> <p>Example: <pre><code>import embed_anything\nmodel = embed_anything.EmbeddingModel.from_pretrained_hf(\n    embed_anything.WhichModel.Bert,\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    revision=\"main\",\n)\ndata = embed_anything.embed_directory(\"test_files\", embeder=model, extensions=[\".pdf\"])\n</code></pre></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_directory(\n    file_path: str,\n    embeder: EmbeddingModel,\n    extensions: list[str],\n    config: TextEmbedConfig | None = None,\n    adapter: Adapter | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the files in the given directory and returns a list of EmbedData objects.\n\n    Args:\n        file_path: The path to the directory containing the files to embed.\n        embeder: The embedding model to use.\n        extensions: The list of file extensions to consider for embedding.\n        config: The configuration for the embedding model.\n        adapter: The adapter to use for storing the embeddings in a vector database.\n\n    Returns:\n        A list of EmbedData objects.\n\n    Example:\n    ```python\n    import embed_anything\n    model = embed_anything.EmbeddingModel.from_pretrained_hf(\n        embed_anything.WhichModel.Bert,\n        model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n        revision=\"main\",\n    )\n    data = embed_anything.embed_directory(\"test_files\", embeder=model, extensions=[\".pdf\"])\n    ```\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.embed_file","title":"<code>embed_file(file_path, embeder, config=None, adapter=None)</code>","text":"<p>Embeds the given file and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the file to embed.</p> required <code>embeder</code> <code>EmbeddingModel</code> <p>The embedding model to use.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding model.</p> <code>None</code> <code>adapter</code> <code>Adapter | None</code> <p>The adapter to use for storing the embeddings in a vector database.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> <p>Example: <pre><code>import embed_anything\nmodel = embed_anything.EmbeddingModel.from_pretrained_hf(\n    embed_anything.WhichModel.Bert,\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    revision=\"main\",\n)\ndata = embed_anything.embed_file(\"test_files/test.pdf\", embeder=model)\n</code></pre></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_file(\n    file_path: str,\n    embeder: EmbeddingModel,\n    config: TextEmbedConfig | None = None,\n    adapter: Adapter | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given file and returns a list of EmbedData objects.\n\n    Args:\n        file_path: The path to the file to embed.\n        embeder: The embedding model to use.\n        config: The configuration for the embedding model.\n        adapter: The adapter to use for storing the embeddings in a vector database.\n\n    Returns:\n        A list of EmbedData objects.\n\n    Example:\n    ```python\n    import embed_anything\n    model = embed_anything.EmbeddingModel.from_pretrained_hf(\n        embed_anything.WhichModel.Bert,\n        model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n        revision=\"main\",\n    )\n    data = embed_anything.embed_file(\"test_files/test.pdf\", embeder=model)\n    ```\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.embed_image_directory","title":"<code>embed_image_directory(file_path, embeder, config=None, adapter=None)</code>","text":"<p>Embeds the images in the given directory and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the directory containing the images to embed.</p> required <code>embeder</code> <code>EmbeddingModel</code> <p>The embedding model to use.</p> required <code>config</code> <code>ImageEmbedConfig | None</code> <p>The configuration for the embedding model.</p> <code>None</code> <code>adapter</code> <code>Adapter | None</code> <p>The adapter to use for storing the embeddings in a vector database.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_image_directory(\n    file_path: str,\n    embeder: EmbeddingModel,\n    config: ImageEmbedConfig | None = None,\n    adapter: Adapter | None = None,\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the images in the given directory and returns a list of EmbedData objects.\n\n    Args:\n        file_path: The path to the directory containing the images to embed.\n        embeder: The embedding model to use.\n        config: The configuration for the embedding model.\n        adapter: The adapter to use for storing the embeddings in a vector database.\n\n    Returns:\n        A list of EmbedData objects.\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.embed_query","title":"<code>embed_query(query, embeder, config=None)</code>","text":"<p>Embeds the given query and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>list[str]</code> <p>The query to embed.</p> required <code>embeder</code> <code>EmbeddingModel</code> <p>The embedding model to use.</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding model.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[EmbedData]</code> <p>A list of EmbedData objects.</p> <p>Example:</p> <pre><code>import embed_anything\nmodel = embed_anything.EmbeddingModel.from_pretrained_hf(\n    embed_anything.WhichModel.Bert,\n    model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n    revision=\"main\",\n)\n</code></pre> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_query(\n    query: list[str], embeder: EmbeddingModel, config: TextEmbedConfig | None = None\n) -&gt; list[EmbedData]:\n    \"\"\"\n    Embeds the given query and returns a list of EmbedData objects.\n\n    Args:\n        query: The query to embed.\n        embeder: The embedding model to use.\n        config: The configuration for the embedding model.\n\n    Returns:\n        A list of EmbedData objects.\n\n    Example:\n\n    ```python\n    import embed_anything\n    model = embed_anything.EmbeddingModel.from_pretrained_hf(\n        embed_anything.WhichModel.Bert,\n        model_id=\"sentence-transformers/all-MiniLM-L6-v2\",\n        revision=\"main\",\n    )\n    ```\n    \"\"\"\n</code></pre>"},{"location":"references/#python.python.embed_anything.embed_webpage","title":"<code>embed_webpage(url, embeder, config, adapter)</code>","text":"<p>Embeds the webpage at the given URL and returns a list of EmbedData objects.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the webpage to embed.</p> required <code>embeder</code> <code>EmbeddingModel</code> <p>The name of the embedding model to use. Choose between \"OpenAI\", \"Jina\", \"Bert\"</p> required <code>config</code> <code>TextEmbedConfig | None</code> <p>The configuration for the embedding model.</p> required <code>adapter</code> <code>Adapter | None</code> <p>The adapter to use for storing the embeddings.</p> required <p>Returns:</p> Type Description <code>list[EmbedData] | None</code> <p>A list of EmbedData objects</p> <p>Example: <pre><code>import embed_anything\n\nconfig = embed_anything.EmbedConfig(\n    openai_config=embed_anything.OpenAIConfig(model=\"text-embedding-3-small\")\n)\ndata = embed_anything.embed_webpage(\n    \"https://www.akshaymakes.com/\", embeder=\"OpenAI\", config=config\n)\n</code></pre></p> Source code in <code>python/python/embed_anything/_embed_anything.pyi</code> <pre><code>def embed_webpage(\n    url: str,\n    embeder: EmbeddingModel,\n    config: TextEmbedConfig | None,\n    adapter: Adapter | None,\n) -&gt; list[EmbedData] | None:\n    \"\"\"Embeds the webpage at the given URL and returns a list of EmbedData\n    objects.\n\n    Args:\n        url: The URL of the webpage to embed.\n        embeder: The name of the embedding model to use. Choose between \"OpenAI\", \"Jina\", \"Bert\"\n        config: The configuration for the embedding model.\n        adapter: The adapter to use for storing the embeddings.\n\n    Returns:\n        A list of EmbedData objects\n\n    Example:\n    ```python\n    import embed_anything\n\n    config = embed_anything.EmbedConfig(\n        openai_config=embed_anything.OpenAIConfig(model=\"text-embedding-3-small\")\n    )\n    data = embed_anything.embed_webpage(\n        \"https://www.akshaymakes.com/\", embeder=\"OpenAI\", config=config\n    )\n    ```\n    \"\"\"\n</code></pre>"},{"location":"references/#adapters","title":"Adapters","text":""},{"location":"RoadMap/roadmap/","title":"\ud83c\udfce\ufe0f RoadMap","text":"<p>One of the aims of EmbedAnything is to allow AI engineers to easily use state of the art embedding models on typical files and documents. A lot has already been accomplished here and these are the formats that we support right now and a few more have to be done. </p>"},{"location":"RoadMap/roadmap/#modalities-and-source","title":"\ud83d\uddbc\ufe0f Modalities and Source","text":"<p>We\u2019re excited to share that we've expanded our platform to support multiple modalities, including:</p> <ul> <li>Audio files</li> <li>Markdowns</li> <li>Websites</li> <li>Images</li> <li>Custom model uploads </li> </ul> <p>This gives you the flexibility to work with various data types all in one place! \ud83c\udf10 </p>"},{"location":"RoadMap/roadmap/#product","title":"\ud83d\udc9c Product","text":"<p>We\u2019ve rolled out some major updates in version 0.3 to improve both functionality and performance. Here\u2019s what\u2019s new:</p> <ul> <li> <p>Semantic Chunking: Optimized chunking strategy for better Retrieval-Augmented Generation (RAG) workflows.</p> </li> <li> <p>Streaming for Efficient Indexing: We\u2019ve introduced streaming for memory-efficient indexing in vector databases. Want to know more? Check out our article on this feature here: https://www.analyticsvidhya.com/blog/2024/09/vector-streaming/</p> </li> <li> <p>Zero-Shot Applications: Explore our zero-shot application demos to see the power of these updates in action.</p> </li> <li> <p>Intuitive Functions: Version 0.3 includes a complete refactor for more intuitive functions, making the platform easier to use.</p> </li> <li> <p>Chunkwise Streaming: Instead of file-by-file streaming, we now support chunkwise streaming, allowing for more flexible and efficient data processing.</p> </li> </ul> <p>Check out the latest release :  and see how these features can supercharge your GenerativeAI pipeline! \u2728</p>"},{"location":"RoadMap/roadmap/#coming-soon","title":"\ud83d\ude80Coming Soon","text":""},{"location":"RoadMap/roadmap/#performance","title":"\u2699\ufe0f Performance","text":"<p>We've received quite a few questions about why we're using Candle, so here's a quick explanation:</p> <p>One of the main reasons is that Candle doesn't require any specific ONNX format models, which means it can work seamlessly with any Hugging Face model. This flexibility has been a key factor for us. However, we also recognize that we\u2019ve been compromising a bit on speed in favor of that flexibility.</p> <p>What\u2019s Next? To address this, we\u2019re excited to announce that we\u2019re introducing Candle-ONNX along with our previous framework on hugging-face ,</p> <p>\u27a1\ufe0f Support for GGUF models  - Significantly faster performance - Stay tuned for these exciting updates! \ud83d\ude80</p>"},{"location":"RoadMap/roadmap/#embeddings","title":"\ud83e\uded0Embeddings:","text":"<p>We had multimodality from day one for our infrastructure. We have already included it for websites, images and audios but we want to expand it further to.</p> <p>\u2611\ufe0fGraph embedding -- build deepwalks embeddings depth first and word to vec  \u2611\ufe0fVideo Embedding  \u2611\ufe0f Yolo Clip </p>"},{"location":"RoadMap/roadmap/#expansion-to-other-vector-adapters","title":"\ud83c\udf0aExpansion to other Vector Adapters","text":"<p>We currently support a wide range of vector databases for streaming embeddings, including:</p> <ul> <li>Elastic: thanks to amazing and active Elastic team for the contribution </li> <li>Weaviate</li> <li>Pinecone</li> <li>Qdrant</li> </ul> <p>But we're not stopping there! We're actively working to expand this list.</p> <p>Want to Contribute? If you\u2019d like to add support for your favorite vector database, we\u2019d love to have your help! Check out our contribution.md for guidelines, or feel free to reach out directly starlight-search@proton.me. Let's build something amazing together! \ud83d\udca1</p>"},{"location":"blog/","title":"All Posts","text":""},{"location":"blog/2024/01/31/vector-streaming/","title":"Vector streaming","text":"<p>Introducing vector streaming in EmbedAnything, a feature designed to optimize large-scale document embedding. By enabling asynchronous chunking and embedding using Rust\u2019s concurrency, it reduces memory usage and speeds up the process. We also show how to integrate it with the Weaviate Vector Database for seamless image embedding and search.</p> <p>In my previous article Supercharge Your Embeddings Pipeline with EmbedAnything, I discussed the idea behind EmbedAnything and how it makes creating embeddings from multiple modalities easy. In this article, I want to introduce a new feature of EmbedAnything called vector streaming and see how it works with Weaviate Vector Database. </p>"},{"location":"blog/2024/01/31/vector-streaming/#what-is-the-problem","title":"What is the problem?","text":"<p>First, let's examine the current problem with creating embeddings, especially in large-scale documents. The current embedding frameworks operate on a two-step process: chunking and embedding. First, the text is extracted from all the files, and chunks/nodes are created. Then, these chunks are fed to an embedding model with a specific batch size to process the embeddings. While this is done, the chunks and the embeddings stay on the system memory. This is not a problem when the files are small, and the embedding dimensions are small. But this becomes a problem when there are many files and you are working with large models and, even worse, multi-vector embeddings. Thus, to work with this, a high RAM is required to process the embeddings. Also, if this is done synchronously, a lot of time is wasted while the chunks are being created, as chunking is not a compute-heavy operation. As the chunks are being made, passing them to the embedding model would be efficient. </p>"},{"location":"blog/2024/01/31/vector-streaming/#our-solution","title":"Our Solution","text":"<p>The solution is to create an asynchronous chunking and embedding task. We can effectively spawn threads to handle this task using Rust's concurrency patterns and thread safety. This is done using Rust's MPSC (Multi-producer Single Consumer) module, which passes messages between threads. Thus, this creates a stream of chunks passed into the embedding thread with a buffer. Once the buffer is complete, it embeds the chunks and sends the embeddings back to the main thread, where they are sent to the vector database. This ensures no time is wasted on a single operation and no bottlenecks. Moreover, only the chunks and embeddings in the buffer are stored in the system memory. They are erased from the memory once moved to the vector database. </p> <p></p>"},{"location":"blog/2024/01/31/vector-streaming/#example-use-case","title":"Example Use Case","text":"<p>Now, let's see this feature in action. </p> <p>With EmbedAnything, streaming the vectors from a directory of files to the vector database is a simple three-step process. </p> <ol> <li> <p>Create an adapter for your vector database: This is a wrapper around the database's functions that allows you to create an index, convert metadata from EmbedAnything's format to the format required by the database, and the function to insert the embeddings in the index. Adapters for the prominent databases are already created and present here: </p> </li> <li> <p>Initiate an embedding model of your choice: You can choose from different local models or even cloud models. The configuration can also be determined to set the chunk size and buffer size for how many embeddings need to be streamed at once. Ideally, this should be as high as possible, but the system RAM limits this. </p> </li> <li> <p>Call the embedding function from EmbedAnything: Just pass the directory path to be embedded, the embedding model, the adapter, and the configuration. </p> </li> </ol> <p>In this example, we will embed a directory of images and send it to the vector databases. </p>"},{"location":"blog/2024/01/31/vector-streaming/#step-1-create-the-adapter","title":"Step 1: Create the Adapter","text":"<p>In EmbedAnything, the adapters are created outside so as to not make the library heavy and you get to choose which database you want to work with. Here is a simple adapter for Weaviate.</p> <pre><code>from embed_anything import EmbedData\nfrom embed_anything.vectordb import Adapter\n\nclass WeaviateAdapter(Adapter):\n    def __init__(self, api_key, url):\n        super().__init__(api_key)\n        self.client = weaviate.connect_to_weaviate_cloud(\n            cluster_url=url, auth_credentials=wvc.init.Auth.api_key(api_key)\n        )\n        if self.client.is_ready():\n            print(\"Weaviate is ready\")\n\n    def create_index(self, index_name: str):\n        self.index_name = index_name\n        self.collection = self.client.collections.create(\n            index_name, vectorizer_config=wvc.config.Configure.Vectorizer.none()\n        )\n        return self.collection\n\n    def convert(self, embeddings: List[EmbedData]):\n        data = []\n        for embedding in embeddings:\n            property = embedding.metadata\n            property[\"text\"] = embedding.text\n            data.append(\n                wvc.data.DataObject(properties=property, vector=embedding.embedding)\n            )\n        return data\n\n    def upsert(self, embeddings):\n        data = self.convert(embeddings)\n        self.client.collections.get(self.index_name).data.insert_many(data)\n\n    def delete_index(self, index_name: str):\n        self.client.collections.delete(index_name)\n\n### Start the client and index\n\nURL = \"your-weaviate-url\"\nAPI_KEY = \"your-weaviate-api-key\"\nweaviate_adapter = WeaviateAdapter(API_KEY, URL)\n\nindex_name = \"Test_index\"\nif index_name in weaviate_adapter.client.collections.list_all():\n    weaviate_adapter.delete_index(index_name)\nweaviate_adapter.create_index(\"Test_index\")\n</code></pre>"},{"location":"blog/2024/01/31/vector-streaming/#step-2-create-the-embedding-model","title":"Step 2: Create the Embedding Model","text":"<p>Here, since we are embedding images, we can use the clip model </p> <pre><code>import embed_anything import WhichModel\n\nmodel = embed_anything.EmbeddingModel.from_pretrained_cloud(\n        embed_anything.WhichModel.Clip,     \n        model_id=\"openai/clip-vit-base-patch16\")\n</code></pre>"},{"location":"blog/2024/01/31/vector-streaming/#step-3-embed-the-directory","title":"Step 3: Embed the Directory","text":"<pre><code>data = embed_anything.embed_image_directory(\n    \"\\image_directory\",\n    embeder=model,\n    adapter=weaviate_adapter,\n    config=embed_anything.ImageEmbedConfig(buffer_size=100),\n)\n</code></pre>"},{"location":"blog/2024/01/31/vector-streaming/#step-4-query-the-vector-database","title":"Step 4: Query the Vector Database","text":"<pre><code>query_vector = embed_anything.embed_query([\"image of a cat\"], embeder=model)[0].embedding\n</code></pre>"},{"location":"blog/2024/01/31/vector-streaming/#step-5-query-the-vector-database","title":"Step 5: Query the Vector Database","text":"<pre><code>response = weaviate_adapter.collection.query.near_vector(\n    near_vector=query_vector,\n    limit=2,\n    return_metadata=wvc.query.MetadataQuery(certainty=True),\n)\n</code></pre> <p>Check the response;</p> <p></p> <p>Check out the notebook for the code here on colab</p> <p></p>"},{"location":"blog/2024/01/31/vector-streaming/#conclusion","title":"Conclusion","text":"<p>We think vector streaming is one of the features that will empower many engineers to opt for a more optimized and no-tech debt solution. Instead of using bulky frameworks on the cloud, you can use a lightweight streaming option. Please don't forget to give us a \u2b50 on our GitHub repo over here: EmbedAnything Repo</p>"}]}