# Searching Images

This example shows how to use the EmbeddingModel from EmbedAnything to perform semantic image search within a directory, leveraging the CLIP model for accurate, language-guided matching.

``` python
--8<-- "examples/clip.py"
```

## Supported Models

EmbedAnything supports the following models for image search:

- openai/clip-vit-base-patch32
- openai/clip-vit-base-patch16
- openai/clip-vit-large-patch14-336
- openai/clip-vit-large-patch14